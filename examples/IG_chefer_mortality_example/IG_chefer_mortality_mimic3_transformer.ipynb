{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba18680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No config path provided, using default config\n",
      "Initializing mimic3 dataset from C:/Users/johnn/PyHealth_Data/Synthetic_MIMIC-III (dev mode: False)\n",
      "Scanning table: patients from C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\PATIENTS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\PATIENTS.csv\n",
      "Scanning table: admissions from C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv\n",
      "Scanning table: icustays from C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ICUSTAYS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ICUSTAYS.csv\n",
      "Scanning table: diagnoses_icd from C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\DIAGNOSES_ICD.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\DIAGNOSES_ICD.csv\n",
      "Joining with table: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv\n",
      "Scanning table: procedures_icd from C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\PROCEDURES_ICD.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\PROCEDURES_ICD.csv\n",
      "Joining with table: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv\n",
      "Scanning table: prescriptions from C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\PRESCRIPTIONS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\PRESCRIPTIONS.csv\n",
      "Joining with table: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv.gz\n",
      "Original path does not exist. Using alternative: C:\\Users\\johnn\\PyHealth_Data\\Synthetic_MIMIC-III\\ADMISSIONS.csv\n",
      "Collecting global event dataframe...\n",
      "Collected dataframe with shape: (13030, 49)\n",
      "Dataset: mimic3\n",
      "Dev mode: False\n",
      "Number of patients: 100\n",
      "Number of events: 13030\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johnn\\anaconda3\\envs\\pyhealth_stable_env\\lib\\site-packages\\pyhealth\\datasets\\mimic3.py:50: UserWarning: Events from prescriptions table only have date timestamp (no specific time). This may affect temporal ordering of events.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Import pyhealth \n",
    "import numpy as np\n",
    "import torch\n",
    "from pyhealth.datasets import MIMIC3Dataset \n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.tasks.mortality_prediction import MortalityPredictionMIMIC3\n",
    "from pyhealth.models import Transformer\n",
    "from pyhealth.trainer import Trainer\n",
    "from pyhealth.interpret.methods import CheferRelevance\n",
    "\n",
    "\n",
    "#Load the data\n",
    "dataset = MIMIC3Dataset(\n",
    "    root=r\"C:/Users/johnn/PyHealth_Data/Synthetic_MIMIC-III\",\n",
    "    tables=[\"diagnoses_icd\", \"procedures_icd\", \"prescriptions\"],\n",
    "    dev=False, \n",
    ")\n",
    "print(dataset.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74742dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting task MortalityPredictionMIMIC3 for mimic3 base dataset...\n",
      "Generating samples with 1 worker(s)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for MortalityPredictionMIMIC3 with 1 worker: 100%|██████████| 100/100 [00:00<00:00, 480.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mortality vocab: {0: 0, 1: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing samples: 100%|██████████| 26/26 [00:00<00:00, 13334.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 26 samples for task MortalityPredictionMIMIC3\n",
      "VISIT SAMPLE INSPECTION (Patient: 10088)\n",
      "Patient ID:    10088\n",
      "\n",
      "INPUT(Codes from the current visit):\n",
      "CONDITIONS (Diagnoses, ICD-9):\n",
      "Total Codes: 17\n",
      "Codes (Token IDs): tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "PROCEDURES (ICD-9):\n",
      "Total Codes: 5\n",
      "Codes (Token IDs): tensor([1, 2, 3, 4, 5])\n",
      "DRUGS (Prescriptions):\n",
      "Total Codes: 94\n",
      "Codes (Token IDs): tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 11, 20, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 29, 30, 29, 28,\n",
      "        29, 31, 32, 28, 30, 33, 33, 28, 28, 34, 20, 23, 28, 29, 28, 32, 28, 28,\n",
      "        35, 36, 34, 37, 36, 36, 26, 33, 38, 39, 40, 37, 41, 42, 43, 34, 23, 23,\n",
      "        44, 44, 32, 45, 45, 46, 37, 36, 34, 19, 47, 28, 29, 48, 16, 37, 42, 43,\n",
      "        34, 23, 28, 34])\n",
      "\n",
      "OUTPUT\n",
      "MORTALITY: 0.0 (0 = Patient survived in next visit, 1 = Mortality in next visit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.tasks import MortalityPredictionMIMIC3\n",
    "\n",
    "# Define the mortality prediction task\n",
    "task = MortalityPredictionMIMIC3()\n",
    "\n",
    "samples = dataset.set_task(task)\n",
    "\n",
    "#Take a look at random sample -> run again for other patients to check their mortality\n",
    "randomsample = samples.samples[0]\n",
    "device = randomsample['mortality'].device.type if isinstance(randomsample['mortality'], torch.Tensor) else 'cpu'\n",
    "\n",
    "print(f\"VISIT SAMPLE INSPECTION (Patient: {randomsample['patient_id']})\")\n",
    "print(f\"Patient ID:    {randomsample['patient_id']}\")\n",
    "\n",
    "print(\"\\nINPUT(Codes from the current visit):\")\n",
    "\n",
    "# CONDITIONS (Diagnoses)\n",
    "conditions = randomsample['conditions']\n",
    "print(f\"CONDITIONS (Diagnoses, ICD-9):\")\n",
    "print(f\"Total Codes: {len(conditions)}\")\n",
    "print(f\"Codes (Token IDs): {conditions}\")\n",
    "\n",
    "# PROCEDURES\n",
    "procedures = randomsample['procedures']\n",
    "print(f\"PROCEDURES (ICD-9):\")\n",
    "print(f\"Total Codes: {len(procedures)}\")\n",
    "print(f\"Codes (Token IDs): {procedures}\")\n",
    "\n",
    "# DRUGS\n",
    "drugs = randomsample['drugs']\n",
    "print(f\"DRUGS (Prescriptions):\")\n",
    "print(f\"Total Codes: {len(drugs)}\")\n",
    "print(f\"Codes (Token IDs): {drugs}\")\n",
    "\n",
    "\n",
    "print(\"\\nOUTPUT\")\n",
    "mortality_label = randomsample['mortality'].item() if isinstance(randomsample['mortality'], torch.Tensor) else randomsample['mortality']\n",
    "print(f\"MORTALITY: {mortality_label} (0 = Patient survived in next visit, 1 = Mortality in next visit)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c17994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples Processed: 26\n",
      "Patients Died: 5.0\n",
      "Patients Survived : 21.0\n",
      "Mortality Rate: 19.230769230769234\n",
      "Patient IDs who died: ['40310', '10059', '10124', '10094', '42135']\n"
     ]
    }
   ],
   "source": [
    "#Check Mortality statistics\n",
    "import torch\n",
    "\n",
    "mortality_count = 0\n",
    "total_samples = len(samples)\n",
    "deceased_patient_ids = set() \n",
    "\n",
    "for sample in samples.samples:\n",
    "    mortality_label = sample.get('mortality')\n",
    "    if isinstance(mortality_label, torch.Tensor):\n",
    "        mortality_value = mortality_label.item()\n",
    "    else:\n",
    "        mortality_value = float(mortality_label)\n",
    "    mortality_count += mortality_value\n",
    "    if mortality_value == 1.0:\n",
    "        deceased_patient_ids.add(sample.get('patient_id'))\n",
    "\n",
    "print(f\"Total Samples Processed: {total_samples}\")\n",
    "print(f\"Patients Died: {mortality_count}\")\n",
    "print(f\"Patients Survived : {total_samples - mortality_count}\")\n",
    "print(f\"Mortality Rate: {mortality_count / total_samples * 100}\")\n",
    "print(f\"Patient IDs who died: {list(deceased_patient_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cfd99f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyhealth.datasets import split_by_sample\n",
    "from pyhealth.datasets import get_dataloader\n",
    "\n",
    "#Split the data for training, validation, and testing\n",
    "train_dataset, val_dataset, test_dataset = split_by_sample(dataset = samples, ratios = [0.8, 0.1, 0.1])\n",
    "\n",
    "#Define dataloaders for training\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7c2eb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding_model): EmbeddingModel(embedding_layers=ModuleDict(\n",
      "    (conditions): Embedding(191, 128, padding_idx=0)\n",
      "    (procedures): Embedding(46, 128, padding_idx=0)\n",
      "    (drugs): Embedding(298, 128, padding_idx=0)\n",
      "  ))\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(heads=2, d_model=128, dropout=0.1)\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(heads=2, d_model=128, dropout=0.1)\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (drugs): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(heads=2, d_model=128, dropout=0.1)\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Initialize the Transformer Model from pyhealth \n",
    "\n",
    "model = Transformer( \n",
    "    dataset=samples,\n",
    "    embedding_dim=128,\n",
    "    heads=2,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b59b5b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (embedding_model): EmbeddingModel(embedding_layers=ModuleDict(\n",
      "    (conditions): Embedding(191, 128, padding_idx=0)\n",
      "    (procedures): Embedding(46, 128, padding_idx=0)\n",
      "    (drugs): Embedding(298, 128, padding_idx=0)\n",
      "  ))\n",
      "  (transformer): ModuleDict(\n",
      "    (conditions): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(heads=2, d_model=128, dropout=0.1)\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (procedures): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(heads=2, d_model=128, dropout=0.1)\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (drugs): TransformerLayer(\n",
      "      (transformer): ModuleList(\n",
      "        (0-1): 2 x TransformerBlock(\n",
      "          (attention): MultiHeadedAttention(heads=2, d_model=128, dropout=0.1)\n",
      "          (feed_forward): PositionwiseFeedForward(\n",
      "            (w_1): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (w_2): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "          (input_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output_sublayer): SublayerConnection(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: ['roc_auc', 'pr_auc', 'accuracy', 'f1']\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Initialize Trainer\n",
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\", \"pr_auc\", \"accuracy\", \"f1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ea21358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Batch size: 2\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x0000018955783370>\n",
      "Monitor: roc_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "Patience: None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5: 100%|██████████| 10/10 [00:00<00:00, 25.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-10 ---\n",
      "loss: 0.7089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 145.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-0, step-10 ---\n",
      "roc_auc: 0.5000\n",
      "pr_auc: 0.5000\n",
      "accuracy: 0.6667\n",
      "f1: 0.0000\n",
      "loss: 0.5723\n",
      "New best roc_auc score (0.5000) at epoch-0, step-10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 5: 100%|██████████| 10/10 [00:00<00:00, 33.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-20 ---\n",
      "loss: 0.4792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 177.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-1, step-20 ---\n",
      "roc_auc: 0.5000\n",
      "pr_auc: 0.5000\n",
      "accuracy: 0.6667\n",
      "f1: 0.0000\n",
      "loss: 0.5503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 / 5: 100%|██████████| 10/10 [00:00<00:00, 30.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-30 ---\n",
      "loss: 0.3979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 194.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-2, step-30 ---\n",
      "roc_auc: 0.5000\n",
      "pr_auc: 0.5000\n",
      "accuracy: 0.6667\n",
      "f1: 0.0000\n",
      "loss: 0.5531\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 / 5: 100%|██████████| 10/10 [00:00<00:00, 27.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-40 ---\n",
      "loss: 0.2630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 214.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-3, step-40 ---\n",
      "roc_auc: 1.0000\n",
      "pr_auc: 1.0000\n",
      "accuracy: 0.6667\n",
      "f1: 0.0000\n",
      "loss: 0.5584\n",
      "New best roc_auc score (1.0000) at epoch-3, step-40\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 / 5: 100%|██████████| 10/10 [00:00<00:00, 29.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-50 ---\n",
      "loss: 0.2075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 150.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-4, step-50 ---\n",
      "roc_auc: 1.0000\n",
      "pr_auc: 1.0000\n",
      "accuracy: 0.6667\n",
      "f1: 0.0000\n",
      "loss: 0.5536\n",
      "Loaded best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Run the training\n",
    "trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=5,\n",
    "    monitor=\"roc_auc\",  \n",
    "    optimizer_params={\"lr\": 1e-4}, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ed9788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00, 145.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results:\n",
      "roc_auc: nan\n",
      "pr_auc: 0.0000\n",
      "accuracy: 1.0000\n",
      "f1: 0.0000\n",
      "loss: 0.1447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\johnn\\anaconda3\\envs\\pyhealth_stable_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:424: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "c:\\Users\\johnn\\anaconda3\\envs\\pyhealth_stable_env\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "c:\\Users\\johnn\\anaconda3\\envs\\pyhealth_stable_env\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the results\n",
    "results = trainer.evaluate(test_dataloader)\n",
    "print(\"Test Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ee8bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature keys: dict_keys(['conditions', 'procedures', 'drugs'])\n",
      "\n",
      "Top 5 Chefer Relevance for conditions:\n",
      "Token indices: [0, 6, 9, 5, 7]\n",
      "Scores: ['0.9340', '0.0160', '0.0147', '0.0096', '0.0084']\n",
      "\n",
      "Top 5 Normalized IG for conditions:\n",
      "Token indices: [0, 10, 8, 6, 3]\n",
      "Scores: ['0.3225', '0.0880', '0.0277', '0.0016', '-0.0121']\n",
      "\n",
      "Top 5 AttInWAvg for conditions:\n",
      "Token indices: [0, 10, 8, 6, 9]\n",
      "Scores: ['0.6283', '0.0441', '0.0144', '0.0088', '-0.0012']\n",
      "\n",
      "Top 5 Chefer Relevance for procedures:\n",
      "Token indices: [0, 1, 2, 3]\n",
      "Scores: ['0.9133', '0.0867', '0.0000', '0.0000']\n",
      "\n",
      "Top 5 Normalized IG for procedures:\n",
      "Token indices: [0, 2, 3, 1]\n",
      "Scores: ['0.7110', '-0.0033', '-0.0204', '-0.2653']\n",
      "\n",
      "Top 5 AttInWAvg for procedures:\n",
      "Token indices: [0, 2, 3, 1]\n",
      "Scores: ['0.8121', '-0.0016', '-0.0102', '-0.0893']\n",
      "\n",
      "Top 5 Chefer Relevance for drugs:\n",
      "Token indices: [0, 7, 78, 51, 32]\n",
      "Scores: ['0.9511', '0.0022', '0.0022', '0.0018', '0.0013']\n",
      "\n",
      "Top 5 Normalized IG for drugs:\n",
      "Token indices: [19, 17, 63, 68, 67]\n",
      "Scores: ['0.0366', '0.0324', '0.0259', '0.0252', '0.0238']\n",
      "\n",
      "Top 5 AttInWAvg for drugs:\n",
      "Token indices: [0, 19, 17, 63, 68]\n",
      "Scores: ['0.4829', '0.0185', '0.0165', '0.0129', '0.0127']\n"
     ]
    }
   ],
   "source": [
    "# Interpreting model predictions using Pyhealth's Chefer and IntegratedGradients\n",
    "# Our original ablation from the reproduced paper introduces the\n",
    "# AttInGrad-Weighted Average (AttInWAvg), defined as:\n",
    "# \n",
    "# AttInWAvg_j = λ · A_j  +  (1 − λ) · NormInputXGrad_j\n",
    "\n",
    "# Where:\n",
    "#   A_j = attention weight for token j\n",
    "#   NormInputXGrad_j = normalized input × gradient attribution\n",
    "#   λ ∈ [0, 1] = interpolation coefficient\n",
    "#\n",
    "# In this example, we replace the original attention weights A_j\n",
    "# with Chefer's relevance scores obtained from PyHealth's\n",
    "# CheferRelevance method, and we use PyHealth's IntegratedGradients\n",
    "# method to compute the gradient-based attributions. We normalize them both also for consistency\n",
    "#\n",
    "# The final weighted attribution becomes:\n",
    "#\n",
    "#       AttInWAvg_j = λ · NormChefer_j  +  (1 − λ) · NormInputXGrad_j\n",
    "#\n",
    "\n",
    "from pyhealth.interpret.methods import IntegratedGradients\n",
    "\n",
    "#1. Chefer\n",
    "chefer = CheferRelevance(model)\n",
    "batch = next(iter(val_dataloader))\n",
    "\n",
    "chefer_scores = chefer.attribute(\n",
    "    conditions=batch[\"conditions\"],\n",
    "    procedures=batch[\"procedures\"],\n",
    "    drugs=batch[\"drugs\"],\n",
    "    mortality=batch[\"mortality\"],\n",
    ")\n",
    "\n",
    "\n",
    "#2. Integrated Gradients\n",
    "ig = IntegratedGradients(model)\n",
    "\n",
    "ig_scores = ig.attribute(\n",
    "    conditions=batch[\"conditions\"],\n",
    "    procedures=batch[\"procedures\"],\n",
    "    drugs=batch[\"drugs\"],\n",
    "    mortality=batch[\"mortality\"],\n",
    "    steps=20,  \n",
    ")\n",
    "\n",
    "#3. Normalize chefer and IG scores\n",
    "norm_chefer = {}\n",
    "norm_ig = {}\n",
    "\n",
    "for key in chefer_scores:\n",
    "    c = chefer_scores[key][0]\n",
    "    g = ig_scores[key][0]\n",
    "\n",
    "    norm_chefer[key] = c / (c.abs().sum() + 1e-9)\n",
    "    norm_ig[key] = g / (g.abs().sum() + 1e-9)\n",
    "\n",
    "#4. Weighted average (AttInWAvg)\n",
    "lambda_val = 0.5\n",
    "AttInWAvg = {}\n",
    "\n",
    "for key in chefer_scores:\n",
    "    AttInWAvg[key] = lambda_val * norm_chefer[key] + (1 - lambda_val) * norm_ig[key]\n",
    "    \n",
    "# 5. Print top tokens for all 3 methods\n",
    "print(\"Feature keys:\", chefer_scores.keys())\n",
    "for key in chefer_scores:\n",
    "\n",
    "    c = norm_chefer[key]\n",
    "    top_c = torch.topk(c, k=min(5, c.size(0)))\n",
    "    print(f\"\\nTop 5 Chefer Relevance for {key}:\")\n",
    "    print(\"Token indices:\", top_c.indices.tolist())\n",
    "    print(\"Scores:\", [f\"{v:.4f}\" for v in top_c.values.tolist()])\n",
    "\n",
    "   \n",
    "    g = norm_ig[key]\n",
    "    top_g = torch.topk(g, k=min(5, g.size(0)))\n",
    "    print(f\"\\nTop 5 Normalized IG for {key}:\")\n",
    "    print(\"Token indices:\", top_g.indices.tolist())\n",
    "    print(\"Scores:\", [f\"{v:.4f}\" for v in top_g.values.tolist()])\n",
    "\n",
    "\n",
    "    w = AttInWAvg[key]\n",
    "    top_w = torch.topk(w, k=min(5, w.size(0)))\n",
    "    print(f\"\\nTop 5 AttInWAvg for {key}:\")\n",
    "    print(\"Token indices:\", top_w.indices.tolist())\n",
    "    print(\"Scores:\", [f\"{v:.4f}\" for v in top_w.values.tolist()])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhealth_stable_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
