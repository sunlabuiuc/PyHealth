{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conformal Prediction for COVID-19 Chest X-Ray Classification\n",
    "\n",
    "This example demonstrates:\n",
    "1. Training a ResNet-18 model on COVID-19 CXR dataset\n",
    "2. Conventional conformal prediction using LABEL\n",
    "3. Covariate shift adaptive conformal prediction using CovariateLabel\n",
    "4. Comparison of coverage and efficiency between the two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/PyHealth/pyhealth/sampler/sage_sampler.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "/home/ubuntu/PyHealth/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a warning of potentially slow compute. You could uncomment this line and use the Python implementation instead of Cython.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pyhealth.calib.predictionset import LABEL\n",
    "from pyhealth.calib.predictionset.covariate import CovariateLabel\n",
    "from pyhealth.calib.utils import extract_embeddings\n",
    "from pyhealth.datasets import (\n",
    "    COVID19CXRDataset,\n",
    "    get_dataloader,\n",
    "    split_by_sample_conformal,\n",
    ")\n",
    "from pyhealth.models import TorchvisionModel\n",
    "from pyhealth.trainer import Trainer, get_metrics_fn\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: Loading COVID-19 CXR Dataset\n",
      "================================================================================\n",
      "No config path provided, using default config\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Raw COVID-19 CXR dataset files not found in ~/Downloads/COVID-19_Radiography_Dataset. Please download the dataset from https://www.kaggle.com/api/v1/datasets/download/tawsifurrahman/covid19-radiography-database and extract the contents to the specified root directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m      5\u001b[39m root = \u001b[33m\"\u001b[39m\u001b[33m~/Downloads/COVID-19_Radiography_Dataset\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m base_dataset = \u001b[43mCOVID19CXRDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m sample_dataset = base_dataset.set_task(cache_dir=\u001b[33m\"\u001b[39m\u001b[33m../../covid19cxr_cache\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal samples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sample_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PyHealth/pyhealth/datasets/covid19_cxr.py:96\u001b[39m, in \u001b[36mCOVID19CXRDataset.__init__\u001b[39m\u001b[34m(self, root, dataset_name, config_path)\u001b[39m\n\u001b[32m     94\u001b[39m     config_path = Path(\u001b[34m__file__\u001b[39m).parent / \u001b[33m\"\u001b[39m\u001b[33mconfigs\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mcovid19_cxr.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_raw_data_exists(root):\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRaw COVID-19 CXR dataset files not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     98\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease download the dataset from \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://www.kaggle.com/api/v1/datasets/download/tawsifurrahman/covid19-radiography-database \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    100\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mand extract the contents to the specified root directory.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    101\u001b[39m     )\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(os.path.join(root, \u001b[33m\"\u001b[39m\u001b[33mcovid19_cxr-metadata-pyhealth.csv\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28mself\u001b[39m.prepare_metadata(root)\n",
      "\u001b[31mValueError\u001b[39m: Raw COVID-19 CXR dataset files not found in ~/Downloads/COVID-19_Radiography_Dataset. Please download the dataset from https://www.kaggle.com/api/v1/datasets/download/tawsifurrahman/covid19-radiography-database and extract the contents to the specified root directory."
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: Loading COVID-19 CXR Dataset\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "root = \"~/Downloads/COVID-19_Radiography_Dataset\"\n",
    "base_dataset = COVID19CXRDataset(root)\n",
    "sample_dataset = base_dataset.set_task(cache_dir=\"../../covid19cxr_cache\")\n",
    "\n",
    "print(f\"Total samples: {len(sample_dataset)}\")\n",
    "print(f\"Task mode: {sample_dataset.output_schema}\")\n",
    "\n",
    "# Split into train/val/cal/test\n",
    "# For conformal prediction, we need a separate calibration set\n",
    "train_data, val_data, cal_data, test_data = split_by_sample_conformal(\n",
    "    dataset=sample_dataset, ratios=[0.6, 0.1, 0.15, 0.15]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_data)}\")\n",
    "print(f\"Val: {len(val_data)}\")\n",
    "print(f\"Cal: {len(cal_data)} (for conformal calibration)\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = get_dataloader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = get_dataloader(val_data, batch_size=32, shuffle=False)\n",
    "cal_loader = get_dataloader(cal_data, batch_size=32, shuffle=False)\n",
    "test_loader = get_dataloader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Train ResNet-18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: Training ResNet-18 Model\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize ResNet-18 with pretrained weights\n",
    "resnet = TorchvisionModel(\n",
    "    dataset=sample_dataset,\n",
    "    model_name=\"resnet18\",\n",
    "    model_config={\"weights\": \"DEFAULT\"},\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "trainer = Trainer(model=resnet, device=device)\n",
    "\n",
    "print(f\"Training on device: {device}\")\n",
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=5,\n",
    "    monitor=\"accuracy\",\n",
    ")\n",
    "\n",
    "print(\"✓ Model training completed\")\n",
    "\n",
    "# Evaluate base model on test set\n",
    "print(\"\\nBase model performance on test set:\")\n",
    "y_true_base, y_prob_base, loss_base = trainer.inference(test_loader)\n",
    "base_metrics = get_metrics_fn(\"multiclass\")(\n",
    "    y_true_base, y_prob_base, metrics=[\"accuracy\", \"f1_weighted\"]\n",
    ")\n",
    "for metric, value in base_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Conventional Conformal Prediction with LABEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 3: Conventional Conformal Prediction (LABEL)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Target miscoverage rate of 10% (90% coverage)\n",
    "alpha = 0.1\n",
    "print(f\"Target miscoverage rate: {alpha} (90% coverage)\")\n",
    "\n",
    "# Create LABEL predictor\n",
    "label_predictor = LABEL(model=resnet, alpha=alpha)\n",
    "\n",
    "# Calibrate on calibration set\n",
    "print(\"Calibrating LABEL predictor...\")\n",
    "label_predictor.calibrate(cal_dataset=cal_data)\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating LABEL predictor on test set...\")\n",
    "y_true_label, y_prob_label, _, extra_label = Trainer(model=label_predictor).inference(\n",
    "    test_loader, additional_outputs=[\"y_predset\"]\n",
    ")\n",
    "\n",
    "label_metrics = get_metrics_fn(\"multiclass\")(\n",
    "    y_true_label,\n",
    "    y_prob_label,\n",
    "    metrics=[\"accuracy\", \"miscoverage_ps\"],\n",
    "    y_predset=extra_label[\"y_predset\"],\n",
    ")\n",
    "\n",
    "# Calculate average set size\n",
    "predset_label = (\n",
    "    torch.tensor(extra_label[\"y_predset\"])\n",
    "    if isinstance(extra_label[\"y_predset\"], np.ndarray)\n",
    "    else extra_label[\"y_predset\"]\n",
    ")\n",
    "avg_set_size_label = predset_label.float().sum(dim=1).mean().item()\n",
    "\n",
    "# Extract scalar values from metrics (handle both scalar and array returns)\n",
    "miscoverage_label = label_metrics[\"miscoverage_ps\"]\n",
    "if isinstance(miscoverage_label, np.ndarray):\n",
    "    miscoverage_label = float(\n",
    "        miscoverage_label.item()\n",
    "        if miscoverage_label.size == 1\n",
    "        else miscoverage_label.mean()\n",
    "    )\n",
    "else:\n",
    "    miscoverage_label = float(miscoverage_label)\n",
    "\n",
    "print(\"\\nLABEL Results:\")\n",
    "print(f\"  Accuracy: {label_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Empirical miscoverage: {miscoverage_label:.4f}\")\n",
    "print(f\"  Average set size: {avg_set_size_label:.2f}\")\n",
    "print(f\"  Target miscoverage: {alpha:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 4: Covariate Shift Adaptive Conformal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 4: Covariate Shift Adaptive Conformal Prediction\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Extract embeddings from the model\n",
    "# For TorchvisionModel, we extract features from avgpool layer (before fc)\n",
    "print(\"Extracting embeddings from calibration set...\")\n",
    "cal_embeddings = extract_embeddings(resnet, cal_data, batch_size=32, device=device)\n",
    "print(f\"  Cal embeddings shape: {cal_embeddings.shape}\")\n",
    "\n",
    "print(\"Extracting embeddings from test set...\")\n",
    "test_embeddings = extract_embeddings(resnet, test_data, batch_size=32, device=device)\n",
    "print(f\"  Test embeddings shape: {test_embeddings.shape}\")\n",
    "\n",
    "# Create CovariateLabel predictor\n",
    "print(\"\\nCreating CovariateLabel predictor...\")\n",
    "covariate_predictor = CovariateLabel(model=resnet, alpha=alpha)\n",
    "\n",
    "# Calibrate with embeddings (KDEs will be fitted automatically)\n",
    "print(\"Calibrating CovariateLabel predictor...\")\n",
    "print(\"  - Fitting KDEs for covariate shift correction...\")\n",
    "covariate_predictor.calibrate(\n",
    "    cal_dataset=cal_data, cal_embeddings=cal_embeddings, test_embeddings=test_embeddings\n",
    ")\n",
    "print(\"✓ Calibration completed\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating CovariateLabel predictor on test set...\")\n",
    "y_true_cov, y_prob_cov, _, extra_cov = Trainer(model=covariate_predictor).inference(\n",
    "    test_loader, additional_outputs=[\"y_predset\"]\n",
    ")\n",
    "\n",
    "cov_metrics = get_metrics_fn(\"multiclass\")(\n",
    "    y_true_cov,\n",
    "    y_prob_cov,\n",
    "    metrics=[\"accuracy\", \"miscoverage_ps\"],\n",
    "    y_predset=extra_cov[\"y_predset\"],\n",
    ")\n",
    "\n",
    "# Calculate average set size\n",
    "predset_cov = (\n",
    "    torch.tensor(extra_cov[\"y_predset\"])\n",
    "    if isinstance(extra_cov[\"y_predset\"], np.ndarray)\n",
    "    else extra_cov[\"y_predset\"]\n",
    ")\n",
    "avg_set_size_cov = predset_cov.float().sum(dim=1).mean().item()\n",
    "\n",
    "# Extract scalar values from metrics (handle both scalar and array returns)\n",
    "miscoverage_cov = cov_metrics[\"miscoverage_ps\"]\n",
    "if isinstance(miscoverage_cov, np.ndarray):\n",
    "    miscoverage_cov = float(\n",
    "        miscoverage_cov.item() if miscoverage_cov.size == 1 else miscoverage_cov.mean()\n",
    "    )\n",
    "else:\n",
    "    miscoverage_cov = float(miscoverage_cov)\n",
    "\n",
    "print(\"\\nCovariateLabel Results:\")\n",
    "print(f\"  Accuracy: {cov_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Empirical miscoverage: {miscoverage_cov:.4f}\")\n",
    "print(f\"  Average set size: {avg_set_size_cov:.2f}\")\n",
    "print(f\"  Target miscoverage: {alpha:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 5: Compare Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 5: Comparison of Methods\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTarget: {1-alpha:.0%} coverage (max {alpha:.0%} miscoverage)\")\n",
    "print(\"\\n{:<40} {:<15} {:<15}\".format(\"Metric\", \"LABEL\", \"CovariateLabel\"))\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Coverage (1 - miscoverage)\n",
    "label_coverage = 1 - miscoverage_label\n",
    "cov_coverage = 1 - miscoverage_cov\n",
    "print(\n",
    "    \"{:<40} {:<15.2%} {:<15.2%}\".format(\n",
    "        \"Empirical Coverage\", label_coverage, cov_coverage\n",
    "    )\n",
    ")\n",
    "\n",
    "# Miscoverage\n",
    "print(\n",
    "    \"{:<40} {:<15.4f} {:<15.4f}\".format(\n",
    "        \"Empirical Miscoverage\",\n",
    "        miscoverage_label,\n",
    "        miscoverage_cov,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Average set size (smaller is better for efficiency)\n",
    "print(\n",
    "    \"{:<40} {:<15.2f} {:<15.2f}\".format(\n",
    "        \"Average Set Size\", avg_set_size_label, avg_set_size_cov\n",
    "    )\n",
    ")\n",
    "\n",
    "# Efficiency (inverse of average set size)\n",
    "efficiency_label = 1.0 / avg_set_size_label\n",
    "efficiency_cov = 1.0 / avg_set_size_cov\n",
    "print(\n",
    "    \"{:<40} {:<15.4f} {:<15.4f}\".format(\n",
    "        \"Efficiency (1/avg_set_size)\", efficiency_label, efficiency_cov\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Summary\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"1. Both methods achieve near-target coverage guarantees\")\n",
    "print(\"2. LABEL: Standard conformal prediction\")\n",
    "print(\"3. CovariateLabel: Adapts to distribution shift between cal and test\")\n",
    "print(\"\\nWhen to use CovariateLabel:\")\n",
    "print(\"  - When test distribution differs from calibration distribution\")\n",
    "print(\"  - When you have access to test embeddings/features\")\n",
    "print(\"  - When you want more robust coverage under distribution shift\")\n",
    "print(\"\\nWhen to use LABEL:\")\n",
    "print(\"  - When cal and test distributions are similar (exchangeable)\")\n",
    "print(\"  - Simpler method, no need to fit KDEs\")\n",
    "print(\"  - Computationally more efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 6: Visualize Prediction Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 6: Example Predictions\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show first 5 test examples\n",
    "n_examples = 5\n",
    "print(f\"\\nShowing first {n_examples} test examples:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i in range(min(n_examples, len(y_true_label))):\n",
    "    true_class = int(y_true_label[i])\n",
    "\n",
    "    # LABEL prediction set\n",
    "    if isinstance(predset_label, np.ndarray):\n",
    "        label_set = np.where(predset_label[i])[0]\n",
    "    else:\n",
    "        label_set = torch.where(predset_label[i])[0].cpu().numpy()\n",
    "\n",
    "    # CovariateLabel prediction set\n",
    "    if isinstance(predset_cov, np.ndarray):\n",
    "        cov_set = np.where(predset_cov[i])[0]\n",
    "    else:\n",
    "        cov_set = torch.where(predset_cov[i])[0].cpu().numpy()\n",
    "\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  True class: {true_class}\")\n",
    "    print(f\"  LABEL set: {label_set.tolist()} (size: {len(label_set)})\")\n",
    "    print(f\"  CovariateLabel set: {cov_set.tolist()} (size: {len(cov_set)})\")\n",
    "    print(f\"  Correct in LABEL? {true_class in label_set}\")\n",
    "    print(f\"  Correct in CovariateLabel? {true_class in cov_set}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Example completed successfully!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
