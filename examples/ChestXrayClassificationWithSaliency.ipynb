{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7e5b21",
   "metadata": {},
   "source": [
    "# Medical Image Classification with PyHealth\n",
    "\n",
    "Welcome to the PyHealth tutorial on image classification and saliency mapping. In this notebook, we will explore how to use PyHealth to analyze chest X-ray images, classify them into various chest diseases, and visualize the model's decision-making process using gradient saliency maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519fe4c",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39fafe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mne pandarallel rdkit transformers torch torchvision openpyxl polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82593a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf PyHealth\n",
    "# !git clone https://github.com/sunlabuiuc/PyHealth.git\n",
    "!git clone -b SaliencyMappingClass https://github.com/Nimanui/PyHealth-fitzpa15.git PyHealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./PyHealth\")\n",
    "sys.path.append(\"./PyHealth-fitzpa15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67302afe",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Next, we will download the dataset containing COVID-19 data. This dataset includes chest X-ray images of normal cases, lung opacity, viral pneumonia, and COVID-19 patients. You can find more information about the dataset [here](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32539a",
   "metadata": {},
   "source": [
    "Download and extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://storage.googleapis.com/pyhealth/covid19_cxr_data/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q -o archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -1 COVID-19_Radiography_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccb47d",
   "metadata": {},
   "source": [
    "Next, we will proceed with the chest X-ray classification task using PyHealth, following a five-stage pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ecc90",
   "metadata": {},
   "source": [
    "## Step 1. Load Data in PyHealth\n",
    "\n",
    "The initial step involves loading the data into PyHealth's internal structure. This process is straightforward: import the appropriate dataset class from PyHealth and specify the root directory where the raw dataset is stored. PyHealth will handle the dataset processing automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import COVID19CXRDataset\n",
    "\n",
    "root = \"COVID-19_Radiography_Dataset\"\n",
    "base_dataset = COVID19CXRDataset(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04133288",
   "metadata": {},
   "source": [
    "Once the data is loaded, we can perform simple queries on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8889c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f244846",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.get_patient(\"0\").get_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241e29a",
   "metadata": {},
   "source": [
    "## Step 2. Define the Task\n",
    "\n",
    "The next step is to define the machine learning task. This step instructs the package to generate a list of samples with the desired features and labels based on the data for each individual patient. Please note that in this dataset, patient identification information is not available. Therefore, we will assume that each chest X-ray belongs to a unique patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16514220",
   "metadata": {},
   "source": [
    "For this dataset, PyHealth offers a default task specifically for chest X-ray classification. This task takes the image as input and aims to predict the chest diseases associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.default_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc161dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = base_dataset.set_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e56f9",
   "metadata": {},
   "source": [
    "Here is an example of a single sample, represented as a dictionary. The dictionary contains keys for feature names, label names, and other metadata associated with the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3fa92",
   "metadata": {},
   "source": [
    "We can also check the input and output schemas, which specify the data types of the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset.input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset.output_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b356f30",
   "metadata": {},
   "source": [
    "Below, we plot the number of samples per classes, and visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = sample_dataset.output_processors[\"disease\"].label_vocab\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd51e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_counts = defaultdict(int)\n",
    "for sample in sample_dataset.samples:\n",
    "    label_counts[id2label[sample[\"disease\"].item()]] += 1\n",
    "print(label_counts)\n",
    "plt.bar(label_counts.keys(), label_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "label_to_idxs = defaultdict(list)\n",
    "for idx, sample in enumerate(sample_dataset.samples):\n",
    "    label_to_idxs[sample[\"disease\"].item()].append(idx)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 3))\n",
    "for ax, label in zip(axs, label_to_idxs.keys()):\n",
    "    ax.set_title(id2label[label], fontsize=15)\n",
    "    idx = random.choice(label_to_idxs[label])\n",
    "    sample = sample_dataset[idx]\n",
    "    image = sample[\"image\"][0]\n",
    "    ax.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d4c95",
   "metadata": {},
   "source": [
    "Finally, we will split the entire dataset into training, validation, and test sets using the ratios of 70%, 10%, and 20%, respectively. We will then obtain the corresponding data loaders for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666cc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_sample\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_sample(\n",
    "    dataset=sample_dataset,\n",
    "    ratios=[0.7, 0.1, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import get_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54353621",
   "metadata": {},
   "source": [
    "## Step 3. Define the Model\n",
    "\n",
    "Next, we will define the deep learning model we want to use for our task. PyHealth supports all major vision models available in the Torchvision package. You can load any of these models using the model_name argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import TorchvisionModel\n",
    "\n",
    "resnet = TorchvisionModel(\n",
    "    dataset=sample_dataset,\n",
    "    model_name=\"resnet18\",\n",
    "    model_config={\"weights\": \"DEFAULT\"}\n",
    ")\n",
    "\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import TorchvisionModel\n",
    "\n",
    "vit = TorchvisionModel(\n",
    "    dataset=sample_dataset,\n",
    "    model_name=\"vit_b_16\",\n",
    "    model_config={\"weights\": \"DEFAULT\"}\n",
    ")\n",
    "\n",
    "vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdccc3c",
   "metadata": {},
   "source": [
    "## Step 4. Training\n",
    "\n",
    "In this step, we will train the model using PyHealth's Trainer class, which simplifies the training process and provides standard functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bddb0",
   "metadata": {},
   "source": [
    "Let us first train the ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "resnet_trainer = Trainer(model=resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712fc710",
   "metadata": {},
   "source": [
    "Before we begin training, let's first evaluate the initial performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet_trainer.evaluate(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc22f4a",
   "metadata": {},
   "source": [
    "Now, let's start the training process. Due to computational constraints, we will train the model for only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a18319",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=1,\n",
    "    monitor=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be6586",
   "metadata": {},
   "source": [
    "After training the model, we can compare its performance before and after. We should expect to see an increase in the accuracy score as the model learns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6176aa1",
   "metadata": {},
   "source": [
    "## Step 5. Evaluation\n",
    "\n",
    "Lastly, we can evaluate the ResNet model on the test set. This can be done using PyHealth's `Trainer.evaluate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet_trainer.evaluate(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc37c6",
   "metadata": {},
   "source": [
    "Additionally, you can perform inference using the `Trainer.inference()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob, loss = resnet_trainer.inference(test_dataloader)\n",
    "y_pred = y_prob.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cbcba",
   "metadata": {},
   "source": [
    "Below we show a confusion matrix of the trained ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax = sns.heatmap(cf_matrix, linewidths=1, annot=True, fmt='g')\n",
    "ax.set_xticklabels([id2label[i] for i in range(4)])\n",
    "ax.set_yticklabels([id2label[i] for i in range(4)])\n",
    "ax.set_xlabel(\"Pred\")\n",
    "ax.set_ylabel(\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89316531",
   "metadata": {},
   "source": [
    "# 6 Gradient Saliency Mapping\n",
    "For a bonus let's look at some simple gradient saliency maps applied to our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_requires_grad(in_dataset):\n",
    "  for sample in in_dataset:\n",
    "    sample['image'].requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e87796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.interpret.methods.basic_gradient import BasicGradientSaliencyMaps\n",
    "from pyhealth.interpret.methods import SaliencyVisualizer\n",
    "import torch\n",
    "\n",
    "# Create a batch with only COVID samples\n",
    "covid_label = label2id['COVID']\n",
    "covid_samples = [sample for sample in sample_dataset.samples if sample['disease'].item() == covid_label]\n",
    "\n",
    "# Take the first 32 COVID samples and create a batch\n",
    "batch_size = min(32, len(covid_samples))\n",
    "covid_batch = {\n",
    "    'image': torch.stack([covid_samples[i]['image'] for i in range(batch_size)]),\n",
    "    'disease': torch.stack([covid_samples[i]['disease'] for i in range(batch_size)])\n",
    "}\n",
    "\n",
    "print(f\"Created COVID batch with {batch_size} samples\")\n",
    "\n",
    "# Initialize saliency maps with batch input only\n",
    "saliency_maps = BasicGradientSaliencyMaps(\n",
    "    resnet,\n",
    "    input_batch=covid_batch\n",
    ")\n",
    "\n",
    "# Initialize the visualization module with correct parameter names\n",
    "visualizer = SaliencyVisualizer(default_cmap='hot', default_alpha=0.6, figure_size=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc05ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show saliency map for the first image in the batch\n",
    "image_0 = covid_batch['image'][0]\n",
    "# Compute saliency for single image using attribute method\n",
    "saliency_result_0 = saliency_maps.attribute(image=image_0.unsqueeze(0), disease=covid_batch['disease'][0:1])\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt, \n",
    "    image=image_0, \n",
    "    saliency=saliency_result_0['image'][0],\n",
    "    title=f\"Gradient Saliency - {id2label[covid_label]} (Sample 0)\"\n",
    ")\n",
    "\n",
    "# Show saliency map for another image in the batch\n",
    "image_3 = covid_batch['image'][3]\n",
    "saliency_result_3 = saliency_maps.attribute(image=image_3.unsqueeze(0), disease=covid_batch['disease'][3:4])\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt, \n",
    "    image=image_3, \n",
    "    saliency=saliency_result_3['image'][0],\n",
    "    title=f\"Gradient Saliency - {id2label[covid_label]} (Sample 3)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a5907",
   "metadata": {},
   "source": [
    "# 7. Layer-wise Relevance Propagation (LRP)\n",
    "\n",
    "LRP is a powerful interpretability method that explains neural network predictions by propagating relevance scores backward through the network. Unlike gradient-based methods, LRP satisfies the conservation property: the sum of relevances at the input layer approximately equals the model's output for the target class.\n",
    "\n",
    "**New Implementation**: PyHealth now includes **UnifiedLRP** - a modular implementation supporting both CNNs and embedding-based models with 11 layer handlers including Conv2d, MaxPool2d, BatchNorm2d, and more!\n",
    "\n",
    "Let's demonstrate LRP on our ResNet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyhealth.interpret.methods import UnifiedLRP\n",
    "\n",
    "# Suppress conservation warnings for cleaner output\n",
    "logging.getLogger('pyhealth.interpret.methods.lrp_base').setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize UnifiedLRP with epsilon rule\n",
    "lrp = UnifiedLRP(\n",
    "    model=resnet.model,  # Use the underlying PyTorch model\n",
    "    rule='epsilon',\n",
    "    epsilon=0.1,  # Larger epsilon for numerical stability\n",
    "    validate_conservation=False\n",
    ")\n",
    "\n",
    "# Compute LRP attributions for the first COVID sample\n",
    "print(f\"Computing LRP attributions for COVID-19 sample...\")\n",
    "covid_image = covid_batch['image'][0:1]\n",
    "\n",
    "# Convert grayscale to RGB (ResNet expects 3 channels)\n",
    "if covid_image.shape[1] == 1:\n",
    "    covid_image = covid_image.repeat(1, 3, 1, 1)\n",
    "\n",
    "# Move to the same device as the model\n",
    "device = next(resnet.model.parameters()).device\n",
    "covid_image = covid_image.to(device)\n",
    "\n",
    "# Forward pass to get prediction\n",
    "with torch.no_grad():\n",
    "    output = resnet.model(covid_image)\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "# Compute LRP attributions\n",
    "lrp_attributions = lrp.attribute(\n",
    "    inputs={'x': covid_image},\n",
    "    target_class=predicted_class\n",
    ")\n",
    "\n",
    "print(f\"‚úì LRP attributions computed!\")\n",
    "print(f\"  Input shape: {covid_image.shape}\")\n",
    "print(f\"  Attribution shape: {lrp_attributions['x'].shape}\")\n",
    "print(f\"  Predicted class: {id2label[predicted_class]}\")\n",
    "print(f\"  Total relevance: {lrp_attributions['x'].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abc71d",
   "metadata": {},
   "source": [
    "## Visualizing LRP Results\n",
    "\n",
    "LRP provides pixel-level explanations showing which image regions contributed to the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LRP relevance map\n",
    "relevance_map = lrp_attributions['x'].squeeze()\n",
    "\n",
    "# For visualization, use the first channel (all channels are the same for grayscale)\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],  # Original grayscale image\n",
    "    saliency=relevance_map[0] if relevance_map.dim() == 3 else relevance_map,  # First channel of attribution\n",
    "    title=f\"LRP Relevance Map - {id2label[predicted_class]} (Epsilon Rule)\",\n",
    ")\n",
    "\n",
    "# Also show gradient saliency for comparison\n",
    "saliency_comparison = saliency_maps.attribute(image=covid_batch['image'][0:1], disease=covid_batch['disease'][0:1])\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    saliency=saliency_comparison['image'][0],\n",
    "    title=f\"Gradient Saliency (for comparison) - {id2label[predicted_class]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0c20c",
   "metadata": {},
   "source": [
    "## Comparing Different LRP Rules\n",
    "\n",
    "LRP supports different propagation rules that handle positive and negative contributions differently:\n",
    "\n",
    "**Epsilon Rule (`rule=\"epsilon\"`):**\n",
    "- Adds a small stabilizer Œµ to prevent division by zero\n",
    "- Best for: General use, numerical stability\n",
    "- Good for layers where both positive and negative activations matter equally\n",
    "- Conservation violations: 5-50% (acceptable)\n",
    "\n",
    "**Alpha-Beta Rule (`rule=\"alphabeta\"`):**\n",
    "- Separates positive and negative contributions with different weights (Œ± and Œ≤)\n",
    "- Default: Œ±=2, Œ≤=1 (emphasizes positive contributions)\n",
    "- Best for: When you want to focus on excitatory (positive) evidence\n",
    "- Often produces sharper, more focused heatmaps\n",
    "- Conservation violations: 50-150% (acceptable)\n",
    "\n",
    "Let's compare both rules on the same image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd47895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon rule (already computed)\n",
    "print(\"LRP with Epsilon Rule (Œµ=0.1)\")\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    saliency=relevance_map[0] if relevance_map.dim() == 3 else relevance_map,\n",
    "    title=f\"LRP Epsilon Rule - {id2label[predicted_class]}\",\n",
    ")\n",
    "\n",
    "# Now compute LRP with Alpha-Beta Rule\n",
    "print(\"\\nComputing LRP with Alpha-Beta Rule (Œ±=2, Œ≤=1)...\")\n",
    "lrp_alphabeta = UnifiedLRP(\n",
    "    model=resnet.model,\n",
    "    rule='alphabeta',\n",
    "    alpha=2.0,\n",
    "    beta=1.0,\n",
    "    validate_conservation=False\n",
    ")\n",
    "\n",
    "alphabeta_attributions = lrp_alphabeta.attribute(\n",
    "    inputs={'x': covid_image},\n",
    "    target_class=predicted_class\n",
    ")\n",
    "\n",
    "alphabeta_relevance = alphabeta_attributions['x'].squeeze()\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    saliency=alphabeta_relevance[0] if alphabeta_relevance.dim() == 3 else alphabeta_relevance,\n",
    "    title=f\"LRP Alpha-Beta Rule (Œ±=2, Œ≤=1) - {id2label[predicted_class]}\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Results:\")\n",
    "print(f\"  Epsilon Rule - Total relevance: {lrp_attributions['x'].sum().item():.4f}\")\n",
    "print(f\"  Alpha-Beta Rule - Total relevance: {alphabeta_attributions['x'].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706a7c5",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison of All Interpretation Methods\n",
    "\n",
    "Let's create a comprehensive comparison showing gradient saliency and both LRP rules side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison of all three methods\n",
    "attributions_dict = {\n",
    "    'Gradient Saliency': saliency_comparison['image'][0],\n",
    "    'LRP Epsilon (Œµ=0.1)': relevance_map[0] if relevance_map.dim() == 3 else relevance_map,\n",
    "    'LRP Alpha-Beta (Œ±=2, Œ≤=1)': alphabeta_relevance[0] if alphabeta_relevance.dim() == 3 else alphabeta_relevance\n",
    "}\n",
    "\n",
    "visualizer.plot_multiple_attributions(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    attributions=attributions_dict\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  ‚Ä¢ Gradient Saliency: Shows regions with high gradient magnitude\")\n",
    "print(\"  ‚Ä¢ LRP Epsilon: More balanced, stable attribution across the image\")\n",
    "print(\"  ‚Ä¢ LRP Alpha-Beta: Sharper focus on positive evidence regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b1cd7",
   "metadata": {},
   "source": [
    "## UnifiedLRP Implementation Details\n",
    "\n",
    "The **UnifiedLRP** implementation supports a wide range of neural network architectures through modular layer handlers:\n",
    "\n",
    "**Supported Layers (11 handlers):**\n",
    "- **Dense/Embedding**: Linear, ReLU, Embedding\n",
    "- **Convolutional**: Conv2d, MaxPool2d, AvgPool2d, AdaptiveAvgPool2d  \n",
    "- **Normalization**: BatchNorm2d\n",
    "- **Utility**: Flatten, Dropout\n",
    "\n",
    "This modular design makes it easy to:\n",
    "- Apply LRP to both CNNs (images) and MLPs (tabular/embedding data)\n",
    "- Extend with custom handlers for new layer types\n",
    "- Validate conservation property at each layer\n",
    "\n",
    "**Current Status**: Production-ready for standard CNN architectures. Future updates will add support for ResNet skip connections and Transformer attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply LRP to multiple samples from the batch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Get the device where the model is located\n",
    "device = next(resnet.model.parameters()).device\n",
    "\n",
    "for idx in range(3):\n",
    "    sample_image = covid_batch['image'][idx:idx+1]\n",
    "    \n",
    "    # Convert grayscale to RGB for ResNet\n",
    "    sample_image_rgb = sample_image.repeat(1, 3, 1, 1) if sample_image.shape[1] == 1 else sample_image\n",
    "    \n",
    "    # Move to the correct device\n",
    "    sample_image_rgb = sample_image_rgb.to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = resnet.model(sample_image_rgb)\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    # Compute LRP\n",
    "    sample_lrp = lrp.attribute(\n",
    "        inputs={'x': sample_image_rgb},\n",
    "        target_class=pred_class\n",
    "    )\n",
    "    \n",
    "    # Plot original image (grayscale)\n",
    "    axes[0, idx].imshow(sample_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[0, idx].set_title(f'Sample {idx}: {id2label[pred_class]}', fontsize=12, fontweight='bold')\n",
    "    axes[0, idx].axis('off')\n",
    "    \n",
    "    # Plot LRP heatmap (use first channel since all are same for grayscale input)\n",
    "    relevance = sample_lrp['x'].squeeze()\n",
    "    if relevance.dim() == 3:  # If shape is (3, H, W)\n",
    "        relevance = relevance[0]  # Take first channel\n",
    "    im = axes[1, idx].imshow(relevance.detach().cpu().numpy(), cmap='seismic', vmin=-0.1, vmax=0.1)\n",
    "    axes[1, idx].set_title(f'LRP Heatmap (Œµ=0.1)', fontsize=10)\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Applied LRP to 3 different COVID-19 X-ray samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2dde88",
   "metadata": {},
   "source": [
    "## Key Takeaways: Gradient Saliency vs. LRP\n",
    "\n",
    "**Gradient Saliency Maps:**\n",
    "- ‚úì Fast - single backward pass through gradients\n",
    "- ‚úì Works with any differentiable model  \n",
    "- ‚úì Good for identifying \"where\" the model looks\n",
    "- ‚úì Straightforward implementation\n",
    "- ‚ö†Ô∏è Can be noisy and may require smoothing\n",
    "- ‚ö†Ô∏è Doesn't satisfy conservation property\n",
    "\n",
    "**Layer-wise Relevance Propagation (LRP):**\n",
    "- ‚úì **Conservation property**: Relevances sum to model output for the target class\n",
    "- ‚úì More theoretically grounded attribution\n",
    "- ‚úì Modular design with layer-specific handlers\n",
    "- ‚úì Better captures \"how much\" each pixel contributes\n",
    "- ‚úì Supports both CNNs and MLPs with UnifiedLRP\n",
    "- ‚ö†Ô∏è Requires layer-specific propagation rules\n",
    "- ‚ö†Ô∏è Expected conservation violations of 5-150% depending on rule\n",
    "\n",
    "**Which one to use?**\n",
    "- Use **Gradient Saliency** for quick exploration and fast prototyping\n",
    "- Use **LRP** when you need precise, quantifiable attributions with conservation\n",
    "- Use **LRP Epsilon Rule** for numerically stable, balanced attributions\n",
    "- Use **LRP Alpha-Beta Rule** for sharper visualizations emphasizing positive evidence\n",
    "- Use **both** to get complementary insights into your model's behavior!\n",
    "\n",
    "**UnifiedLRP Status:**\n",
    "- ‚úÖ Production-ready for CNNs (11 layer handlers implemented)\n",
    "- ‚úÖ Supports: Conv2d, MaxPool2d, BatchNorm2d, Linear, ReLU, Flatten, Dropout, and more\n",
    "- ‚è≥ Future: ResNet skip connections, Transformer attention, RNN support"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
