{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ab438d2",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "Use this section to configure deterministic behaviour and import the libraries required for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8378db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.tasks.mortality_prediction import MortalityPredictionMIMIC4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0532e53a",
   "metadata": {},
   "source": [
    "# 2. Load MIMIC-IV Sample Extract\n",
    "Point to the preprocessed MIMIC-IV tables, optionally override individual files, and preview their structure before building a task dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c37df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MIMIC4Dataset(\n",
    "    ehr_root=\"/home/logic/physionet.org/files/mimic-iv-demo/2.2\",\n",
    "    ehr_tables=[\n",
    "        \"patients\",\n",
    "        \"admissions\",\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"prescriptions\",\n",
    "        \"labevents\",\n",
    "    ],\n",
    "    dev=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ce3ca",
   "metadata": {},
   "source": [
    "# 3. Prepare PyHealth Dataset\n",
    "Leverage the built-in `MortalityPredictionMIMIC4` task to convert patients into labeled visit samples and split them into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1d4af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MortalityPredictionMIMIC4()\n",
    "sample_dataset = dataset.set_task(task)\n",
    "\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "if len(sample_dataset) == 0:\n",
    "    raise RuntimeError(\"The task did not produce any samples. Disable dev mode or adjust table selections.\")\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(sample_dataset, [0.7, 0.1, 0.2], seed=SEED)\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a140ce3d",
   "metadata": {},
   "source": [
    "# 4. Inspect Batch Structure\n",
    "Build PyHealth dataloaders and quickly verify the keys and tensor shapes emitted before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fb8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE) if len(val_ds) else None\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE) if len(test_ds) else None\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    raise RuntimeError(\"The training loader is empty. Increase the dataset size or adjust the task configuration.\")\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "def describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__}(shape={tuple(value.shape)})\"\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return f\"{type(value).__name__}(len={len(value)})\"\n",
    "    return type(value).__name__\n",
    "\n",
    "batch_summary = {key: describe(value) for key, value in first_batch.items()}\n",
    "print(batch_summary)\n",
    "\n",
    "mortality_targets = first_batch[\"mortality\"]\n",
    "if hasattr(mortality_targets, \"shape\"):\n",
    "    preview = mortality_targets[:5].cpu().tolist()\n",
    "else:\n",
    "    preview = list(mortality_targets)[:5]\n",
    "print(f\"Sample mortality labels: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1542849d",
   "metadata": {},
   "source": [
    "# 5. Instantiate GCN Model\n",
    "Create the PyHealth GCN with custom hyperparameters and inspect the parameter footprint prior to optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e3a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import GCN\n",
    "\n",
    "model = GCN(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=64,\n",
    "    nhid=64,\n",
    "    num_layers=2,\n",
    "    ).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Visit-level adjacency during training\n",
    "We wrap each dataloader so every batch carries a `[batch_size, num_visits, num_visits]` visit graph. Here we use per-patient identity matrices (visits only connect to themselves), but you can replace them with any temporal graph (e.g., chronological chains or similarity-based connects).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisitAdjacencyLoader:\n",
    "    def __init__(self, base_loader, label_key, feature_keys):\n",
    "        self.base_loader = base_loader\n",
    "        self.label_key = label_key\n",
    "        self.feature_keys = feature_keys\n",
    "        self.batch_size = getattr(base_loader, \"batch_size\", None)\n",
    "        self.dataset = getattr(base_loader, \"dataset\", None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_loader)\n",
    "\n",
    "    def _infer_num_visits(self, batch):\n",
    "        for key in self.feature_keys:\n",
    "            value = batch.get(key)\n",
    "            if isinstance(value, torch.Tensor) and value.dim() >= 2:\n",
    "                return value.shape[1]\n",
    "        return 1\n",
    "\n",
    "    def __iter__(self):\n",
    "        for batch in self.base_loader:\n",
    "            batch = {k: v for k, v in batch.items()}\n",
    "            batch_size = batch[self.label_key].shape[0]\n",
    "            num_visits = self._infer_num_visits(batch)\n",
    "            visit_adj = torch.eye(num_visits, dtype=torch.float32).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "            batch[\"visit_adj\"] = visit_adj\n",
    "            yield batch\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self.base_loader, name)\n",
    "\n",
    "train_loader = VisitAdjacencyLoader(train_loader, model.label_key, model.feature_keys)\n",
    "val_loader = VisitAdjacencyLoader(val_loader, model.label_key, model.feature_keys) if val_loader else None\n",
    "test_loader = VisitAdjacencyLoader(test_loader, model.label_key, model.feature_keys) if test_loader else None\n",
    "\n",
    "visit_adj = next(iter(train_loader))[\"visit_adj\"]\n",
    "print(f\"Visit adjacency injected with shape {tuple(visit_adj.shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7cfd5",
   "metadata": {},
   "source": [
    "# 7. Configure Trainer\n",
    "Wrap the model with the PyHealth `Trainer` to handle optimisation, gradient clipping, and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091598e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\"],\n",
    "    device=str(device),\n",
    "    enable_logging=False,\n",
    " )\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"optimizer_params\": {\"lr\": 1e-3},\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"monitor\": \"roc_auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f108ba",
   "metadata": {},
   "source": [
    "# 8. Train the Model\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac799ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_kwargs = dict(training_config)\n",
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    **train_kwargs,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2c10dd",
   "metadata": {},
   "source": [
    "# 9. Evaluate on Validation Split\n",
    "Switch to evaluation mode, collect predictions for the validation split, and compute AUROC and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a279856",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "for split_name, loader in {\"validation\": val_loader, \"test\": test_loader}.items():\n",
    "    if loader is None:\n",
    "        continue\n",
    "    metrics = trainer.evaluate(loader)\n",
    "    evaluation_results[split_name] = metrics\n",
    "    formatted = \", \".join(f\"{k}={v:.4f}\" for k, v in metrics.items())\n",
    "    print(f\"{split_name.title()} metrics: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8d0a7",
   "metadata": {},
   "source": [
    "# 10. Inspect Sample Predictions\n",
    "Run a quick inference pass on the validation or test split to preview predicted probabilities alongside ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3692c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loader = val_loader if val_loader is not None else train_loader\n",
    "\n",
    "y_true, y_prob, mean_loss = trainer.inference(target_loader)\n",
    "positive_prob = y_prob if y_prob.ndim == 1 else y_prob[..., -1]\n",
    "preview_pairs = list(zip(y_true[:5].tolist(), positive_prob[:5].tolist()))\n",
    "print(f\"Mean loss: {mean_loss:.4f}\")\n",
    "print(f\"Preview (label, positive_prob): {preview_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e616e2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
