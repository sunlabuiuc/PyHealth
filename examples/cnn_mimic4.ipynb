{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1ec419",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "Use this section to configure deterministic behaviour and import the libraries required for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb256423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.tasks.mortality_prediction import MortalityPredictionMIMIC4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693d12",
   "metadata": {},
   "source": [
    "# 2. Load MIMIC-IV Sample Extract\n",
    "Point to the preprocessed MIMIC-IV tables, optionally override individual files, and preview their structure before building a task dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61694c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage Starting MIMIC4Dataset init: 1407.4 MB\n",
      "Initializing MIMIC4EHRDataset with tables: ['patients', 'admissions', 'diagnoses_icd', 'procedures_icd', 'prescriptions', 'labevents'] (dev mode: True)\n",
      "Using default EHR config: /home/logic/miniforge3/envs/pyhealth/lib/python3.12/site-packages/pyhealth/datasets/configs/mimic4_ehr.yaml\n",
      "Memory usage Before initializing mimic4_ehr: 1407.4 MB\n",
      "Duplicate table names in tables list. Removing duplicates.\n",
      "Initializing mimic4_ehr dataset from /home/logic/Github/mimic4 (dev mode: False)\n",
      "Scanning table: diagnoses_icd from /home/logic/Github/mimic4/hosp/diagnoses_icd.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: admissions from /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: prescriptions from /home/logic/Github/mimic4/hosp/prescriptions.csv.gz\n",
      "Scanning table: icustays from /home/logic/Github/mimic4/icu/icustays.csv.gz\n",
      "Scanning table: patients from /home/logic/Github/mimic4/hosp/patients.csv.gz\n",
      "Scanning table: labevents from /home/logic/Github/mimic4/hosp/labevents.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/d_labitems.csv.gz\n",
      "Scanning table: procedures_icd from /home/logic/Github/mimic4/hosp/procedures_icd.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Memory usage After initializing mimic4_ehr: 1442.8 MB\n",
      "Memory usage After EHR dataset initialization: 1442.8 MB\n",
      "Memory usage Before combining data: 1442.8 MB\n",
      "Combining data from ehr dataset\n",
      "Creating combined dataframe\n",
      "Memory usage After combining data: 1442.8 MB\n",
      "Memory usage Completed MIMIC4Dataset init: 1442.8 MB\n"
     ]
    }
   ],
   "source": [
    "dataset = MIMIC4Dataset(\n",
    "    ehr_root=\"/home/logic/Github/mimic4\",\n",
    "    ehr_tables=[\n",
    "        \"patients\",\n",
    "        \"admissions\",\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"prescriptions\",\n",
    "        \"labevents\",\n",
    "    ],\n",
    "    dev=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648825d7",
   "metadata": {},
   "source": [
    "# 3. Prepare PyHealth Dataset\n",
    "Leverage the built-in `MortalityPredictionMIMIC4` task to convert patients into labeled visit samples and split them into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b83716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting task MortalityPredictionMIMIC4 for mimic4 base dataset...\n",
      "Generating samples with 1 worker(s)...\n",
      "Collecting global event dataframe...\n",
      "Dev mode enabled: limiting to 1000 patients\n",
      "Collected dataframe with shape: (131557, 47)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for MortalityPredictionMIMIC4 with 1 worker: 100%|██████████| 100/100 [00:02<00:00, 47.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mortality vocab: {0: 0, 1: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing samples: 100%|██████████| 108/108 [00:00<00:00, 20164.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 108 samples for task MortalityPredictionMIMIC4\n",
      "Total task samples: 108\n",
      "Input schema: {'conditions': 'sequence', 'procedures': 'sequence', 'drugs': 'sequence'}\n",
      "Output schema: {'mortality': 'binary'}\n",
      "Train/Val/Test sizes: 76, 12, 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "task = MortalityPredictionMIMIC4()\n",
    "sample_dataset = dataset.set_task(task)\n",
    "\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "if len(sample_dataset) == 0:\n",
    "    raise RuntimeError(\"The task did not produce any samples. Disable dev mode or adjust table selections.\")\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(sample_dataset, [0.7, 0.1, 0.2], seed=SEED)\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ca0dd",
   "metadata": {},
   "source": [
    "# 4. Inspect Batch Structure\n",
    "Build PyHealth dataloaders and quickly verify the keys and tensor shapes emitted before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01ff2e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'visit_id': 'list(len=32)', 'patient_id': 'list(len=32)', 'conditions': 'Tensor(shape=(32, 39))', 'procedures': 'Tensor(shape=(32, 16))', 'drugs': 'Tensor(shape=(32, 348))', 'mortality': 'Tensor(shape=(32, 1))'}\n",
      "Sample mortality labels: [[0.0], [0.0], [0.0], [0.0], [0.0]]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE) if len(val_ds) else None\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE) if len(test_ds) else None\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    raise RuntimeError(\"The training loader is empty. Increase the dataset size or adjust the task configuration.\")\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "def describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__}(shape={tuple(value.shape)})\"\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return f\"{type(value).__name__}(len={len(value)})\"\n",
    "    return type(value).__name__\n",
    "\n",
    "batch_summary = {key: describe(value) for key, value in first_batch.items()}\n",
    "print(batch_summary)\n",
    "\n",
    "mortality_targets = first_batch[\"mortality\"]\n",
    "if hasattr(mortality_targets, \"shape\"):\n",
    "    preview = mortality_targets[:5].cpu().tolist()\n",
    "else:\n",
    "    preview = list(mortality_targets)[:5]\n",
    "print(f\"Sample mortality labels: {preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e648fe",
   "metadata": {},
   "source": [
    "# 5. Instantiate CNN Model\n",
    "Create the PyHealth CNN with custom hyperparameters and inspect the parameter footprint prior to optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8864561",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/logic/miniforge3/envs/pyhealth/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature keys: ['conditions', 'procedures', 'drugs']\n",
      "Label key: mortality\n",
      "Total parameters: 250,369\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.models import CNN\n",
    "\n",
    "model = CNN(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    ).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6771c",
   "metadata": {},
   "source": [
    "# 6. Configure Trainer\n",
    "Wrap the model with the PyHealth `Trainer` to handle optimisation, gradient clipping, and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e0bb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (embedding_model): EmbeddingModel(embedding_layers=ModuleDict(\n",
      "    (conditions): Embedding(865, 64, padding_idx=0)\n",
      "    (procedures): Embedding(218, 64, padding_idx=0)\n",
      "    (drugs): Embedding(486, 64, padding_idx=0)\n",
      "  ))\n",
      "  (cnn): ModuleDict(\n",
      "    (conditions): CNNLayer(\n",
      "      (cnn): ModuleDict(\n",
      "        (CNN-0): CNNBlock(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (CNN-1): CNNBlock(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "    (procedures): CNNLayer(\n",
      "      (cnn): ModuleDict(\n",
      "        (CNN-0): CNNBlock(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (CNN-1): CNNBlock(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "    (drugs): CNNLayer(\n",
      "      (cnn): ModuleDict(\n",
      "        (CNN-0): CNNBlock(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (CNN-1): CNNBlock(\n",
      "          (conv1): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (2): ReLU()\n",
      "          )\n",
      "          (conv2): Sequential(\n",
      "            (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "            (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "      )\n",
      "      (pooling): AdaptiveAvgPool1d(output_size=1)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=192, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: ['roc_auc']\n",
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\"],\n",
    "    device=str(device),\n",
    "    enable_logging=False,\n",
    " )\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"optimizer_params\": {\"lr\": 1e-3},\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"monitor\": \"roc_auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56da21",
   "metadata": {},
   "source": [
    "# 7. Train the Model\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49f62657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Batch size: 32\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: 5.0\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7f5e1c120f50>\n",
      "Monitor: roc_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 5: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-3 ---\n",
      "loss: 0.4153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 11.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-0, step-3 ---\n",
      "roc_auc: 0.8182\n",
      "loss: 0.5464\n",
      "New best roc_auc score (0.8182) at epoch-0, step-3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 / 5: 100%|██████████| 3/3 [00:01<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-6 ---\n",
      "loss: 0.2351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 238.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-1, step-6 ---\n",
      "roc_auc: 0.7273\n",
      "loss: 0.4530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 / 5: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-9 ---\n",
      "loss: 0.2450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 159.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-2, step-9 ---\n",
      "roc_auc: 0.3636\n",
      "loss: 0.3684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 / 5: 100%|██████████| 3/3 [00:01<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-12 ---\n",
      "loss: 0.1937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 231.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-3, step-12 ---\n",
      "roc_auc: 0.3636\n",
      "loss: 0.3210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 / 5: 100%|██████████| 3/3 [00:02<00:00,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-15 ---\n",
      "loss: 0.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 154.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-4, step-15 ---\n",
      "roc_auc: 0.3636\n",
      "loss: 0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_kwargs = dict(training_config)\n",
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    **train_kwargs,\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659ad5b",
   "metadata": {},
   "source": [
    "# 8. Evaluate on Validation Split\n",
    "Switch to evaluation mode, collect predictions for the validation split, and compute AUROC and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fb62f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 152.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: roc_auc=0.3636, loss=0.3027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: roc_auc=0.5882, loss=0.4141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "for split_name, loader in {\"validation\": val_loader, \"test\": test_loader}.items():\n",
    "    if loader is None:\n",
    "        continue\n",
    "    metrics = trainer.evaluate(loader)\n",
    "    evaluation_results[split_name] = metrics\n",
    "    formatted = \", \".join(f\"{k}={v:.4f}\" for k, v in metrics.items())\n",
    "    print(f\"{split_name.title()} metrics: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fc6d6",
   "metadata": {},
   "source": [
    "# 9. Inspect Sample Predictions\n",
    "Run a quick inference pass on the validation or test split to preview predicted probabilities alongside ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba3510f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 1/1 [00:00<00:00, 131.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 0.3027\n",
      "Preview (label, positive_prob): [([0.0], 0.15316888689994812), ([0.0], 0.14590689539909363), ([0.0], 0.09844371676445007), ([0.0], 0.12233827263116837), ([0.0], 0.1447029411792755)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_loader = val_loader if val_loader is not None else train_loader\n",
    "\n",
    "y_true, y_prob, mean_loss = trainer.inference(target_loader)\n",
    "positive_prob = y_prob if y_prob.ndim == 1 else y_prob[..., -1]\n",
    "preview_pairs = list(zip(y_true[:5].tolist(), positive_prob[:5].tolist()))\n",
    "print(f\"Mean loss: {mean_loss:.4f}\")\n",
    "print(f\"Preview (label, positive_prob): {preview_pairs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5372734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
