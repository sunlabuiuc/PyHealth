{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxQb0zWLRsemvdVn2w859g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielWarner/PyHealth/blob/mimic-cxr-sentence-example/examples/radiology_sentence_classification_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk5zRodjAlL9"
      },
      "outputs": [],
      "source": [
        "# Radiology Sentence Classification with PyHealth Tokenizer and Metrics\n",
        "\n",
        "# **Author:** Gabriel Warner\n",
        "# **NetID:** gsw3 (UIUC Online MCS)\n",
        "\n",
        "#**Paper title (DL4H final project):**\n",
        "#Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis\n",
        "\n",
        "#**Paper link:**\n",
        "#<ADD YOUR ARXIV / PDF / GITHUB LINK HERE>\n",
        "\n",
        "#**Description of the task:**\n",
        "\n",
        "#This notebook shows a small, reproducible example of **sentence-level radiology classification** using PyHealthâ€™s reusable components.\n",
        "#We create a toy dataset of radiology report sentences labeled as **normal**, **abnormal**, or **uncertain**, then:\n",
        "\n",
        "#- Use `pyhealth.tokenizer.Tokenizer` to turn tokens into indices\n",
        "#- Train a small PyTorch text classifier\n",
        "#- Evaluate using `pyhealth.metrics.multiclass.multiclass_metrics_fn`\n",
        "\n",
        "#This example mirrors the sentence-level classification component from our DL4H final project and demonstrates how PyHealth modules can be applied to clinical NLP tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This example assumes you're running inside the PyHealth repo.\n",
        "# If you're running it standalone (e.g., in Colab), uncomment the pip install:\n",
        "# %pip install pyhealth\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from pyhealth.tokenizer import Tokenizer\n",
        "from pyhealth.metrics import multiclass_metrics_fn  # documented in PyHealth metrics API\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "3GHgu62LA2kg",
        "outputId": "ea4cfadf-f6f2-41b7-c791-c42542c082cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyhealth'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-778737943.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyhealth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyhealth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmulticlass_metrics_fn\u001b[0m  \u001b[0;31m# documented in PyHealth metrics API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyhealth'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a fixed random seed for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ],
      "metadata": {
        "id": "u6pvCuMiA4vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "radiology_samples = [\n",
        "    # normal\n",
        "    (\"Lungs are clear. No acute cardiopulmonary abnormality.\", \"normal\"),\n",
        "    (\"No focal consolidation, pleural effusion, or pneumothorax.\", \"normal\"),\n",
        "    (\"Cardiomediastinal silhouette is within normal limits.\", \"normal\"),\n",
        "\n",
        "    # abnormal\n",
        "    (\"Left lower lobe opacity consistent with pneumonia.\", \"abnormal\"),\n",
        "    (\"Moderate right pleural effusion with associated atelectasis.\", \"abnormal\"),\n",
        "    (\"Patchy bilateral ground-glass opacities concerning for infection.\", \"abnormal\"),\n",
        "\n",
        "    # uncertain\n",
        "    (\"Findings could represent early interstitial edema.\", \"uncertain\"),\n",
        "    (\"Cannot exclude a small left apical pneumothorax.\", \"uncertain\"),\n",
        "    (\"Opacity may represent atelectasis versus consolidation.\", \"uncertain\"),\n",
        "]\n",
        "\n",
        "label2id = {\"normal\": 0, \"abnormal\": 1, \"uncertain\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "texts = [s for s, _ in radiology_samples]\n",
        "labels = [label2id[y] for _, y in radiology_samples]\n",
        "\n",
        "list(zip(texts, labels))\n"
      ],
      "metadata": {
        "id": "9fUGVeJnA6TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenize(text: str) -> List[str]:\n",
        "    # very simple whitespace + punctuation handling\n",
        "    text = text.lower().replace(\".\", \"\").replace(\",\", \"\")\n",
        "    return text.split()\n",
        "\n",
        "# Build token space from our tiny corpus\n",
        "token_space = sorted({tok for txt in texts for tok in simple_tokenize(txt)})\n",
        "\n",
        "token_space\n"
      ],
      "metadata": {
        "id": "RYnAxo84BOta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(tokens=token_space, special_tokens=[\"<pad>\", \"<unk>\"])\n",
        "\n",
        "vocab_size = tokenizer.get_vocabulary_size()\n",
        "pad_index = tokenizer.get_padding_index()\n",
        "\n",
        "vocab_size, pad_index\n"
      ],
      "metadata": {
        "id": "rKn3kNEyBT1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 32  # small, just for demo\n",
        "\n",
        "\n",
        "class RadiologySentenceDataset(Dataset):\n",
        "    \"\"\"Tiny radiology sentence-level dataset using PyHealth Tokenizer.\n",
        "\n",
        "    Each sample returns:\n",
        "      - input_ids: LongTensor of token indices (padded/truncated)\n",
        "      - label: LongTensor scalar (0=normal,1=abnormal,2=uncertain)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, texts: List[str], labels: List[int], tokenizer: Tokenizer):\n",
        "        assert len(texts) == len(labels)\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        tokens = simple_tokenize(text)\n",
        "        # Tokenizer expects 2D batch; we wrap [tokens] then unwrap index 0\n",
        "        indices_2d = self.tokenizer.batch_encode_2d(\n",
        "            batch=[tokens],\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=MAX_LENGTH,\n",
        "        )\n",
        "        input_ids = torch.tensor(indices_2d[0], dtype=torch.long)\n",
        "\n",
        "        return input_ids, torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "4R7iWJaoBVPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple 2/1 split: 6 for \"train\", 3 for \"val\"\n",
        "train_texts, val_texts = texts[:6], texts[6:]\n",
        "train_labels, val_labels = labels[:6], labels[6:]\n",
        "\n",
        "train_dataset = RadiologySentenceDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = RadiologySentenceDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=3, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=False)\n",
        "\n",
        "len(train_dataset), len(val_dataset)\n"
      ],
      "metadata": {
        "id": "Yth4SqORBWq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int\n",
        "    embed_dim: int = 32\n",
        "    num_classes: int = 3\n",
        "    pad_index: int = 0\n",
        "\n",
        "\n",
        "class SimpleTextClassifier(nn.Module):\n",
        "    \"\"\"Minimal text classifier to pair with PyHealth Tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=config.vocab_size,\n",
        "            embedding_dim=config.embed_dim,\n",
        "            padding_idx=config.pad_index,\n",
        "        )\n",
        "        self.fc = nn.Linear(config.embed_dim, config.num_classes)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: [batch_size, seq_len] LongTensor\n",
        "\n",
        "        Returns:\n",
        "            logits: [batch_size, num_classes]\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(input_ids)  # [B, L, D]\n",
        "        # simple average pooling over non-pad positions\n",
        "        mask = (input_ids != pad_index).unsqueeze(-1)  # [B, L, 1]\n",
        "        summed = (embedded * mask).sum(dim=1)\n",
        "        denom = mask.sum(dim=1).clamp(min=1)\n",
        "        pooled = summed / denom\n",
        "        logits = self.fc(pooled)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "tTHwuonjBXjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig(\n",
        "    vocab_size=vocab_size,\n",
        "    embed_dim=32,\n",
        "    num_classes=len(label2id),\n",
        "    pad_index=pad_index,\n",
        ")\n",
        "\n",
        "model = SimpleTextClassifier(config).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "model\n"
      ],
      "metadata": {
        "id": "Z2vs-lm1BZU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(loader: DataLoader, train: bool = True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "    all_labels = []\n",
        "\n",
        "    total_loss = 0.0\n",
        "    count = 0\n",
        "\n",
        "    for input_ids, labels_batch in loader:\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels_batch)\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        batch_size = labels_batch.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        count += batch_size\n",
        "\n",
        "        all_logits.append(logits.detach().cpu())\n",
        "        all_labels.append(labels_batch.detach().cpu())\n",
        "\n",
        "    avg_loss = total_loss / max(count, 1)\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "    return avg_loss, all_logits, all_labels\n"
      ],
      "metadata": {
        "id": "ZcBwi0QpBakY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, _, _ = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_logits, val_labels = run_epoch(val_loader, train=False)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "A6G_mmIwBbsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get predictions and probabilities on the full dataset (train + val)\n",
        "_, train_logits, train_labels_tensor = run_epoch(train_loader, train=False)\n",
        "_, val_logits, val_labels_tensor = run_epoch(val_loader, train=False)\n",
        "\n",
        "y_true = np.concatenate(\n",
        "    [train_labels_tensor.numpy(), val_labels_tensor.numpy()], axis=0\n",
        ")\n",
        "\n",
        "y_prob = torch.softmax(\n",
        "    torch.cat([train_logits, val_logits], dim=0), dim=-1\n",
        ").numpy()\n",
        "\n",
        "metrics = multiclass_metrics_fn(\n",
        "    y_true=y_true,\n",
        "    y_prob=y_prob,\n",
        "    metrics=[\"accuracy\", \"macro_f1\"],\n",
        ")\n",
        "\n",
        "metrics\n"
      ],
      "metadata": {
        "id": "hUocx1k_Bcgx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}