{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec17da31",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Started\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the latest alpha release of StageNet modernized for PyHealth:\n",
    "\n",
    "```bash\n",
    "pip install pyhealth==2.0a8\n",
    "```\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "Load the PyHealth dataset for mortality prediction.\n",
    "\n",
    "PyHealth datasets use a `config.yaml` file to define:\n",
    "- Input tables (.csv, .tsv, etc.)\n",
    "- Features to extract\n",
    "- Aggregation methods\n",
    "\n",
    "The result is a single dataframe where each row represents one patient and their features.\n",
    "\n",
    "For more details on PyHealth datasets, see [this resource](https://colab.research.google.com/drive/1voSx7wEfzXfEf2sIfW6b-8p1KqMyuWxK#scrollTo=NSrb2PGFqUgS).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd30b75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johnwu3/miniconda3/envs/medical_coding_demo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage Starting MIMIC4Dataset init: 772.0 MB\n",
      "Initializing MIMIC4EHRDataset with tables: ['patients', 'admissions', 'diagnoses_icd', 'procedures_icd', 'labevents'] (dev mode: False)\n",
      "Using default EHR config: /home/johnwu3/projects/PyHealth_Branch_Testing/PyHealth/pyhealth/datasets/configs/mimic4_ehr.yaml\n",
      "Memory usage Before initializing mimic4_ehr: 772.0 MB\n",
      "Duplicate table names in tables list. Removing duplicates.\n",
      "Initializing mimic4_ehr dataset from /srv/local/data/physionet.org/files/mimiciv/2.2/ (dev mode: False)\n",
      "Scanning table: procedures_icd from /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/procedures_icd.csv.gz\n",
      "Joining with table: /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz\n",
      "Original path does not exist. Using alternative: /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/admissions.csv\n",
      "Scanning table: labevents from /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/labevents.csv.gz\n",
      "Joining with table: /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/d_labitems.csv.gz\n",
      "Scanning table: icustays from /srv/local/data/physionet.org/files/mimiciv/2.2/icu/icustays.csv.gz\n",
      "Scanning table: patients from /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/patients.csv.gz\n",
      "Scanning table: diagnoses_icd from /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/diagnoses_icd.csv.gz\n",
      "Joining with table: /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz\n",
      "Original path does not exist. Using alternative: /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/admissions.csv\n",
      "Scanning table: admissions from /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/admissions.csv.gz\n",
      "Original path does not exist. Using alternative: /srv/local/data/physionet.org/files/mimiciv/2.2/hosp/admissions.csv\n",
      "Memory usage After initializing mimic4_ehr: 30378.1 MB\n",
      "Memory usage After EHR dataset initialization: 30378.1 MB\n",
      "Memory usage Before combining data: 30378.1 MB\n",
      "Combining data from ehr dataset\n",
      "Creating combined dataframe\n",
      "Memory usage After combining data: 30378.1 MB\n",
      "Memory usage Completed MIMIC4Dataset init: 30378.1 MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Example of using StageNet for mortality prediction on MIMIC-IV.\n",
    "\n",
    "This example demonstrates:\n",
    "1. Loading MIMIC-IV data\n",
    "2. Applying the MortalityPredictionStageNetMIMIC4 task\n",
    "3. Creating a SampleDataset with StageNet processors\n",
    "4. Training a StageNet model\n",
    "\"\"\"\n",
    "\n",
    "from pyhealth.datasets import (\n",
    "    MIMIC4Dataset,\n",
    "    get_dataloader,\n",
    "    split_by_patient,\n",
    ")\n",
    "from pyhealth.models import StageNet\n",
    "from pyhealth.tasks import MortalityPredictionStageNetMIMIC4\n",
    "from pyhealth.trainer import Trainer\n",
    "import torch\n",
    "\n",
    "# STEP 1: Load MIMIC-IV base dataset\n",
    "base_dataset = MIMIC4Dataset(\n",
    "    ehr_root=\"/srv/local/data/physionet.org/files/mimiciv/2.2/\",\n",
    "    ehr_tables=[\n",
    "        \"patients\",\n",
    "        \"admissions\",\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"labevents\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf0f1c",
   "metadata": {},
   "source": [
    "## Input and Output Schemas\n",
    "Input and output schemas map feature keys (e.g., \"labs\", \"icd_codes\") to StageNet processors. Each processor converts features into tuple objects used for training and inference.\n",
    "\n",
    "**Required format:** Each feature processed in our task call must follow this structure:\n",
    "```python\n",
    "\"feature\": (my_times_list, my_values_list)\n",
    "```\n",
    "We offer two types of StageNet processors, one for categorical variables, and the other for numerical feature variables. Our goal here is to represent each feature as a pre-defined tuple (time, value) that we can later pass to StageNet for processing.\n",
    "\n",
    "\n",
    "## What are these processors?\n",
    "\n",
    "Effectively processors take existing data variables and turns them into a tensor format. Here, we define a set of custom processors so we can leverage StageNet's ability to take in a time-series set of time intervals and feature sets.\n",
    "\n",
    "## StageNetProcessor - For Categories (Labels)\n",
    "\n",
    "**What it handles:** Text labels like diagnosis codes, medication names, or lab test types.\n",
    "\n",
    "**What it does:**\n",
    "- Takes lists of codes (like `[\"diabetes\", \"hypertension\"]`)\n",
    "- Converts each word into a unique number (like `[\"diabetes\"=1, \"hypertension\"=2]`)\n",
    "- Keeps track of when things happened (timestamps)\n",
    "- Can handle nested lists (like multiple codes per visit)\n",
    "\n",
    "**Example:** If a patient had 3 doctor visits with different diagnoses, this processor remembers what diagnosis happened at each visit and when.\n",
    "\n",
    "## StageNetTensorProcessor - For Numbers (Measurements)\n",
    "\n",
    "**What it handles:** Actual measurements like blood pressure, temperature, or lab values.\n",
    "\n",
    "**What it does:**\n",
    "- Takes lists of numbers (like `[98.6, 99.1, 98.8]` for temperatures)\n",
    "- Fills in missing measurements using the last known value (forward-fill)\n",
    "- Keeps track of when measurements were taken\n",
    "- Can handle multiple measurements at once (like blood pressure AND heart rate)\n",
    "\n",
    "**Example:** If a patient's heart rate was measured as `[72, None, 68]`, it fills in the missing value as `[72, 72, 68]` (copying the last known value).\n",
    "\n",
    "## How Time Processing Works\n",
    "\n",
    "Both processors handle time information in a flexible way:\n",
    "\n",
    "**Input formats accepted:**\n",
    "- Simple list: `[0.0, 1.5, 3.0]` - time intervals in hours/days\n",
    "- Nested list: `[[0.0], [1.5], [3.0]]` - automatically flattened\n",
    "- No time: `None` - when timing doesn't matter\n",
    "\n",
    "**What the time means:**\n",
    "- Times represent intervals or delays between events\n",
    "- For example: `[0.0, 2.5, 1.0]` could mean \"first event at start, second event 2.5 hours later, third event 1 hour after that\"\n",
    "- Times are converted to float tensors so the model can learn temporal patterns\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# Patient temperature readings\n",
    "data = {\n",
    "    \"value\": [98.6, 99.1, 98.8],  # temperatures in °F\n",
    "    \"time\": [0.0, 2.0, 1.0]        # hours since previous admissions\n",
    "}\n",
    "```\n",
    "\n",
    "The processor keeps the time and values paired together, so the model knows that 99.1°F was recorded at 2 hours after admission.\n",
    "\n",
    "For syntactic reasons, we add the suffix \"Ex\" as they're already implemented in PyHealth. This is more to showcase what's happening underneath the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba3e055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "from pyhealth.processors import register_processor\n",
    "from pyhealth.processors.base_processor import FeatureProcessor\n",
    "\n",
    "\n",
    "@register_processor(\"stagenet_ex\")\n",
    "class StageNetProcessorEx(FeatureProcessor):\n",
    "    \"\"\"\n",
    "    Feature processor for StageNet CODE inputs with coupled value/time data.\n",
    "\n",
    "    This processor handles categorical code sequences (flat or nested).\n",
    "    For numeric features, use StageNetTensorProcessor instead.\n",
    "\n",
    "    Input Format (tuple):\n",
    "        (time, values) where:\n",
    "        - time: List of scalars [0.0, 2.0, 1.3] or None\n",
    "        - values: [\"code1\", \"code2\"] or [[\"A\", \"B\"], [\"C\"]]\n",
    "\n",
    "    The processor automatically detects:\n",
    "    - List of strings -> flat code sequences\n",
    "    - List of lists of strings -> nested code sequences\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (time_tensor, value_tensor) where time_tensor can be None\n",
    "\n",
    "    Examples:\n",
    "        >>> # Case 1: Code sequence with time\n",
    "        >>> processor = StageNetProcessor()\n",
    "        >>> data = ([0.0, 1.5, 2.3], [\"code1\", \"code2\", \"code3\"])\n",
    "        >>> time, values = processor.process(data)\n",
    "        >>> values.shape  # (3,) - sequence of code indices\n",
    "        >>> time.shape    # (3,) - time intervals\n",
    "\n",
    "        >>> # Case 2: Nested codes with time\n",
    "        >>> data = ([0.0, 1.5], [[\"A\", \"B\"], [\"C\"]])\n",
    "        >>> time, values = processor.process(data)\n",
    "        >>> values.shape  # (2, max_inner_len) - padded nested sequences\n",
    "        >>> time.shape    # (2,)\n",
    "\n",
    "        >>> # Case 3: Codes without time\n",
    "        >>> data = (None, [\"code1\", \"code2\"])\n",
    "        >>> time, values = processor.process(data)\n",
    "        >>> values.shape  # (2,)\n",
    "        >>> time          # None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.code_vocab: Dict[Any, int] = {\"<unk>\": -1, \"<pad>\": 0}\n",
    "        self._next_index = 1\n",
    "        self._is_nested = None  # Will be determined during fit\n",
    "        self._max_nested_len = None  # Max inner sequence length for nested codes\n",
    "\n",
    "    def fit(self, samples: List[Dict], key: str) -> None:\n",
    "        \"\"\"Build vocabulary and determine input structure.\n",
    "\n",
    "        Args:\n",
    "            samples: List of sample dictionaries\n",
    "            key: The key in samples that contains tuple (time, values)\n",
    "        \"\"\"\n",
    "        # Examine first non-None sample to determine structure\n",
    "        for sample in samples:\n",
    "            if key in sample and sample[key] is not None:\n",
    "                # Unpack tuple: (time, values)\n",
    "                time_data, value_data = sample[key]\n",
    "\n",
    "                # Determine nesting level for codes\n",
    "                if isinstance(value_data, list) and len(value_data) > 0:\n",
    "                    first_elem = value_data[0]\n",
    "\n",
    "                    if isinstance(first_elem, str):\n",
    "                        # Case 1: [\"code1\", \"code2\", ...]\n",
    "                        self._is_nested = False\n",
    "                    elif isinstance(first_elem, list):\n",
    "                        if len(first_elem) > 0 and isinstance(first_elem[0], str):\n",
    "                            # Case 2: [[\"A\", \"B\"], [\"C\"], ...]\n",
    "                            self._is_nested = True\n",
    "                break\n",
    "\n",
    "        # Build vocabulary for codes and find max nested length\n",
    "        max_inner_len = 0\n",
    "        for sample in samples:\n",
    "            if key in sample and sample[key] is not None:\n",
    "                # Unpack tuple: (time, values)\n",
    "                time_data, value_data = sample[key]\n",
    "\n",
    "                if self._is_nested:\n",
    "                    # Nested codes\n",
    "                    for inner_list in value_data:\n",
    "                        # Track max inner length\n",
    "                        max_inner_len = max(max_inner_len, len(inner_list))\n",
    "                        for code in inner_list:\n",
    "                            if code is not None and code not in self.code_vocab:\n",
    "                                self.code_vocab[code] = self._next_index\n",
    "                                self._next_index += 1\n",
    "                else:\n",
    "                    # Flat codes\n",
    "                    for code in value_data:\n",
    "                        if code is not None and code not in self.code_vocab:\n",
    "                            self.code_vocab[code] = self._next_index\n",
    "                            self._next_index += 1\n",
    "\n",
    "        # Store max nested length (at least 1 for empty sequences)\n",
    "        if self._is_nested:\n",
    "            self._max_nested_len = max(1, max_inner_len)\n",
    "\n",
    "    def process(\n",
    "        self, value: Tuple[Optional[List], List]\n",
    "    ) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"Process tuple format data into tensors.\n",
    "\n",
    "        Args:\n",
    "            value: Tuple of (time, values) where values are codes\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (time_tensor, value_tensor), time can be None\n",
    "        \"\"\"\n",
    "        # Unpack tuple: (time, values)\n",
    "        time_data, value_data = value\n",
    "\n",
    "        # Encode codes to indices\n",
    "        if self._is_nested:\n",
    "            # Nested codes: [[\"A\", \"B\"], [\"C\"]]\n",
    "            value_tensor = self._encode_nested_codes(value_data)\n",
    "        else:\n",
    "            # Flat codes: [\"code1\", \"code2\"]\n",
    "            value_tensor = self._encode_codes(value_data)\n",
    "\n",
    "        # Process time if present\n",
    "        time_tensor = None\n",
    "        if time_data is not None and len(time_data) > 0:\n",
    "            # Handle both [0.0, 1.5] and [[0.0], [1.5]] formats\n",
    "            if isinstance(time_data[0], list):\n",
    "                # Flatten [[0.0], [1.5]] -> [0.0, 1.5]\n",
    "                time_data = [t[0] if isinstance(t, list) else t for t in time_data]\n",
    "            time_tensor = torch.tensor(time_data, dtype=torch.float)\n",
    "\n",
    "        return (time_tensor, value_tensor)\n",
    "\n",
    "    def _encode_codes(self, codes: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Encode flat code list to indices.\"\"\"\n",
    "        # Handle empty code list - return single padding token\n",
    "        if len(codes) == 0:\n",
    "            return torch.tensor([self.code_vocab[\"<pad>\"]], dtype=torch.long)\n",
    "\n",
    "        indices = []\n",
    "        for code in codes:\n",
    "            if code is None or code not in self.code_vocab:\n",
    "                indices.append(self.code_vocab[\"<unk>\"])\n",
    "            else:\n",
    "                indices.append(self.code_vocab[code])\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def _encode_nested_codes(self, nested_codes: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\"Encode nested code lists to padded 2D tensor.\n",
    "\n",
    "        Pads all inner sequences to self._max_nested_len (global max).\n",
    "        \"\"\"\n",
    "        # Handle empty nested codes (no visits/events)\n",
    "        # Return single padding token with shape (1, max_len)\n",
    "        if len(nested_codes) == 0:\n",
    "            pad_token = self.code_vocab[\"<pad>\"]\n",
    "            return torch.tensor([[pad_token] * self._max_nested_len], dtype=torch.long)\n",
    "\n",
    "        encoded_sequences = []\n",
    "        # Use global max length determined during fit\n",
    "        max_len = self._max_nested_len\n",
    "\n",
    "        for inner_codes in nested_codes:\n",
    "            indices = []\n",
    "            for code in inner_codes:\n",
    "                if code is None or code not in self.code_vocab:\n",
    "                    indices.append(self.code_vocab[\"<unk>\"])\n",
    "                else:\n",
    "                    indices.append(self.code_vocab[code])\n",
    "            # Pad to GLOBAL max_len\n",
    "            while len(indices) < max_len:\n",
    "                indices.append(self.code_vocab[\"<pad>\"])\n",
    "            encoded_sequences.append(indices)\n",
    "\n",
    "        return torch.tensor(encoded_sequences, dtype=torch.long)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return vocabulary size.\"\"\"\n",
    "        return len(self.code_vocab)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self._is_nested:\n",
    "            return (\n",
    "                f\"StageNetProcessor(is_nested={self._is_nested}, \"\n",
    "                f\"vocab_size={len(self.code_vocab)}, \"\n",
    "                f\"max_nested_len={self._max_nested_len})\"\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                f\"StageNetProcessor(is_nested={self._is_nested}, \"\n",
    "                f\"vocab_size={len(self.code_vocab)})\"\n",
    "            )\n",
    "\n",
    "\n",
    "@register_processor(\"stagenet_tensor_ex\")\n",
    "class StageNetTensorProcessorEx(FeatureProcessor):\n",
    "    \"\"\"\n",
    "    Feature processor for StageNet NUMERIC inputs with coupled value/time data.\n",
    "\n",
    "    This processor handles numeric feature sequences (flat or nested) and applies\n",
    "    forward-fill imputation to handle missing values (NaN/None).\n",
    "    For categorical codes, use StageNetProcessor instead.\n",
    "\n",
    "    Format:\n",
    "    {\n",
    "        \"value\": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],  # nested numerics\n",
    "        \"time\": [0.0, 1.5] or None\n",
    "    }\n",
    "\n",
    "    The processor automatically detects:\n",
    "    - List of numbers -> flat numeric sequences\n",
    "    - List of lists of numbers -> nested numeric sequences (feature vectors)\n",
    "\n",
    "    Imputation Strategy:\n",
    "    - Forward-fill: Missing values (NaN/None) are filled with the last observed\n",
    "      value for that feature dimension. If no prior value exists, 0.0 is used.\n",
    "    - Applied per feature dimension independently\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (time_tensor, value_tensor) where time_tensor can be None\n",
    "\n",
    "    Examples:\n",
    "        >>> # Case 1: Feature vectors with missing values\n",
    "        >>> processor = StageNetTensorProcessor()\n",
    "        >>> data = {\n",
    "        ...     \"value\": [[1.0, None, 3.0], [None, 5.0, 6.0], [7.0, 8.0, None]],\n",
    "        ...     \"time\": [0.0, 1.5, 3.0]\n",
    "        ... }\n",
    "        >>> time, values = processor.process(data)\n",
    "        >>> values  # [[1.0, 0.0, 3.0], [1.0, 5.0, 6.0], [7.0, 8.0, 6.0]]\n",
    "        >>> values.dtype  # torch.float32\n",
    "        >>> time.shape    # (3,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._size = None  # Feature dimension (set during fit)\n",
    "        self._is_nested = None\n",
    "\n",
    "    def fit(self, samples: List[Dict], key: str) -> None:\n",
    "        \"\"\"Determine input structure.\n",
    "\n",
    "        Args:\n",
    "            samples: List of sample dictionaries\n",
    "            key: The key in samples that contains tuple (time, values)\n",
    "        \"\"\"\n",
    "        # Examine first non-None sample to determine structure\n",
    "        for sample in samples:\n",
    "            if key in sample and sample[key] is not None:\n",
    "                # Unpack tuple: (time, values)\n",
    "                time_data, value_data = sample[key]\n",
    "\n",
    "                # Determine nesting level for numerics\n",
    "                if isinstance(value_data, list) and len(value_data) > 0:\n",
    "                    first_elem = value_data[0]\n",
    "\n",
    "                    if isinstance(first_elem, (int, float)):\n",
    "                        # Flat numeric: [1.5, 2.0, ...]\n",
    "                        self._is_nested = False\n",
    "                        self._size = 1\n",
    "                    elif isinstance(first_elem, list):\n",
    "                        if len(first_elem) > 0:\n",
    "                            if isinstance(first_elem[0], (int, float)):\n",
    "                                # Nested numerics: [[1.0, 2.0], [3.0, 4.0]]\n",
    "                                self._is_nested = True\n",
    "                                self._size = len(first_elem)\n",
    "                break\n",
    "\n",
    "    def process(\n",
    "        self, value: Tuple[Optional[List], List]\n",
    "    ) -> Tuple[Optional[torch.Tensor], torch.Tensor]:\n",
    "        \"\"\"Process tuple format numeric data into tensors.\n",
    "\n",
    "        Applies forward-fill imputation to handle NaN/None values.\n",
    "        For each feature dimension, missing values are filled with the\n",
    "        last observed value (or 0.0 if no prior value exists).\n",
    "\n",
    "        Args:\n",
    "            value: Tuple of (time, values) where values are numerics\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (time_tensor, value_tensor), time can be None\n",
    "        \"\"\"\n",
    "        # Unpack tuple: (time, values)\n",
    "        time_data, value_data = value\n",
    "\n",
    "        # Convert to numpy for easier imputation handling\n",
    "        import numpy as np\n",
    "\n",
    "        value_array = np.array(value_data, dtype=float)\n",
    "\n",
    "        # Apply forward-fill imputation\n",
    "        if value_array.ndim == 1:\n",
    "            # Flat numeric: [1.5, 2.0, nan, 3.0, ...]\n",
    "            last_value = 0.0\n",
    "            for i in range(len(value_array)):\n",
    "                if not np.isnan(value_array[i]):\n",
    "                    last_value = value_array[i]\n",
    "                else:\n",
    "                    value_array[i] = last_value\n",
    "        elif value_array.ndim == 2:\n",
    "            # Feature vectors: [[1.0, nan, 3.0], [nan, 5.0, 6.0]]\n",
    "            num_features = value_array.shape[1]\n",
    "            for f in range(num_features):\n",
    "                last_value = 0.0\n",
    "                for t in range(value_array.shape[0]):\n",
    "                    if not np.isnan(value_array[t, f]):\n",
    "                        last_value = value_array[t, f]\n",
    "                    else:\n",
    "                        value_array[t, f] = last_value\n",
    "\n",
    "        # Convert to float tensor\n",
    "        value_tensor = torch.tensor(value_array, dtype=torch.float)\n",
    "\n",
    "        # Process time if present\n",
    "        time_tensor = None\n",
    "        if time_data is not None and len(time_data) > 0:\n",
    "            # Handle both [0.0, 1.5] and [[0.0], [1.5]] formats\n",
    "            if isinstance(time_data[0], list):\n",
    "                # Flatten [[0.0], [1.5]] -> [0.0, 1.5]\n",
    "                time_data = [t[0] if isinstance(t, list) else t for t in time_data]\n",
    "            time_tensor = torch.tensor(time_data, dtype=torch.float)\n",
    "\n",
    "        return (time_tensor, value_tensor)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"Return feature dimension.\"\"\"\n",
    "        return self._size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"StageNetTensorProcessor(is_nested={self._is_nested}, \"\n",
    "            f\"feature_dim={self._size})\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d590f39",
   "metadata": {},
   "source": [
    "## Defining a Our StageNet-specific Task\n",
    "\n",
    "We'll predict patient mortality using StageNet across time-series data from multiple visits. Each visit includes:\n",
    "\n",
    "- Diagnosis codes\n",
    "- Procedure codes\n",
    "- Lab events\n",
    "\n",
    "Here, each feature will also need have its own corresponding time intervals. As defined by the StageNet paper, each time interval is defined as the difference in time between the current visit and the previous visit. \n",
    "\n",
    "To define a task, specify the `__call__` method, input schema, and output schema. For a detailed explanation, see [this tutorial](https://colab.research.google.com/drive/1kKKBVS_GclHoYTbnOtjyYnSee79hsyT?usp=sharing).\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "Use `patient.get_events()` to retrieve all events from a specific table, with optional filtering. See the [MIMIC-IV YAML file](https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/configs/mimic4_ehr.yaml) for available tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2288cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, ClassVar, Dict, List\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from pyhealth.tasks.base_task import BaseTask\n",
    "from pyhealth.processors import StageNetProcessor, StageNetTensorProcessor\n",
    "\n",
    "class MortalityPredictionStageNetMIMIC4(BaseTask):\n",
    "    \"\"\"Task for predicting mortality using MIMIC-IV with StageNet format.\n",
    "\n",
    "    This task creates PATIENT-LEVEL samples (not visit-level) by aggregating\n",
    "    all admissions for each patient. ICD codes (diagnoses + procedures) and\n",
    "    lab results across all visits are combined with time intervals calculated\n",
    "    from the patient's first admission timestamp.\n",
    "\n",
    "    Time Calculation:\n",
    "        - ICD codes: Hours from previous admission (0 for first visit,\n",
    "          then time intervals between consecutive visits)\n",
    "        - Labs: Hours from admission start (within-visit measurements)\n",
    "\n",
    "    Lab Processing:\n",
    "        - 10-dimensional vectors (one per lab category)\n",
    "        - Multiple itemids per category → take first observed value\n",
    "        - Missing categories → None/NaN in vector\n",
    "\n",
    "    Attributes:\n",
    "        task_name (str): The name of the task.\n",
    "        input_schema (Dict[str, str]): The schema for input data:\n",
    "            - icd_codes: Combined diagnosis + procedure ICD codes\n",
    "              (stagenet format, nested by visit)\n",
    "            - labs: Lab results (stagenet_tensor, 10D vectors per timestamp)\n",
    "        output_schema (Dict[str, str]): The schema for output data:\n",
    "            - mortality: Binary indicator (1 if any admission had mortality)\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str = \"MortalityPredictionStageNetMIMIC4\"\n",
    "    input_schema: Dict[str, str] = {\n",
    "        \"icd_codes\": StageNetProcessor,\n",
    "        \"labs\": StageNetTensorProcessor,\n",
    "    }\n",
    "    output_schema: Dict[str, str] = {\"mortality\": \"binary\"}\n",
    "\n",
    "    # Organize lab items by category\n",
    "    # Each category will map to ONE dimension in the output vector\n",
    "    LAB_CATEGORIES: ClassVar[Dict[str, List[str]]] = {\n",
    "        \"Sodium\": [\"50824\", \"52455\", \"50983\", \"52623\"],\n",
    "        \"Potassium\": [\"50822\", \"52452\", \"50971\", \"52610\"],\n",
    "        \"Chloride\": [\"50806\", \"52434\", \"50902\", \"52535\"],\n",
    "        \"Bicarbonate\": [\"50803\", \"50804\"],\n",
    "        \"Glucose\": [\"50809\", \"52027\", \"50931\", \"52569\"],\n",
    "        \"Calcium\": [\"50808\", \"51624\"],\n",
    "        \"Magnesium\": [\"50960\"],\n",
    "        \"Anion Gap\": [\"50868\", \"52500\"],\n",
    "        \"Osmolality\": [\"52031\", \"50964\", \"51701\"],\n",
    "        \"Phosphate\": [\"50970\"],\n",
    "    }\n",
    "\n",
    "    # Ordered list of category names (defines vector dimension order)\n",
    "    LAB_CATEGORY_NAMES: ClassVar[List[str]] = [\n",
    "        \"Sodium\",\n",
    "        \"Potassium\",\n",
    "        \"Chloride\",\n",
    "        \"Bicarbonate\",\n",
    "        \"Glucose\",\n",
    "        \"Calcium\",\n",
    "        \"Magnesium\",\n",
    "        \"Anion Gap\",\n",
    "        \"Osmolality\",\n",
    "        \"Phosphate\",\n",
    "    ]\n",
    "\n",
    "    # Flat list of all lab item IDs for filtering\n",
    "    LABITEMS: ClassVar[List[str]] = [\n",
    "        item for itemids in LAB_CATEGORIES.values() for item in itemids\n",
    "    ]\n",
    "\n",
    "    def __call__(self, patient: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a patient to create mortality prediction samples.\n",
    "\n",
    "        Creates ONE sample per patient with all admissions aggregated.\n",
    "        Time intervals are calculated between consecutive admissions.\n",
    "\n",
    "        Args:\n",
    "            patient: Patient object with get_events method\n",
    "\n",
    "        Returns:\n",
    "            List with single sample containing patient_id, all conditions,\n",
    "            procedures, labs across visits, and final mortality label\n",
    "        \"\"\"\n",
    "        # Filter patients by age (>= 18)\n",
    "        demographics = patient.get_events(event_type=\"patients\")\n",
    "        if not demographics:\n",
    "            return []\n",
    "\n",
    "        demographics = demographics[0]\n",
    "        try:\n",
    "            anchor_age = int(demographics.anchor_age)\n",
    "            if anchor_age < 18:\n",
    "                return []\n",
    "        except (ValueError, TypeError, AttributeError):\n",
    "            # If age can't be determined, skip patient\n",
    "            return []\n",
    "\n",
    "        # Get all admissions\n",
    "        admissions = patient.get_events(event_type=\"admissions\")\n",
    "        if len(admissions) < 1:\n",
    "            return []\n",
    "\n",
    "        # Initialize aggregated data structures\n",
    "        # List of ICD codes (diagnoses + procedures) per visit\n",
    "        all_icd_codes = []\n",
    "        all_icd_times = []  # Time from previous admission per visit\n",
    "        all_lab_values = []  # List of 10D lab vectors\n",
    "        all_lab_times = []  # Time from admission start per measurement\n",
    "\n",
    "        # Track previous admission timestamp for interval calculation\n",
    "        previous_admission_time = None\n",
    "\n",
    "        # Track if patient had any mortality event\n",
    "        final_mortality = 0\n",
    "\n",
    "        # Process each admission\n",
    "        for i, admission in enumerate(admissions):\n",
    "            # Parse admission and discharge times\n",
    "            try:\n",
    "                admission_time = admission.timestamp\n",
    "                admission_dischtime = datetime.strptime(\n",
    "                    admission.dischtime, \"%Y-%m-%d %H:%M:%S\"\n",
    "                )\n",
    "            except (ValueError, AttributeError):\n",
    "                # Skip if timestamps invalid\n",
    "                continue\n",
    "\n",
    "            # Skip if discharge is before admission (data quality issue)\n",
    "            if admission_dischtime < admission_time:\n",
    "                continue\n",
    "\n",
    "            # Calculate time from previous admission (in hours)\n",
    "            # First admission will have time = 0\n",
    "            if previous_admission_time is None:\n",
    "                time_from_previous = 0.0\n",
    "            else:\n",
    "                time_from_previous = (\n",
    "                    admission_time - previous_admission_time\n",
    "                ).total_seconds() / 3600.0\n",
    "\n",
    "            # Update previous admission time for next iteration\n",
    "            previous_admission_time = admission_time\n",
    "\n",
    "            # Update mortality label if this admission had mortality\n",
    "            try:\n",
    "                if int(admission.hospital_expire_flag) == 1:\n",
    "                    final_mortality = 1\n",
    "            except (ValueError, TypeError, AttributeError):\n",
    "                pass\n",
    "\n",
    "            # Get diagnosis codes for this admission using hadm_id\n",
    "            diagnoses_icd = patient.get_events(\n",
    "                event_type=\"diagnoses_icd\",\n",
    "                filters=[(\"hadm_id\", \"==\", admission.hadm_id)],\n",
    "            )\n",
    "            visit_diagnoses = [\n",
    "                event.icd_code\n",
    "                for event in diagnoses_icd\n",
    "                if hasattr(event, \"icd_code\") and event.icd_code\n",
    "            ]\n",
    "\n",
    "            # Get procedure codes for this admission using hadm_id\n",
    "            procedures_icd = patient.get_events(\n",
    "                event_type=\"procedures_icd\",\n",
    "                filters=[(\"hadm_id\", \"==\", admission.hadm_id)],\n",
    "            )\n",
    "            visit_procedures = [\n",
    "                event.icd_code\n",
    "                for event in procedures_icd\n",
    "                if hasattr(event, \"icd_code\") and event.icd_code\n",
    "            ]\n",
    "\n",
    "            # Combine diagnoses and procedures into single ICD code list\n",
    "            visit_icd_codes = visit_diagnoses + visit_procedures\n",
    "\n",
    "            if visit_icd_codes:\n",
    "                all_icd_codes.append(visit_icd_codes)\n",
    "                all_icd_times.append(time_from_previous)\n",
    "\n",
    "            # Get lab events for this admission\n",
    "            labevents_df = patient.get_events(\n",
    "                event_type=\"labevents\",\n",
    "                start=admission_time,\n",
    "                end=admission_dischtime,\n",
    "                return_df=True,\n",
    "            )\n",
    "\n",
    "            # Filter to relevant lab items\n",
    "            labevents_df = labevents_df.filter(\n",
    "                pl.col(\"labevents/itemid\").is_in(self.LABITEMS)\n",
    "            )\n",
    "\n",
    "            # Parse storetime and filter\n",
    "            if labevents_df.height > 0:\n",
    "                labevents_df = labevents_df.with_columns(\n",
    "                    pl.col(\"labevents/storetime\").str.strptime(\n",
    "                        pl.Datetime, \"%Y-%m-%d %H:%M:%S\"\n",
    "                    )\n",
    "                )\n",
    "                labevents_df = labevents_df.filter(\n",
    "                    pl.col(\"labevents/storetime\") <= admission_dischtime\n",
    "                )\n",
    "\n",
    "                if labevents_df.height > 0:\n",
    "                    # Select relevant columns\n",
    "                    labevents_df = labevents_df.select(\n",
    "                        pl.col(\"timestamp\"),\n",
    "                        pl.col(\"labevents/itemid\"),\n",
    "                        pl.col(\"labevents/valuenum\").cast(pl.Float64),\n",
    "                    )\n",
    "\n",
    "                    # Group by timestamp and aggregate into 10D vectors\n",
    "                    # For each timestamp, create vector of lab categories\n",
    "                    unique_timestamps = sorted(\n",
    "                        labevents_df[\"timestamp\"].unique().to_list()\n",
    "                    )\n",
    "\n",
    "                    for lab_ts in unique_timestamps:\n",
    "                        # Get all lab events at this timestamp\n",
    "                        ts_labs = labevents_df.filter(pl.col(\"timestamp\") == lab_ts)\n",
    "\n",
    "                        # Create 10-dimensional vector (one per category)\n",
    "                        lab_vector = []\n",
    "                        for category_name in self.LAB_CATEGORY_NAMES:\n",
    "                            category_itemids = self.LAB_CATEGORIES[category_name]\n",
    "\n",
    "                            # Find first matching value for this category\n",
    "                            category_value = None\n",
    "                            for itemid in category_itemids:\n",
    "                                matching = ts_labs.filter(\n",
    "                                    pl.col(\"labevents/itemid\") == itemid\n",
    "                                )\n",
    "                                if matching.height > 0:\n",
    "                                    category_value = matching[\"labevents/valuenum\"][0]\n",
    "                                    break\n",
    "\n",
    "                            lab_vector.append(category_value)\n",
    "\n",
    "                        # Calculate time from admission start (hours)\n",
    "                        time_from_admission = (\n",
    "                            lab_ts - admission_time\n",
    "                        ).total_seconds() / 3600.0\n",
    "\n",
    "                        all_lab_values.append(lab_vector)\n",
    "                        all_lab_times.append(time_from_admission)\n",
    "\n",
    "        # Skip if no lab events (required for this task)\n",
    "        if len(all_lab_values) == 0:\n",
    "            return []\n",
    "\n",
    "        # Also skip if no ICD codes across all admissions\n",
    "        if len(all_icd_codes) == 0:\n",
    "            return []\n",
    "\n",
    "        # Format as tuples: (time, values)\n",
    "        # ICD codes: nested list with times\n",
    "        icd_codes_data = (all_icd_times, all_icd_codes)\n",
    "\n",
    "        # Labs: list of 10D vectors with times\n",
    "        labs_data = (all_lab_times, all_lab_values)\n",
    "\n",
    "        # Create single patient-level sample\n",
    "        sample = {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"icd_codes\": icd_codes_data,\n",
    "            \"labs\": labs_data,\n",
    "            \"mortality\": final_mortality,\n",
    "        }\n",
    "        return [sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b799ce",
   "metadata": {},
   "source": [
    "## Setting the task and caching the data for quicker use down the road\n",
    "We can finally set our task and get our training set below. Notice that we save a processed version of our dataset in .parquet files in our \"cache_dir\" here. We can also define a number of works for faster parallel processing (note this can be unstable if the value is too high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e01f7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting task MortalityPredictionStageNetMIMIC4 for mimic4 base dataset...\n",
      "Loading cached samples from /home/johnwu3/projects/mimic4_stagenet_cache/MortalityPredictionStageNetMIMIC4.parquet\n",
      "Loaded 137778 cached samples\n",
      "Label mortality vocab: {0: 0, 1: 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing samples: 100%|██████████| 137778/137778 [00:33<00:00, 4171.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 137778 samples for task MortalityPredictionStageNetMIMIC4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Apply StageNet mortality prediction task\n",
    "sample_dataset = base_dataset.set_task(\n",
    "    MortalityPredictionStageNetMIMIC4(),\n",
    "    num_workers=4,\n",
    "    cache_dir=\"/home/johnwu3/projects/mimic4_stagenet_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c765bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample structure:\n",
      "  Patient ID: 17503482\n",
      "ICD Codes: (tensor([  0.0000, 315.3167]), tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0],\n",
      "        [15, 16,  3,  4, 17,  7, 18, 19, 12, 14, 20,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0]]))\n",
      "  Labs shape: 3 timesteps\n",
      "  Mortality: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample\n",
    "sample = sample_dataset.samples[0]\n",
    "print(\"\\nSample structure:\")\n",
    "print(f\"  Patient ID: {sample['patient_id']}\")\n",
    "print(f\"ICD Codes: {sample['icd_codes']}\")\n",
    "print(f\"  Labs shape: {len(sample['labs'][0])} timesteps\")\n",
    "print(f\"  Mortality: {sample['mortality']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65704934",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Splits and Training\n",
    "\n",
    "This section fundamentally follows any typical training pipeline. We don't recommend the PyHealth trainer beyond just testing out baselines, but any code you write here should flexibly translate to more advanced deep learning training packages like PyTorch lightning and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1708dca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'icd_codes': pyhealth.processors.stagenet_processor.StageNetProcessor,\n",
       " 'labs': pyhealth.processors.stagenet_processor.StageNetTensorProcessor}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset.input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0333b99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model initialized with 9337777 parameters\n",
      "StageNet(\n",
      "  (embedding_model): EmbeddingModel(embedding_layers=ModuleDict(\n",
      "    (icd_codes): Embedding(36681, 128, padding_idx=0)\n",
      "    (labs): Linear(in_features=10, out_features=128, bias=True)\n",
      "  ))\n",
      "  (stagenet): ModuleDict(\n",
      "    (icd_codes): StageNetLayer(\n",
      "      (kernel): Linear(in_features=129, out_features=1542, bias=True)\n",
      "      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)\n",
      "      (nn_scale): Linear(in_features=384, out_features=64, bias=True)\n",
      "      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)\n",
      "      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))\n",
      "      (nn_dropconnect): Dropout(p=0.3, inplace=False)\n",
      "      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)\n",
      "      (nn_dropout): Dropout(p=0.3, inplace=False)\n",
      "      (nn_dropres): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "    (labs): StageNetLayer(\n",
      "      (kernel): Linear(in_features=129, out_features=1542, bias=True)\n",
      "      (recurrent_kernel): Linear(in_features=385, out_features=1542, bias=True)\n",
      "      (nn_scale): Linear(in_features=384, out_features=64, bias=True)\n",
      "      (nn_rescale): Linear(in_features=64, out_features=384, bias=True)\n",
      "      (nn_conv): Conv1d(384, 384, kernel_size=(10,), stride=(1,))\n",
      "      (nn_dropconnect): Dropout(p=0.3, inplace=False)\n",
      "      (nn_dropconnect_r): Dropout(p=0.3, inplace=False)\n",
      "      (nn_dropout): Dropout(p=0.3, inplace=False)\n",
      "      (nn_dropres): Dropout(p=0.3, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
      ")\n",
      "Metrics: ['pr_auc', 'roc_auc', 'accuracy', 'f1']\n",
      "Device: cuda:4\n",
      "\n",
      "Training:\n",
      "Batch size: 256\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 1e-05}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: None\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x7eead4788700>\n",
      "Monitor: roc_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 / 1: 100%|██████████| 431/431 [08:32<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-431 ---\n",
      "loss: 0.4052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation: 100%|██████████| 54/54 [00:14<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-0, step-431 ---\n",
      "pr_auc: 0.0840\n",
      "roc_auc: 0.5105\n",
      "accuracy: 0.9439\n",
      "f1: 0.0000\n",
      "loss: 0.2540\n",
      "New best roc_auc score (0.5105) at epoch-0, step-431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 54/54 [00:15<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "  pr_auc: 0.0817\n",
      "  roc_auc: 0.4831\n",
      "  accuracy: 0.9382\n",
      "  f1: 0.0000\n",
      "  loss: 0.2813\n",
      "\n",
      "Sample predictions:\n",
      "  Predicted probabilities: tensor([[0.0171],\n",
      "        [0.0088],\n",
      "        [0.0096],\n",
      "        [0.0073],\n",
      "        [0.0144]], device='cuda:4')\n",
      "  True labels: tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:4')\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Split dataset\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(\n",
    "    sample_dataset, [0.8, 0.1, 0.1]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = get_dataloader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = get_dataloader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = get_dataloader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# STEP 4: Initialize StageNet model\n",
    "model = StageNet(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=128,\n",
    "    chunk_size=128,\n",
    "    levels=3,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel initialized with {num_params} parameters\")\n",
    "\n",
    "# STEP 5: Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=\"cuda:4\",  # or \"cpu\"\n",
    "    metrics=[\"pr_auc\", \"roc_auc\", \"accuracy\", \"f1\"],\n",
    ")\n",
    "\n",
    "# 1 epoch for demonstration; increase for real training, it should work pretty well closer to 50\n",
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=1,\n",
    "    monitor=\"roc_auc\",\n",
    "    optimizer_params={\"lr\": 1e-5},\n",
    ")\n",
    "\n",
    "# STEP 6: Evaluate on test set\n",
    "results = trainer.evaluate(test_loader)\n",
    "print(\"\\nTest Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# STEP 7: Inspect model predictions\n",
    "sample_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    output = model(**sample_batch)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(f\"  Predicted probabilities: {output['y_prob'][:5]}\")\n",
    "print(f\"  True labels: {output['y_true'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d1b7f",
   "metadata": {},
   "source": [
    "## Post-hoc ML processing (TBD)\n",
    "We note that once the model's trained and evaluation metrics are derived. People may be interested in things like post-hoc interpretability or uncertainty quantification.\n",
    "\n",
    "We note that this is quite a work-in-progress for PyHealth 2.0, but the roadmap includes the following:\n",
    "\n",
    "- Layer-wise relevance propagation (deep NN-based interpretability)\n",
    "- Conformal Prediction: We do have many other UQ techniques [here](https://pyhealth.readthedocs.io/en/latest/api/calib.html)\n",
    "\n",
    "For quick and dirty feature attribution, I would highly recommend something like [SHAP](https://shap.readthedocs.io/en/latest/). For conceptual interpretability within the embedding space, I highly recommend looking into sparse autoencoders."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medical_coding_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
