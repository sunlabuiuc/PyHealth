{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec17da31",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Started\n",
    "\n",
    "## Installation\n",
    "\n",
    "Install the latest alpha release of StageNet modernized for PyHealth:\n",
    "\n",
    "```bash\n",
    "pip install pyhealth==2.0a8\n",
    "```\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "Load the PyHealth dataset for mortality prediction.\n",
    "\n",
    "PyHealth datasets use a `config.yaml` file to define:\n",
    "- Input tables (.csv, .tsv, etc.)\n",
    "- Features to extract\n",
    "- Aggregation methods\n",
    "\n",
    "The result is a single dataframe where each row represents one patient and their features.\n",
    "\n",
    "For more details on PyHealth datasets, see [this resource](https://colab.research.google.com/drive/1voSx7wEfzXfEf2sIfW6b-8p1KqMyuWxK#scrollTo=NSrb2PGFqUgS).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example of using StageNet for mortality prediction on MIMIC-IV.\n",
    "\n",
    "This example demonstrates:\n",
    "1. Loading MIMIC-IV data\n",
    "2. Applying the MortalityPredictionStageNetMIMIC4 task\n",
    "3. Creating a SampleDataset with StageNet processors\n",
    "4. Training a StageNet model\n",
    "\"\"\"\n",
    "\n",
    "from pyhealth.datasets import (\n",
    "    MIMIC4Dataset,\n",
    "    get_dataloader,\n",
    "    split_by_patient,\n",
    ")\n",
    "from pyhealth.models import StageNet\n",
    "from pyhealth.tasks import MortalityPredictionStageNetMIMIC4\n",
    "from pyhealth.trainer import Trainer\n",
    "import torch\n",
    "\n",
    "# STEP 1: Load MIMIC-IV base dataset\n",
    "base_dataset = MIMIC4Dataset(\n",
    "    ehr_root=\"/srv/local/data/physionet.org/files/mimiciv/2.2/\",\n",
    "    ehr_tables=[\n",
    "        \"patients\",\n",
    "        \"admissions\",\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"labevents\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e0fc54",
   "metadata": {},
   "source": [
    "## Defining a PyHealth Task\n",
    "\n",
    "We'll predict patient mortality using StageNet across time-series data from multiple visits. Each visit includes:\n",
    "\n",
    "- Diagnosis codes\n",
    "- Procedure codes\n",
    "- Lab events\n",
    "\n",
    "To define a task, specify the `__call__` method, input schema, and output schema. For a detailed explanation, see [this tutorial](https://colab.research.google.com/drive/1kKKBVS_GclHoYTbnOtjyYnSee79hsyT?usp=sharing).\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "Use `patient.get_events()` to retrieve all events from a specific table, with optional filtering. See the [MIMIC-IV YAML file](https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/configs/mimic4_ehr.yaml) for available tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70fd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import Any, ClassVar, Dict, List\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from pyhealth.tasks.base_task import BaseTask\n",
    "\n",
    "\n",
    "class MortalityPredictionStageNetMIMIC4(BaseTask):\n",
    "    \"\"\"Task for predicting mortality using MIMIC-IV with StageNet format.\n",
    "\n",
    "    This task creates PATIENT-LEVEL samples (not visit-level) by aggregating\n",
    "    all admissions for each patient. ICD codes (diagnoses + procedures) and\n",
    "    lab results across all visits are combined with time intervals calculated\n",
    "    from the patient's first admission timestamp.\n",
    "\n",
    "    Time Calculation:\n",
    "        - ICD codes: Hours from previous admission (0 for first visit,\n",
    "          then time intervals between consecutive visits)\n",
    "        - Labs: Hours from admission start (within-visit measurements)\n",
    "\n",
    "    Lab Processing:\n",
    "        - 10-dimensional vectors (one per lab category)\n",
    "        - Multiple itemids per category → take first observed value\n",
    "        - Missing categories → None/NaN in vector\n",
    "\n",
    "    Attributes:\n",
    "        task_name (str): The name of the task.\n",
    "        input_schema (Dict[str, str]): The schema for input data:\n",
    "            - icd_codes: Combined diagnosis + procedure ICD codes\n",
    "              (stagenet format, nested by visit)\n",
    "            - labs: Lab results (stagenet_tensor, 10D vectors per timestamp)\n",
    "        output_schema (Dict[str, str]): The schema for output data:\n",
    "            - mortality: Binary indicator (1 if any admission had mortality)\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: str = \"MortalityPredictionStageNetMIMIC4\"\n",
    "    input_schema: Dict[str, str] = {\n",
    "        \"icd_codes\": \"stagenet\",\n",
    "        \"labs\": \"stagenet_tensor\",\n",
    "    }\n",
    "    output_schema: Dict[str, str] = {\"mortality\": \"binary\"}\n",
    "\n",
    "    # Organize lab items by category\n",
    "    # Each category will map to ONE dimension in the output vector\n",
    "    LAB_CATEGORIES: ClassVar[Dict[str, List[str]]] = {\n",
    "        \"Sodium\": [\"50824\", \"52455\", \"50983\", \"52623\"],\n",
    "        \"Potassium\": [\"50822\", \"52452\", \"50971\", \"52610\"],\n",
    "        \"Chloride\": [\"50806\", \"52434\", \"50902\", \"52535\"],\n",
    "        \"Bicarbonate\": [\"50803\", \"50804\"],\n",
    "        \"Glucose\": [\"50809\", \"52027\", \"50931\", \"52569\"],\n",
    "        \"Calcium\": [\"50808\", \"51624\"],\n",
    "        \"Magnesium\": [\"50960\"],\n",
    "        \"Anion Gap\": [\"50868\", \"52500\"],\n",
    "        \"Osmolality\": [\"52031\", \"50964\", \"51701\"],\n",
    "        \"Phosphate\": [\"50970\"],\n",
    "    }\n",
    "\n",
    "    # Ordered list of category names (defines vector dimension order)\n",
    "    LAB_CATEGORY_NAMES: ClassVar[List[str]] = [\n",
    "        \"Sodium\",\n",
    "        \"Potassium\",\n",
    "        \"Chloride\",\n",
    "        \"Bicarbonate\",\n",
    "        \"Glucose\",\n",
    "        \"Calcium\",\n",
    "        \"Magnesium\",\n",
    "        \"Anion Gap\",\n",
    "        \"Osmolality\",\n",
    "        \"Phosphate\",\n",
    "    ]\n",
    "\n",
    "    # Flat list of all lab item IDs for filtering\n",
    "    LABITEMS: ClassVar[List[str]] = [\n",
    "        item for itemids in LAB_CATEGORIES.values() for item in itemids\n",
    "    ]\n",
    "\n",
    "    def __call__(self, patient: Any) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process a patient to create mortality prediction samples.\n",
    "\n",
    "        Creates ONE sample per patient with all admissions aggregated.\n",
    "        Time intervals are calculated between consecutive admissions.\n",
    "\n",
    "        Args:\n",
    "            patient: Patient object with get_events method\n",
    "\n",
    "        Returns:\n",
    "            List with single sample containing patient_id, all conditions,\n",
    "            procedures, labs across visits, and final mortality label\n",
    "        \"\"\"\n",
    "        # Filter patients by age (>= 18)\n",
    "        demographics = patient.get_events(event_type=\"patients\")\n",
    "        if not demographics:\n",
    "            return []\n",
    "\n",
    "        demographics = demographics[0]\n",
    "        try:\n",
    "            anchor_age = int(demographics.anchor_age)\n",
    "            if anchor_age < 18:\n",
    "                return []\n",
    "        except (ValueError, TypeError, AttributeError):\n",
    "            # If age can't be determined, skip patient\n",
    "            return []\n",
    "\n",
    "        # Get all admissions\n",
    "        admissions = patient.get_events(event_type=\"admissions\")\n",
    "        if len(admissions) < 1:\n",
    "            return []\n",
    "\n",
    "        # Initialize aggregated data structures\n",
    "        # List of ICD codes (diagnoses + procedures) per visit\n",
    "        all_icd_codes = []\n",
    "        all_icd_times = []  # Time from previous admission per visit\n",
    "        all_lab_values = []  # List of 10D lab vectors\n",
    "        all_lab_times = []  # Time from admission start per measurement\n",
    "\n",
    "        # Track previous admission timestamp for interval calculation\n",
    "        previous_admission_time = None\n",
    "\n",
    "        # Track if patient had any mortality event\n",
    "        final_mortality = 0\n",
    "\n",
    "        # Process each admission\n",
    "        for i, admission in enumerate(admissions):\n",
    "            # Parse admission and discharge times\n",
    "            try:\n",
    "                admission_time = admission.timestamp\n",
    "                admission_dischtime = datetime.strptime(\n",
    "                    admission.dischtime, \"%Y-%m-%d %H:%M:%S\"\n",
    "                )\n",
    "            except (ValueError, AttributeError):\n",
    "                # Skip if timestamps invalid\n",
    "                continue\n",
    "\n",
    "            # Skip if discharge is before admission (data quality issue)\n",
    "            if admission_dischtime < admission_time:\n",
    "                continue\n",
    "\n",
    "            # Calculate time from previous admission (in hours)\n",
    "            # First admission will have time = 0\n",
    "            if previous_admission_time is None:\n",
    "                time_from_previous = 0.0\n",
    "            else:\n",
    "                time_from_previous = (\n",
    "                    admission_time - previous_admission_time\n",
    "                ).total_seconds() / 3600.0\n",
    "\n",
    "            # Update previous admission time for next iteration\n",
    "            previous_admission_time = admission_time\n",
    "\n",
    "            # Update mortality label if this admission had mortality\n",
    "            try:\n",
    "                if int(admission.hospital_expire_flag) == 1:\n",
    "                    final_mortality = 1\n",
    "            except (ValueError, TypeError, AttributeError):\n",
    "                pass\n",
    "\n",
    "            # Get diagnosis codes for this admission using hadm_id\n",
    "            diagnoses_icd = patient.get_events(\n",
    "                event_type=\"diagnoses_icd\",\n",
    "                filters=[(\"hadm_id\", \"==\", admission.hadm_id)],\n",
    "            )\n",
    "            visit_diagnoses = [\n",
    "                event.icd_code\n",
    "                for event in diagnoses_icd\n",
    "                if hasattr(event, \"icd_code\") and event.icd_code\n",
    "            ]\n",
    "\n",
    "            # Get procedure codes for this admission using hadm_id\n",
    "            procedures_icd = patient.get_events(\n",
    "                event_type=\"procedures_icd\",\n",
    "                filters=[(\"hadm_id\", \"==\", admission.hadm_id)],\n",
    "            )\n",
    "            visit_procedures = [\n",
    "                event.icd_code\n",
    "                for event in procedures_icd\n",
    "                if hasattr(event, \"icd_code\") and event.icd_code\n",
    "            ]\n",
    "\n",
    "            # Combine diagnoses and procedures into single ICD code list\n",
    "            visit_icd_codes = visit_diagnoses + visit_procedures\n",
    "\n",
    "            if visit_icd_codes:\n",
    "                all_icd_codes.append(visit_icd_codes)\n",
    "                all_icd_times.append(time_from_previous)\n",
    "\n",
    "            # Get lab events for this admission\n",
    "            labevents_df = patient.get_events(\n",
    "                event_type=\"labevents\",\n",
    "                start=admission_time,\n",
    "                end=admission_dischtime,\n",
    "                return_df=True,\n",
    "            )\n",
    "\n",
    "            # Filter to relevant lab items\n",
    "            labevents_df = labevents_df.filter(\n",
    "                pl.col(\"labevents/itemid\").is_in(self.LABITEMS)\n",
    "            )\n",
    "\n",
    "            # Parse storetime and filter\n",
    "            if labevents_df.height > 0:\n",
    "                labevents_df = labevents_df.with_columns(\n",
    "                    pl.col(\"labevents/storetime\").str.strptime(\n",
    "                        pl.Datetime, \"%Y-%m-%d %H:%M:%S\"\n",
    "                    )\n",
    "                )\n",
    "                labevents_df = labevents_df.filter(\n",
    "                    pl.col(\"labevents/storetime\") <= admission_dischtime\n",
    "                )\n",
    "\n",
    "                if labevents_df.height > 0:\n",
    "                    # Select relevant columns\n",
    "                    labevents_df = labevents_df.select(\n",
    "                        pl.col(\"timestamp\"),\n",
    "                        pl.col(\"labevents/itemid\"),\n",
    "                        pl.col(\"labevents/valuenum\").cast(pl.Float64),\n",
    "                    )\n",
    "\n",
    "                    # Group by timestamp and aggregate into 10D vectors\n",
    "                    # For each timestamp, create vector of lab categories\n",
    "                    unique_timestamps = sorted(\n",
    "                        labevents_df[\"timestamp\"].unique().to_list()\n",
    "                    )\n",
    "\n",
    "                    for lab_ts in unique_timestamps:\n",
    "                        # Get all lab events at this timestamp\n",
    "                        ts_labs = labevents_df.filter(pl.col(\"timestamp\") == lab_ts)\n",
    "\n",
    "                        # Create 10-dimensional vector (one per category)\n",
    "                        lab_vector = []\n",
    "                        for category_name in self.LAB_CATEGORY_NAMES:\n",
    "                            category_itemids = self.LAB_CATEGORIES[category_name]\n",
    "\n",
    "                            # Find first matching value for this category\n",
    "                            category_value = None\n",
    "                            for itemid in category_itemids:\n",
    "                                matching = ts_labs.filter(\n",
    "                                    pl.col(\"labevents/itemid\") == itemid\n",
    "                                )\n",
    "                                if matching.height > 0:\n",
    "                                    category_value = matching[\"labevents/valuenum\"][0]\n",
    "                                    break\n",
    "\n",
    "                            lab_vector.append(category_value)\n",
    "\n",
    "                        # Calculate time from admission start (hours)\n",
    "                        time_from_admission = (\n",
    "                            lab_ts - admission_time\n",
    "                        ).total_seconds() / 3600.0\n",
    "\n",
    "                        all_lab_values.append(lab_vector)\n",
    "                        all_lab_times.append(time_from_admission)\n",
    "\n",
    "        # Skip if no lab events (required for this task)\n",
    "        if len(all_lab_values) == 0:\n",
    "            return []\n",
    "\n",
    "        # Also skip if no ICD codes across all admissions\n",
    "        if len(all_icd_codes) == 0:\n",
    "            return []\n",
    "\n",
    "        # Format as tuples: (time, values)\n",
    "        # ICD codes: nested list with times\n",
    "        icd_codes_data = (all_icd_times, all_icd_codes)\n",
    "\n",
    "        # Labs: list of 10D vectors with times\n",
    "        labs_data = (all_lab_times, all_lab_values)\n",
    "\n",
    "        # Create single patient-level sample\n",
    "        sample = {\n",
    "            \"patient_id\": patient.patient_id,\n",
    "            \"icd_codes\": icd_codes_data,\n",
    "            \"labs\": labs_data,\n",
    "            \"mortality\": final_mortality,\n",
    "        }\n",
    "        return [sample]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf0f1c",
   "metadata": {},
   "source": [
    "## Input and Output Schemas\n",
    "Input and output schemas map feature keys (e.g., \"labs\", \"icd_codes\") to StageNet processors. Each processor converts features into `StageNetFeature` objects used for training and inference.\n",
    "\n",
    "**Required format:** Each feature processed in our task call must follow this structure:\n",
    "```python\n",
    "\"feature\": (my_times_list, my_values_list)\n",
    "\n",
    "We offer two types of StageNet processors, one for categorical variables, and the other for numerical feature variables. Our goal here is to represent each feature as a pre-defined tuple (time, value) that we can later pass to StageNet for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3e055d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "\n",
    "from pyhealth.processors import register_processor\n",
    "from pyhealth.processors.base_processor import FeatureProcessor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StageNetFeature:\n",
    "    \"\"\"Container for StageNet feature with values and optional time intervals.\n",
    "\n",
    "    Attributes:\n",
    "        value: The feature tensor (1D for sequences, 2D for nested sequences, 3D for feature vectors)\n",
    "        time: Optional time interval tensor (1D, matching the sequence length of value)\n",
    "    \"\"\"\n",
    "\n",
    "    value: torch.Tensor\n",
    "    time: Optional[torch.Tensor] = None\n",
    "\n",
    "\n",
    "@register_processor(\"stagenet\")\n",
    "class StageNetProcessor(FeatureProcessor):\n",
    "    \"\"\"\n",
    "    Feature processor for StageNet CODE inputs with coupled value/time data.\n",
    "\n",
    "    This processor handles categorical code sequences (flat or nested).\n",
    "    For numeric features, use StageNetTensorProcessor instead.\n",
    "\n",
    "    Format:\n",
    "    {\n",
    "        \"value\": [\"code1\", \"code2\"] or [[\"A\", \"B\"], [\"C\"]],\n",
    "        \"time\": [0.0, 2.0, 1.3] or None\n",
    "    }\n",
    "\n",
    "    The processor automatically detects:\n",
    "    - List of strings -> flat code sequences\n",
    "    - List of lists of strings -> nested code sequences\n",
    "\n",
    "    Time intervals should be simple lists of scalars, one per sequence position.\n",
    "\n",
    "    Examples:\n",
    "        >>> # Case 1: Code sequence with time\n",
    "        >>> processor = StageNetProcessor()\n",
    "        >>> data = {\"value\": [\"code1\", \"code2\", \"code3\"], \"time\": [0.0, 1.5, 2.3]}\n",
    "        >>> result = processor.process(data)\n",
    "        >>> result.value.shape  # (3,) - sequence of code indices\n",
    "        >>> result.time.shape   # (3,) - time intervals\n",
    "\n",
    "        >>> # Case 2: Nested codes with time\n",
    "        >>> data = {\"value\": [[\"A\", \"B\"], [\"C\"]], \"time\": [0.0, 1.5]}\n",
    "        >>> result = processor.process(data)\n",
    "        >>> result.value.shape  # (2, max_inner_len) - padded nested sequences\n",
    "        >>> result.time.shape   # (2,)\n",
    "\n",
    "        >>> # Case 3: Feature vectors without time\n",
    "        >>> data = {\"value\": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], \"time\": None}\n",
    "        >>> result = processor.process(data)\n",
    "        >>> result.value.shape  # (2, 3)\n",
    "        >>> result.time         # None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.code_vocab: Dict[Any, int] = {\"<unk>\": -1, \"<pad>\": 0}\n",
    "        self._next_index = 1\n",
    "        self._is_nested = None  # Will be determined during fit\n",
    "        self._max_nested_len = None  # Max inner sequence length for nested codes\n",
    "\n",
    "    def fit(self, samples: List[Dict], key: str) -> None:\n",
    "        \"\"\"Build vocabulary and determine input structure.\n",
    "\n",
    "        Args:\n",
    "            samples: List of sample dictionaries\n",
    "            key: The key in samples that contains StageNet format data\n",
    "        \"\"\"\n",
    "        # Examine first non-None sample to determine structure\n",
    "        for sample in samples:\n",
    "            if key in sample and sample[key] is not None:\n",
    "                value_data = sample[key][\"value\"]\n",
    "\n",
    "                # Determine nesting level for codes\n",
    "                if isinstance(value_data, list) and len(value_data) > 0:\n",
    "                    first_elem = value_data[0]\n",
    "\n",
    "                    if isinstance(first_elem, str):\n",
    "                        # Case 1: [\"code1\", \"code2\", ...]\n",
    "                        self._is_nested = False\n",
    "                    elif isinstance(first_elem, list):\n",
    "                        if len(first_elem) > 0 and isinstance(first_elem[0], str):\n",
    "                            # Case 2: [[\"A\", \"B\"], [\"C\"], ...]\n",
    "                            self._is_nested = True\n",
    "                break\n",
    "\n",
    "        # Build vocabulary for codes and find max nested length\n",
    "        max_inner_len = 0\n",
    "        for sample in samples:\n",
    "            if key in sample and sample[key] is not None:\n",
    "                value_data = sample[key][\"value\"]\n",
    "\n",
    "                if self._is_nested:\n",
    "                    # Nested codes\n",
    "                    for inner_list in value_data:\n",
    "                        # Track max inner length\n",
    "                        max_inner_len = max(max_inner_len, len(inner_list))\n",
    "                        for code in inner_list:\n",
    "                            if code is not None and code not in self.code_vocab:\n",
    "                                self.code_vocab[code] = self._next_index\n",
    "                                self._next_index += 1\n",
    "                else:\n",
    "                    # Flat codes\n",
    "                    for code in value_data:\n",
    "                        if code is not None and code not in self.code_vocab:\n",
    "                            self.code_vocab[code] = self._next_index\n",
    "                            self._next_index += 1\n",
    "\n",
    "        # Store max nested length (at least 1 for empty sequences)\n",
    "        if self._is_nested:\n",
    "            self._max_nested_len = max(1, max_inner_len)\n",
    "\n",
    "    def process(self, value: Dict[str, Any]) -> StageNetFeature:\n",
    "        \"\"\"Process StageNet format data into tensors.\n",
    "\n",
    "        Args:\n",
    "            value: Dictionary with \"value\" and optional \"time\" keys\n",
    "\n",
    "        Returns:\n",
    "            StageNetFeature with value and time tensors\n",
    "        \"\"\"\n",
    "        value_data = value[\"value\"]\n",
    "        time_data = value.get(\"time\", None)\n",
    "\n",
    "        # Encode codes to indices\n",
    "        if self._is_nested:\n",
    "            # Nested codes: [[\"A\", \"B\"], [\"C\"]]\n",
    "            value_tensor = self._encode_nested_codes(value_data)\n",
    "        else:\n",
    "            # Flat codes: [\"code1\", \"code2\"]\n",
    "            value_tensor = self._encode_codes(value_data)\n",
    "\n",
    "        # Process time if present\n",
    "        time_tensor = None\n",
    "        if time_data is not None and len(time_data) > 0:\n",
    "            # Handle both [0.0, 1.5] and [[0.0], [1.5]] formats\n",
    "            if isinstance(time_data[0], list):\n",
    "                # Flatten [[0.0], [1.5]] -> [0.0, 1.5]\n",
    "                time_data = [t[0] if isinstance(t, list) else t for t in time_data]\n",
    "            time_tensor = torch.tensor(time_data, dtype=torch.float)\n",
    "\n",
    "        return StageNetFeature(value=value_tensor, time=time_tensor)\n",
    "\n",
    "    def _encode_codes(self, codes: List[str]) -> torch.Tensor:\n",
    "        \"\"\"Encode flat code list to indices.\"\"\"\n",
    "        # Handle empty code list - return single padding token\n",
    "        if len(codes) == 0:\n",
    "            return torch.tensor([self.code_vocab[\"<pad>\"]], dtype=torch.long)\n",
    "\n",
    "        indices = []\n",
    "        for code in codes:\n",
    "            if code is None or code not in self.code_vocab:\n",
    "                indices.append(self.code_vocab[\"<unk>\"])\n",
    "            else:\n",
    "                indices.append(self.code_vocab[code])\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def _encode_nested_codes(self, nested_codes: List[List[str]]) -> torch.Tensor:\n",
    "        \"\"\"Encode nested code lists to padded 2D tensor.\n",
    "\n",
    "        Pads all inner sequences to self._max_nested_len (global max).\n",
    "        \"\"\"\n",
    "        # Handle empty nested codes (no visits/events)\n",
    "        # Return single padding token with shape (1, max_len)\n",
    "        if len(nested_codes) == 0:\n",
    "            pad_token = self.code_vocab[\"<pad>\"]\n",
    "            return torch.tensor([[pad_token] * self._max_nested_len], dtype=torch.long)\n",
    "\n",
    "        encoded_sequences = []\n",
    "        # Use global max length determined during fit\n",
    "        max_len = self._max_nested_len\n",
    "\n",
    "        for inner_codes in nested_codes:\n",
    "            indices = []\n",
    "            for code in inner_codes:\n",
    "                if code is None or code not in self.code_vocab:\n",
    "                    indices.append(self.code_vocab[\"<unk>\"])\n",
    "                else:\n",
    "                    indices.append(self.code_vocab[code])\n",
    "            # Pad to GLOBAL max_len\n",
    "            while len(indices) < max_len:\n",
    "                indices.append(self.code_vocab[\"<pad>\"])\n",
    "            encoded_sequences.append(indices)\n",
    "\n",
    "        return torch.tensor(encoded_sequences, dtype=torch.long)\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return vocabulary size.\"\"\"\n",
    "        return len(self.code_vocab)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self._is_nested:\n",
    "            return (\n",
    "                f\"StageNetProcessor(is_nested={self._is_nested}, \"\n",
    "                f\"vocab_size={len(self.code_vocab)}, \"\n",
    "                f\"max_nested_len={self._max_nested_len})\"\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                f\"StageNetProcessor(is_nested={self._is_nested}, \"\n",
    "                f\"vocab_size={len(self.code_vocab)})\"\n",
    "            )\n",
    "\n",
    "\n",
    "@register_processor(\"stagenet_tensor\")\n",
    "class StageNetTensorProcessor(FeatureProcessor):\n",
    "    \"\"\"\n",
    "    Feature processor for StageNet NUMERIC inputs with coupled value/time data.\n",
    "\n",
    "    This processor handles numeric feature sequences (flat or nested) and applies\n",
    "    forward-fill imputation to handle missing values (NaN/None).\n",
    "    For categorical codes, use StageNetProcessor instead.\n",
    "\n",
    "    Format:\n",
    "    {\n",
    "        \"value\": [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],  # nested numerics\n",
    "        \"time\": [0.0, 1.5] or None\n",
    "    }\n",
    "\n",
    "    The processor automatically detects:\n",
    "    - List of numbers -> flat numeric sequences\n",
    "    - List of lists of numbers -> nested numeric sequences (feature vectors)\n",
    "\n",
    "    Imputation Strategy:\n",
    "    - Forward-fill: Missing values (NaN/None) are filled with the last observed\n",
    "      value for that feature dimension. If no prior value exists, 0.0 is used.\n",
    "    - Applied per feature dimension independently\n",
    "\n",
    "    Examples:\n",
    "        >>> # Case 1: Feature vectors with missing values\n",
    "        >>> processor = StageNetTensorProcessor()\n",
    "        >>> data = {\n",
    "        ...     \"value\": [[1.0, None, 3.0], [None, 5.0, 6.0], [7.0, 8.0, None]],\n",
    "        ...     \"time\": [0.0, 1.5, 3.0]\n",
    "        ... }\n",
    "        >>> result = processor.process(data)\n",
    "        >>> result.value  # [[1.0, 0.0, 3.0], [1.0, 5.0, 6.0], [7.0, 8.0, 6.0]]\n",
    "        >>> result.value.dtype  # torch.float32\n",
    "        >>> result.time.shape   # (3,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self._size = None  # Feature dimension (set during fit)\n",
    "        self._is_nested = None\n",
    "\n",
    "    def fit(self, samples: List[Dict], key: str) -> None:\n",
    "        \"\"\"Determine input structure.\n",
    "\n",
    "        Args:\n",
    "            samples: List of sample dictionaries\n",
    "            key: The key in samples that contains StageNet format data\n",
    "        \"\"\"\n",
    "        # Examine first non-None sample to determine structure\n",
    "        for sample in samples:\n",
    "            if key in sample and sample[key] is not None:\n",
    "                value_data = sample[key][\"value\"]\n",
    "\n",
    "                # Determine nesting level for numerics\n",
    "                if isinstance(value_data, list) and len(value_data) > 0:\n",
    "                    first_elem = value_data[0]\n",
    "\n",
    "                    if isinstance(first_elem, (int, float)):\n",
    "                        # Flat numeric: [1.5, 2.0, ...]\n",
    "                        self._is_nested = False\n",
    "                        self._size = 1\n",
    "                    elif isinstance(first_elem, list):\n",
    "                        if len(first_elem) > 0:\n",
    "                            if isinstance(first_elem[0], (int, float)):\n",
    "                                # Nested numerics: [[1.0, 2.0], [3.0, 4.0]]\n",
    "                                self._is_nested = True\n",
    "                                self._size = len(first_elem)\n",
    "                break\n",
    "\n",
    "    def process(self, value: Dict[str, Any]) -> StageNetFeature:\n",
    "        \"\"\"Process StageNet format numeric data into tensors.\n",
    "\n",
    "        Applies forward-fill imputation to handle NaN/None values in the data.\n",
    "        For each feature dimension, missing values are filled with the last\n",
    "        observed value (or 0.0 if no prior value exists).\n",
    "\n",
    "        Args:\n",
    "            value: Dictionary with \"value\" and optional \"time\" keys\n",
    "\n",
    "        Returns:\n",
    "            StageNetFeature with value and time tensors (imputed)\n",
    "        \"\"\"\n",
    "        value_data = value[\"value\"]\n",
    "        time_data = value.get(\"time\", None)\n",
    "\n",
    "        # Convert to numpy for easier imputation handling\n",
    "        import numpy as np\n",
    "\n",
    "        value_array = np.array(value_data, dtype=float)\n",
    "\n",
    "        # Apply forward-fill imputation\n",
    "        if value_array.ndim == 1:\n",
    "            # Flat numeric: [1.5, 2.0, nan, 3.0, ...]\n",
    "            last_value = 0.0\n",
    "            for i in range(len(value_array)):\n",
    "                if not np.isnan(value_array[i]):\n",
    "                    last_value = value_array[i]\n",
    "                else:\n",
    "                    value_array[i] = last_value\n",
    "        elif value_array.ndim == 2:\n",
    "            # Feature vectors: [[1.0, nan, 3.0], [nan, 5.0, 6.0], ...]\n",
    "            num_features = value_array.shape[1]\n",
    "            for f in range(num_features):\n",
    "                last_value = 0.0\n",
    "                for t in range(value_array.shape[0]):\n",
    "                    if not np.isnan(value_array[t, f]):\n",
    "                        last_value = value_array[t, f]\n",
    "                    else:\n",
    "                        value_array[t, f] = last_value\n",
    "\n",
    "        # Convert to float tensor\n",
    "        value_tensor = torch.tensor(value_array, dtype=torch.float)\n",
    "\n",
    "        # Process time if present\n",
    "        time_tensor = None\n",
    "        if time_data is not None and len(time_data) > 0:\n",
    "            # Handle both [0.0, 1.5] and [[0.0], [1.5]] formats\n",
    "            if isinstance(time_data[0], list):\n",
    "                # Flatten [[0.0], [1.5]] -> [0.0, 1.5]\n",
    "                time_data = [t[0] if isinstance(t, list) else t for t in time_data]\n",
    "            time_tensor = torch.tensor(time_data, dtype=torch.float)\n",
    "\n",
    "        return StageNetFeature(value=value_tensor, time=time_tensor)\n",
    "\n",
    "    @property\n",
    "    def size(self):\n",
    "        \"\"\"Return feature dimension.\"\"\"\n",
    "        return self._size\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"StageNetTensorProcessor(is_nested={self._is_nested}, \"\n",
    "            f\"feature_dim={self._size})\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b799ce",
   "metadata": {},
   "source": [
    "## Setting the task and caching the data for quicker use down the road\n",
    "We can finally set our task and get our training set below. Notice that we save a processed version of our dataset in .parquet files in our \"cache_dir\" here. We can also define a number of works for faster parallel processing (note this can be unstable if the value is too high)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e01f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Apply StageNet mortality prediction task\n",
    "sample_dataset = base_dataset.set_task(\n",
    "    MortalityPredictionStageNetMIMIC4(),\n",
    "    num_workers=4,\n",
    "    cache_dir=\"../../mimic4_stagenet_cache\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c765bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample\n",
    "sample = sample_dataset.samples[0]\n",
    "print(\"\\nSample structure:\")\n",
    "print(f\"  Patient ID: {sample['patient_id']}\")\n",
    "print(f\"ICD Codes: {sample['icd_codes']}\")\n",
    "print(f\"  Labs shape: {len(sample['labs'].value)} timesteps\")\n",
    "print(f\"  Mortality: {sample['mortality']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65704934",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Splits and Training\n",
    "\n",
    "This section fundamentally follows any typical training pipeline. We don't recommend the PyHealth trainer beyond just testing out baselines, but any code you write here should flexibly translate to more advanced deep learning training packages like PyTorch lightning and many others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333b99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Split dataset\n",
    "train_dataset, val_dataset, test_dataset = split_by_patient(\n",
    "    sample_dataset, [0.8, 0.1, 0.1]\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = get_dataloader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = get_dataloader(val_dataset, batch_size=256, shuffle=False)\n",
    "test_loader = get_dataloader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "# STEP 4: Initialize StageNet model\n",
    "model = StageNet(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=128,\n",
    "    chunk_size=128,\n",
    "    levels=3,\n",
    "    dropout=0.3,\n",
    ")\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel initialized with {num_params} parameters\")\n",
    "\n",
    "# STEP 5: Train the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    device=\"cuda:2\",  # or \"cpu\"\n",
    "    metrics=[\"pr_auc\", \"roc_auc\", \"accuracy\", \"f1\"],\n",
    ")\n",
    "\n",
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    epochs=50,\n",
    "    monitor=\"roc_auc\",\n",
    "    optimizer_params={\"lr\": 1e-5},\n",
    ")\n",
    "\n",
    "# STEP 6: Evaluate on test set\n",
    "results = trainer.evaluate(test_loader)\n",
    "print(\"\\nTest Results:\")\n",
    "for metric, value in results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "# STEP 7: Inspect model predictions\n",
    "sample_batch = next(iter(test_loader))\n",
    "with torch.no_grad():\n",
    "    output = model(**sample_batch)\n",
    "\n",
    "print(\"\\nSample predictions:\")\n",
    "print(f\"  Predicted probabilities: {output['y_prob'][:5]}\")\n",
    "print(f\"  True labels: {output['y_true'][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3d1b7f",
   "metadata": {},
   "source": [
    "## Post-hoc ML processing (TBD)\n",
    "We note that once the model's trained and evaluation metrics are derived. People may be interested in things like post-hoc interpretability or uncertainty quantification.\n",
    "\n",
    "We note that this is quite a work-in-progress for PyHealth 2.0, but the roadmap includes the following:\n",
    "\n",
    "- Layer-wise relevance propagation (deep NN-based interpretability)\n",
    "- Conformal Prediction: Although we do have many other UQ techniques [here](https://pyhealth.readthedocs.io/en/latest/api/calib.html)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
