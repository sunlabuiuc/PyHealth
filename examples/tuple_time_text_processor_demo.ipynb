{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# TupleTimeTextProcessor — Demo Notebook\n",
    "\n",
    "This notebook demonstrates `TupleTimeTextProcessor`, a PyHealth processor for clinical text paired\n",
    "with temporal information (time-since-admission). It supports two modes:\n",
    "\n",
    "| Mode | `tokenizer_model` | Output |\n",
    "|------|------------------|--------|\n",
    "| **Raw** | `None` (default) | `(List[str], time_tensor, type_tag)` |\n",
    "| **Tokenized** | e.g. `\"prajjwal1/bert-tiny\"` | `(input_ids, attention_mask, token_type_ids, time_tensor, type_tag)` |\n",
    "\n",
    "**Sections:**\n",
    "1. Environment setup\n",
    "2. Raw mode — temporal text handling\n",
    "3. Tokenized mode — HuggingFace integration\n",
    "4. Schema & API inspection\n",
    "5. Integration with `EmbeddingModel`\n",
    "6. End-to-end training step with `RNN`\n",
    "7. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on: {device}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"Transformers: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️  transformers not installed — tokenized mode will be skipped.\")\n",
    "    print(\"   Install with: pip install transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2",
   "metadata": {},
   "source": [
    "## 2. Raw Mode — Temporal Text Handling\n",
    "\n",
    "Without a tokenizer, the processor acts as a lightweight wrapper that:\n",
    "- Validates input structure\n",
    "- Converts time diffs to a float tensor\n",
    "- Attaches a `type_tag` for modality routing downstream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raw_mode",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.processors import TupleTimeTextProcessor\n",
    "\n",
    "# Default: no tokenizer\n",
    "processor = TupleTimeTextProcessor(type_tag=\"clinical_note\")\n",
    "\n",
    "# Simulate three notes from a single patient visit\n",
    "notes = [\n",
    "    \"Patient admitted with acute chest pain and dyspnea.\",\n",
    "    \"Day 1: Troponin elevated. Initiated anticoagulation.\",\n",
    "    \"Day 3: Symptoms resolved. Discharge planned.\",\n",
    "]\n",
    "time_diffs = [0.0, 24.0, 72.0]  # hours since first note\n",
    "\n",
    "texts_out, time_tensor, tag = processor.process((notes, time_diffs))\n",
    "\n",
    "print(f\"Output texts:       {texts_out}\")\n",
    "print(f\"Time tensor:        {time_tensor}\")\n",
    "print(f\"Time tensor shape:  {time_tensor.shape}\")\n",
    "print(f\"Type tag:           '{tag}'\")\n",
    "print(f\"Schema:             {processor.schema()}\")\n",
    "print(f\"is_token():         {processor.is_token()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s3",
   "metadata": {},
   "source": [
    "## 3. Tokenized Mode — HuggingFace Integration\n",
    "\n",
    "When `tokenizer_model` is provided, the processor runs `AutoTokenizer` internally\n",
    "and returns **pre-tokenized tensors** — no raw strings are serialized.\n",
    "This avoids pickling overhead in PyHealth's dataset pipeline.\n",
    "\n",
    "We use `prajjwal1/bert-tiny` here to keep downloads fast (4.4 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenized_mode",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_processor = TupleTimeTextProcessor(\n",
    "    tokenizer_model=\"prajjwal1/bert-tiny\",\n",
    "    max_length=32,\n",
    "    type_tag=\"clinical_note\",\n",
    ")\n",
    "\n",
    "result = tok_processor.process((notes, time_diffs))\n",
    "input_ids, attention_mask, token_type_ids, time_tensor, tag = result\n",
    "\n",
    "N, L = input_ids.shape\n",
    "print(f\"Notes (N):          {N}\")\n",
    "print(f\"Max length (L):     {L}\")\n",
    "print(f\"input_ids shape:    {input_ids.shape}   (N × L)\")\n",
    "print(f\"attention_mask:     {attention_mask.shape}\")\n",
    "print(f\"token_type_ids:     {token_type_ids.shape}\")\n",
    "print(f\"time_tensor:        {time_tensor}\")\n",
    "print(f\"type_tag:           '{tag}'\")\n",
    "print()\n",
    "print(\"Example input_ids (note 0):\")\n",
    "print(input_ids[0])\n",
    "print(\"Attention mask (note 0):\")\n",
    "print(attention_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s4",
   "metadata": {},
   "source": [
    "## 4. Schema & API Inspection\n",
    "\n",
    "The schema defines how downstream models interpret each element of the output tuple.\n",
    "`\"value\"` and `\"mask\"` are canonical keys understood by `BaseModel` and `EmbeddingModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema_api",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Raw processor ===\")\n",
    "print(f\"schema(): {processor.schema()}\")\n",
    "print(f\"dim():    {processor.dim()}\")\n",
    "print(f\"is_token(): {processor.is_token()}\")\n",
    "print(f\"repr:     {repr(processor)}\")\n",
    "\n",
    "print()\n",
    "print(\"=== Tokenized processor ===\")\n",
    "print(f\"schema(): {tok_processor.schema()}\")\n",
    "print(f\"dim():    {tok_processor.dim()}\")\n",
    "print(f\"is_token(): {tok_processor.is_token()}\")\n",
    "print(f\"repr:     {repr(tok_processor)}\")\n",
    "\n",
    "print()\n",
    "print(\"Schema key mapping (tokenized mode):\")\n",
    "schema = tok_processor.schema()\n",
    "for i, key in enumerate(schema):\n",
    "    print(f\"  result[{i}]  ->  '{key}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5",
   "metadata": {},
   "source": [
    "## 5. Integration with `EmbeddingModel`\n",
    "\n",
    "`EmbeddingModel` automatically detects `is_token() == True` and loads the corresponding\n",
    "HuggingFace `AutoModel` as the embedding backend.\n",
    "\n",
    "For a 3D input `(Batch, N_notes, L)` it:\n",
    "1. Flattens to `(B*N, L)`\n",
    "2. Runs the BERT encoder\n",
    "3. Pools via the `[CLS]` token → `(B*N, H)`\n",
    "4. Unflattens back to `(B, N, H)` — a sequence of *note embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import SampleDataset\n",
    "from pyhealth.models.embedding import EmbeddingModel\n",
    "\n",
    "# Build a tiny SampleDataset\n",
    "samples = [\n",
    "    {\n",
    "        \"patient_id\": f\"p{i}\",\n",
    "        \"visit_id\": f\"v{i}\",\n",
    "        \"notes\": (\n",
    "            [\"Admission note.\", \"Progress note.\", \"Discharge note.\"][:i+1],\n",
    "            [0.0, 24.0, 72.0][:i+1],\n",
    "        ),\n",
    "        \"label\": i % 2,\n",
    "    }\n",
    "    for i in range(3)\n",
    "]\n",
    "\n",
    "dataset = SampleDataset(\n",
    "    samples=samples,\n",
    "    input_schema={\"notes\": tok_processor},\n",
    "    output_schema={\"label\": \"binary\"},\n",
    ")\n",
    "\n",
    "embedding_dim = 64\n",
    "emb_model = EmbeddingModel(dataset, embedding_dim=embedding_dim)\n",
    "\n",
    "# Manually build a batch from two samples\n",
    "from pyhealth.datasets import get_dataloader\n",
    "loader = get_dataloader(dataset, batch_size=2, shuffle=False)\n",
    "batch = next(iter(loader))\n",
    "\n",
    "notes_feature = batch[\"notes\"]          # Tuple from the processor\n",
    "print(f\"Feature tuple length:  {len(notes_feature)}\")\n",
    "print(f\"input_ids batch shape: {notes_feature[0].shape}  (B, N, L)\")\n",
    "\n",
    "# Run through EmbeddingModel (masks propagated automatically)\n",
    "input_ids_batch = notes_feature[0]      # (B, N, L)\n",
    "attention_mask_batch = notes_feature[1] # (B, N, L)\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded = emb_model(\n",
    "        {\"notes\": input_ids_batch},\n",
    "        masks={\"notes\": attention_mask_batch},\n",
    "    )\n",
    "\n",
    "note_embeddings = embedded[\"notes\"]\n",
    "print(f\"\\nEmbedded shape: {note_embeddings.shape}  (B, N, embedding_dim)\")\n",
    "print(f\"Expected:       (2, N, {embedding_dim})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s6",
   "metadata": {},
   "source": [
    "## 6. End-to-End Training Step with `RNN`\n",
    "\n",
    "The `RNN` model treats each note embedding as a timestep in the sequence,\n",
    "and automatically extracts & propagates masks from the feature tuple.\n",
    "This is a full forward + backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rnn_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import RNN\n",
    "\n",
    "model = RNN(\n",
    "    dataset=dataset,\n",
    "    feature_keys=[\"notes\"],\n",
    "    label_key=\"label\",\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=32,\n",
    "    rnn_type=\"GRU\",\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Running one training step...\")\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "out = model(**{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()})\n",
    "\n",
    "loss = out[\"loss\"]\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(f\"Loss:    {loss.item():.4f}\")\n",
    "print(f\"y_prob:  {out['y_prob'].detach().cpu().numpy().round(3)}\")\n",
    "print(f\"y_true:  {out['y_true'].cpu().numpy()}\")\n",
    "print(\"\\n✅ Forward + backward pass complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s7",
   "metadata": {},
   "source": [
    "## 7. Choosing Your Tokenizer\n",
    "\n",
    "Any HuggingFace model can be used as the tokenizer/encoder backend.\n",
    "Here is a quick comparison of popular clinical NLP options:\n",
    "\n",
    "| Model | Size | Best for |\n",
    "|-------|------|----------|\n",
    "| `prajjwal1/bert-tiny` | 4.4 MB | Fast prototyping / unit tests |\n",
    "| `emilyalsentzer/Bio_ClinicalBERT` | 418 MB | General clinical notes |\n",
    "| `microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract` | 418 MB | Biomedical literature |\n",
    "| `allenai/longformer-base-4096` | 580 MB | Very long discharge summaries |\n",
    "\n",
    "Just swap the `tokenizer_model` string — everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenizer_choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: switch to Bio_ClinicalBERT (requires ~418 MB download on first run)\n",
    "# clinical_processor = TupleTimeTextProcessor(\n",
    "#     tokenizer_model=\"emilyalsentzer/Bio_ClinicalBERT\",\n",
    "#     max_length=512,\n",
    "#     type_tag=\"clinical_note\",\n",
    "# )\n",
    "\n",
    "# Preview the repr\n",
    "print(repr(tok_processor))\n",
    "print(f\"Vocab size: {tok_processor.tokenizer.vocab_size:,} tokens\")\n",
    "print(\"\\nAll APIs:\")\n",
    "print(f\"  .schema()   -> {tok_processor.schema()}\")\n",
    "print(f\"  .is_token() -> {tok_processor.is_token()}\")\n",
    "print(f\"  .dim()      -> {tok_processor.dim()}\")\n",
    "print(f\"  .size()     -> {tok_processor.size():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Capability | API |\n",
    "|------------|-----|\n",
    "| Raw temporal text | `TupleTimeTextProcessor(type_tag=\"note\")` |\n",
    "| Tokenized text | `TupleTimeTextProcessor(tokenizer_model=\"...\", max_length=128)` |\n",
    "| Schema inspection | `.schema()` → `('value', 'mask', 'token_type_ids', 'time', 'type_tag')` |\n",
    "| Modality detection | `.is_token()` → `True/False` |\n",
    "| Auto-encoder selection | `EmbeddingModel` reads `is_token()` and loads `AutoModel` |\n",
    "| 3D note handling | `EmbeddingModel` flattens `(B,N,L)` → encode → pool → `(B,N,H)` |\n",
    "| Downstream use | `RNN`, `MLP`, `Transformer`, `MultimodalRNN` all handle it natively |\n",
    "\n",
    "**Design philosophy**: preprocessing logic lives in the processor, not the model.\n",
    "This keeps models generic and enables swapping encoders without touching training code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
