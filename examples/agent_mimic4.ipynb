{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21eabedd",
   "metadata": {},
   "source": [
    "# Dr. Agent Model Tutorial with MIMIC-IV\n",
    "\n",
    "This notebook demonstrates how to use the Dr. Agent model for mortality prediction on MIMIC-IV data. Dr. Agent uses two reinforcement learning agents with dynamic skip connections to capture long-term dependencies in patient EHR sequences.\n",
    "\n",
    "**Paper:** Gao et al. \"Dr. Agent: Clinical predictive model via mimicked second opinions\" (JAMIA 2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f9a3d",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "\n",
    "Configure deterministic behaviour and import the libraries required for the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0de2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.tasks.mortality_prediction import MortalityPredictionMIMIC4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763b8dbf",
   "metadata": {},
   "source": [
    "# 2. Load MIMIC-IV Dataset\n",
    "Point to the preprocessed MIMIC-IV tables and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MIMIC4Dataset(\n",
    "    ehr_root=\"/path/to/mimic4\",  # Update this path\n",
    "    ehr_tables=[\n",
    "        \"patients\",\n",
    "        \"admissions\",\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"prescriptions\",\n",
    "    ],\n",
    "    dev=True,  # Set to False for full dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587cb3a",
   "metadata": {},
   "source": [
    "# 3. Prepare PyHealth Dataset\n",
    "Use the built-in `MortalityPredictionMIMIC4` task to convert patients into labeled visit samples and split them into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = MortalityPredictionMIMIC4()\n",
    "sample_dataset = dataset.set_task(task)\n",
    "\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "if len(sample_dataset) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"The task did not produce any samples. \"\n",
    "        \"Disable dev mode or adjust table selections.\"\n",
    "    )\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(\n",
    "    sample_dataset, [0.7, 0.1, 0.2], seed=SEED\n",
    ")\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4c231",
   "metadata": {},
   "source": [
    "# 4. Inspect Batch Structure\n",
    "\n",
    "Build PyHealth dataloaders and verify the keys and tensor shapes before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e10f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE) if len(val_ds) else None\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE) if len(test_ds) else None\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    raise RuntimeError(\n",
    "        \"The training loader is empty. \"\n",
    "        \"Increase the dataset size or adjust the task configuration.\"\n",
    "    )\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "\n",
    "def describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__}(shape={tuple(value.shape)})\"\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return f\"{type(value).__name__}(len={len(value)})\"\n",
    "    return type(value).__name__\n",
    "\n",
    "\n",
    "batch_summary = {key: describe(value) for key, value in first_batch.items()}\n",
    "print(\"Batch structure:\")\n",
    "for key, desc in batch_summary.items():\n",
    "    print(f\"  {key}: {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5045908d",
   "metadata": {},
   "source": [
    "# 5. Instantiate Dr. Agent Model\n",
    "\n",
    "Create the PyHealth Agent model with custom hyperparameters. The model uses:\n",
    "- **Primary agent**: Observes current visit to learn current health status\n",
    "- **Second-opinion agent**: Considers entire patient history for a global view\n",
    "- **Dynamic skip connections**: Selects optimal historical states via policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71526748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import Agent\n",
    "\n",
    "model = Agent(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=128,\n",
    "    # Agent-specific hyperparameters\n",
    "    n_actions=10,      # History window size (K in paper)\n",
    "    n_units=64,        # Agent MLP hidden units\n",
    "    dropout=0.5,       # Dropout rate\n",
    "    lamda=0.5,         # Skip connection weight\n",
    "    cell=\"gru\",        # RNN cell type: \"gru\" or \"lstm\"\n",
    "    use_baseline=True, # Use baseline for variance reduction\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Mode: {model.mode}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82eddb",
   "metadata": {},
   "source": [
    "# 6. Verify Forward Pass\n",
    "\n",
    "Run a single forward pass to ensure the model works correctly before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db30b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = {k: v.to(device) if hasattr(v, \"to\") else v for k, v in first_batch.items()}\n",
    "    output = model(**sample_batch)\n",
    "\n",
    "print(\"Forward pass output keys:\", list(output.keys()))\n",
    "print(f\"Loss: {output['loss'].item():.4f}\")\n",
    "print(f\"y_prob shape: {output['y_prob'].shape}\")\n",
    "print(f\"y_true shape: {output['y_true'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875a844e",
   "metadata": {},
   "source": [
    "# 7. Configure Trainer\n",
    "\n",
    "Wrap the model with the PyHealth `Trainer` to handle optimisation, gradient clipping, and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb37a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\", \"pr_auc\"],\n",
    "    device=str(device),\n",
    "    enable_logging=True,\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 20,\n",
    "    \"optimizer_params\": {\"lr\": 1e-3},\n",
    "    \"max_grad_norm\": 5.0,\n",
    "    \"monitor\": \"roc_auc\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e1027",
   "metadata": {},
   "source": [
    "# 8. Train the Model\n",
    "\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6652947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    **training_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a291e274",
   "metadata": {},
   "source": [
    "# 9. Evaluate on Test Split\n",
    "\n",
    "Switch to evaluation mode, collect predictions for validation and test splits, and compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for split_name, loader in [(\"validation\", val_loader), (\"test\", test_loader)]:\n",
    "    if loader is None:\n",
    "        print(f\"Skipping {split_name} (no data)\")\n",
    "        continue\n",
    "    \n",
    "    metrics = trainer.evaluate(loader)\n",
    "    evaluation_results[split_name] = metrics\n",
    "    \n",
    "    formatted = \", \".join(f\"{k}={v:.4f}\" for k, v in metrics.items())\n",
    "    print(f\"{split_name.title()} metrics: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85716e6",
   "metadata": {},
   "source": [
    "# 10. Inspect Sample Predictions\n",
    "\n",
    "Run inference to preview predicted probabilities alongside ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loader = test_loader if test_loader is not None else val_loader\n",
    "if target_loader is None:\n",
    "    target_loader = train_loader\n",
    "\n",
    "y_true, y_prob, mean_loss = trainer.inference(target_loader)\n",
    "\n",
    "# For binary classification, get positive class probability\n",
    "positive_prob = y_prob if y_prob.ndim == 1 else y_prob[..., -1]\n",
    "\n",
    "print(f\"Mean inference loss: {mean_loss:.4f}\")\n",
    "print(f\"\\nSample predictions (first 10):\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'True Label':<12} {'Pred Prob':<12}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for label, prob in zip(y_true[:10].tolist(), positive_prob[:10].tolist()):\n",
    "    print(f\"{int(label):<12} {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bcad2",
   "metadata": {},
   "source": [
    "# 11. Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c73f1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"agent_mimic4_mortality.pt\"\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"training_config\": training_config,\n",
    "    \"evaluation_results\": evaluation_results,\n",
    "}, checkpoint_path)\n",
    "\n",
    "print(f\"Model saved to: {checkpoint_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
