import polars as pl
import pandas as pd
import os
import json
import argparse
from pathlib import Path
import traceback

from pyhealth.datasets import MIMIC3Dataset


def prepare_mimic3_mortality_notes(
    mimic_root: str,
    output_file: str,
    config_path: str = None,
    dev_mode: bool = True
):
    """Loads MIMIC-III patients, admissions, and notes using PyHealth
    MIMIC3Dataset, extracts relevant information for in-hospital mortality
    prediction based on notes, performs text cleaning, and saves the
    structured data to a JSON Lines file.

    This function implements a workaround for the issue in some PyHealth
    versions where the standardized 'visit_id' column is not automatically
    generated by BaseDataset. It relies on 'hadm_id' being present in the
    'attributes' list for both 'admissions' and 'noteevents' tables within
    the mimic3.yaml config. It accesses these attributes
    (e.g., 'admissions/hadm_id'), joins on them, and creates the final
    'visit_id' column manually.

    Args:
        mimic_root (str): Path to the root directory of the MIMIC-III v1.4
            dataset (containing the .csv.gz files like ADMISSIONS.csv.gz, etc.).
        output_file (str): Path where output JSON Lines file (.jsonl) will be
            saved. Each line represents an admission with patient_id, visit_id,
            label, and a chronologically sorted sequence of notes.
        config_path (str, optional): Path to specific mimic3.yaml configuration
            file. If None, PyHealth's default config is used. Use this if you
            have modified a copy of the YAML file
            (e.g., to add 'hadm_id' to noteevents attributes). Defaults to None.
        dev_mode (bool, optional): If True, runs in development mode, limiting
            processing to a small subset of patients (typically 1000) as
            defined by `BaseDataset`. Defaults to True.

    Returns:
        None: The function writes the processed data to the specified
        `output_file`.
    """
    print("--- Initializing MIMIC3Dataset ---")
    try:
        # Request the tables needed, point to config if modified externally
        dataset = MIMIC3Dataset(
            root=mimic_root,
            tables=["patients", "admissions", "noteevents"],
            config_path=config_path,
            dev=dev_mode
        )
        print("--- MIMIC3Dataset Initialized Successfully ---")

    except Exception as e:
        print(f"Error initializing MIMIC3Dataset: {e}")
        print("Check if mimic_root is correct and if using a \
                custom config, ensure it's valid.")
        traceback.print_exc()
        return

    print("--- Accessing Global LazyFrame ---")
    all_events_lf = dataset.global_event_df

    print("--- Filtering for specific tables ---")
    try:
        # Filter admissions data. Select the prefixed 'admissions/hadm_id'
        # and rename it to simple 'hadm_id' for easier processing.
        admission_info_lf = all_events_lf.filter(
                pl.col("event_type") == "admissions"
            ).select(
            pl.col("patient_id").cast(pl.Int64),
            pl.col("admissions/hadm_id").cast(pl.Int64).alias("hadm_id"),
            pl.col("admissions/hospital_expire_flag").cast(pl.Int8).alias("label")
        ).unique(subset=["hadm_id"], keep="first")

        noteevents_lf_raw = all_events_lf.filter(
                pl.col("event_type") == "noteevents").select(
            pl.col("patient_id").cast(pl.Int64),
            pl.col("noteevents/hadm_id").cast(pl.Int64).alias("hadm_id"),
            pl.col("timestamp"),
            pl.col("noteevents/text").alias("TEXT")
        )

    except Exception as e:
        print(f"Error during filtering/selecting from global_event_df: {e}")
        print("Please ensure 'hadm_id' was added to 'noteevents' \
                attributes in your mimic3.yaml.")
        print("Available columns in global_event_df:", all_events_lf.columns)
        traceback.print_exc()
        return

    print("--- Preprocessing Notes Data (Text Cleaning) ---")
    # Text cleaning happens on the filtered noteevents frame
    notes_processed_lf = (
        noteevents_lf_raw
        .filter(pl.col("hadm_id").is_not_null())
        .filter(pl.col("timestamp").is_not_null())
        .select(
            # Keep the simple 'hadm_id' alias for joining
            pl.col("hadm_id"),
            pl.col("patient_id"),
            pl.col("timestamp"),
            pl.col("TEXT")
            .str.replace_all(r"\[\*\*.*?\*\*\]", "", literal=False)
            .str.replace_all(r"[^\w\s.,!?-]", " ", literal=False)
            .str.replace_all(r"\s+", " ", literal=False)
            .str.strip_chars()
            .str.to_lowercase()
            .alias("text_cleaned")
        )
        .filter(pl.col("text_cleaned").str.len_chars() > 0)
    )

    print("--- Joining Notes with Admission Labels ---")
    # Join notes with admission info using the simple 'hadm_id' alias
    final_lf = notes_processed_lf.join(
        admission_info_lf.select(["hadm_id", "label"]),
        on="hadm_id",
        how="inner"
    ).select(
        pl.col("patient_id"),
        pl.col("hadm_id").alias("visit_id"),
        pl.col("timestamp"),
        pl.col("text_cleaned"),
        pl.col("label")
    ).sort(["visit_id", "timestamp"])

    print("--- Collecting and Structuring Output ---")
    try:
        final_df_collected = final_lf.collect()
    except Exception as e:
        print(f"Error during final .collect(): {e}")
        print("Schema of final_lf just before collect:", final_lf.schema)
        traceback.print_exc()
        return

    # Structure into samples using Pandas
    final_df_pd = final_df_collected.to_pandas()
    samples = []
    grouped = final_df_pd.groupby("visit_id")
    total_visits = len(grouped)
    print(f"Found {total_visits} unique visits (admissions) with \
            notes after processing.")
    processed_count = 0

    for visit_id_pd, group_pd in grouped:
        label = int(group_pd["label"].iloc[0])
        patient_id = int(group_pd["patient_id"].iloc[0])

        notes_sequence = []
        for ts_obj, text_val in (
                zip(group_pd["timestamp"], group_pd["text_cleaned"])):
            iso_ts = ts_obj.isoformat() if pd.notna(ts_obj) else None
            notes_sequence.append((iso_ts, text_val))

        if not notes_sequence:
            continue

        samples.append({
            "patient_id": patient_id,
            "visit_id": int(visit_id_pd),
            "notes_sequence": notes_sequence,
            "label": label
        })
        processed_count += 1
        if processed_count % 5000 == 0:
            print(f"Processed {processed_count}/{total_visits} visits...")

    print(f"Finished processing. Generated {len(samples)} structured samples.")

    # --- Saving Logic ---
    output_dir = Path(output_file).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    try:
        with open(output_file, 'w') as f:
            for sample in samples:
                f.write(json.dumps(sample) + '\n')
        print(f"--- Successfully saved output file to {output_file}! ---")
    except IOError as e:
        print(f"Error writing output file: {e}")
        traceback.print_exc()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="""Preprocess MIMIC-III Notes for Mortality Prediction
        using PyHealth (Workaround for visit_id)."""
    )
    parser.add_argument(
        "--mimic_root", type=str, required=True, help="""Path to MIMIC-III
        v1.4 root directory containing .csv files."""
    )
    parser.add_argument(
        "--output_file", type=str, required=True, help="""Path for output
        JSON Lines file."""
    )
    parser.add_argument(
        "--config_path", type=str, default=None, help="""(Optional) Path
        to a custom mimic3.yaml config file.
        Required if you modified it to add 'hadm_id' to
        noteevents attributes."""
    )
    parser.add_argument(
        "--full_data", action="store_true", help="""Use full dataset
        instead of dev mode (first 1000 patients)."""
    )
    args = parser.parse_args()

    # Basic input validation
    mimic_admissions_path = Path(args.mimic_root) / "ADMISSIONS.csv"
    if not mimic_admissions_path.exists():
        print(f"Error: Cannot find 'ADMISSIONS.csv' \
              in the specified mimic_root: {args.mimic_root}")
    else:
        # Remind user about the config requirement for this script version
        if args.config_path is None:
            print("WARNING: No --config_path provided. Using default mimic3.yaml")
            print("If not, this script may fail finding 'noteevents/hadm_id'.")
        elif not Path(args.config_path).exists():
            print(f"ERROR: Provided --config_path '{args.config_path}' \
                    does not exist.")
            exit()

        prepare_mimic3_mortality_notes(
             args.mimic_root,
             args.output_file,
             dev_mode=not args.full_data
         )
