{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b5059",
   "metadata": {},
   "source": [
    "# TextEmbedding Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `TextEmbedding` module for encoding clinical text in PyHealth.\n",
    "\n",
    "**Overview:**\n",
    "- Initialize TextEmbedding with Bio_ClinicalBERT\n",
    "- Demonstrate 128-token chunking for long clinical notes\n",
    "- Show different pooling modes (none, cls, mean)\n",
    "- Verify mask output and backward compatibility\n",
    "- Use TupleTimeTextProcessor for temporal text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c778a6a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cc5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879729",
   "metadata": {},
   "source": [
    "## 2. Basic Usage\n",
    "\n",
    "Initialize `TextEmbedding` and encode sample clinical text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import TextEmbedding\n",
    "\n",
    "# Initialize with default Bio_ClinicalBERT\n",
    "encoder = TextEmbedding(\n",
    "    embedding_dim=128,\n",
    "    chunk_size=128,\n",
    "    pooling=\"none\",\n",
    ")\n",
    "\n",
    "print(f\"Model: {encoder.model_name}\")\n",
    "print(f\"Embedding dim: {encoder.embedding_dim}\")\n",
    "print(f\"Chunk size: {encoder.chunk_size}\")\n",
    "print(f\"Pooling: {encoder.pooling}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f49ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sample clinical notes\n",
    "texts = [\n",
    "    \"Patient presents with chest pain and shortness of breath.\",\n",
    "    \"Follow-up visit for diabetes management. Blood glucose stable.\"\n",
    "]\n",
    "\n",
    "embeddings, mask = encoder(texts)\n",
    "\n",
    "print(f\"Input: {len(texts)} texts\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}  # [batch, tokens, embedding_dim]\")\n",
    "print(f\"Mask shape: {mask.shape}  # [batch, tokens]\")\n",
    "print(f\"Mask dtype: {mask.dtype}\")\n",
    "print(f\"Valid tokens per sample: {mask.sum(dim=1).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed75fa",
   "metadata": {},
   "source": [
    "## 3. Chunking Behavior\n",
    "\n",
    "Long texts are split into non-overlapping chunks of `chunk_size - 2` tokens (2 reserved for [CLS]/[SEP])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a long clinical note by repeating text\n",
    "long_note = \"Patient admitted with acute myocardial infarction. \" * 50\n",
    "print(f\"Long note word count: {len(long_note.split())}\")\n",
    "\n",
    "embeddings, mask = encoder([long_note])\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Total tokens encoded: {mask.sum().item()}\")\n",
    "print(f\"\\nNote: Multiple chunks were created and concatenated along the sequence dimension.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591559c8",
   "metadata": {},
   "source": [
    "## 4. Pooling Modes\n",
    "\n",
    "Three pooling modes are available:\n",
    "- `\"none\"`: All token embeddings [B, T, E']\n",
    "- `\"cls\"`: [CLS] token per chunk [B, C, E']\n",
    "- `\"mean\"`: Mean-pooled per chunk [B, C, E']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47ebd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare pooling modes on the same long text\n",
    "long_text = \"Patient presents with symptoms. \" * 100\n",
    "\n",
    "for pooling in [\"none\", \"cls\", \"mean\"]:\n",
    "    enc = TextEmbedding(embedding_dim=128, pooling=pooling)\n",
    "    emb, mask = enc([long_text])\n",
    "    print(f\"pooling='{pooling}': shape={tuple(emb.shape)}, valid_positions={mask.sum().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9f226",
   "metadata": {},
   "source": [
    "## 5. Performance Guardrails\n",
    "\n",
    "The `max_chunks` parameter prevents memory issues with very long texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d35bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very long text that exceeds max_chunks\n",
    "very_long_text = \"Clinical observation. \" * 2000  # Approximately 10,000+ tokens\n",
    "\n",
    "encoder_limited = TextEmbedding(\n",
    "    embedding_dim=128,\n",
    "    pooling=\"cls\",  # Use CLS for cleaner output\n",
    "    max_chunks=10,  # Limit to 10 chunks\n",
    ")\n",
    "\n",
    "# This will trigger a UserWarning about truncation\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    emb, mask = encoder_limited([very_long_text])\n",
    "    if w:\n",
    "        print(f\"Warning: {w[0].message}\")\n",
    "\n",
    "print(f\"\\nOutput shape: {emb.shape}\")\n",
    "print(f\"Chunks retained: {mask.sum().item()} (capped at max_chunks=10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce223e3",
   "metadata": {},
   "source": [
    "## 6. Backward Compatibility\n",
    "\n",
    "The `return_mask` parameter allows single-tensor return for compatibility with older code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344eac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New API (default): returns tuple\n",
    "encoder_new = TextEmbedding(return_mask=True)\n",
    "result = encoder_new([\"Test text\"])\n",
    "print(f\"return_mask=True: type={type(result)}, len={len(result)}\")\n",
    "\n",
    "# Backward compatible: returns tensor only\n",
    "encoder_compat = TextEmbedding(return_mask=False)\n",
    "result = encoder_compat([\"Test text\"])\n",
    "print(f\"return_mask=False: type={type(result)}, shape={result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f2d33",
   "metadata": {},
   "source": [
    "## 7. Mask Convention\n",
    "\n",
    "The mask uses `True` for valid positions and `False` for padding, compatible with PyHealth's TransformerLayer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c7e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different length texts to show padding behavior\n",
    "texts = [\n",
    "    \"Short note.\",\n",
    "    \"This is a longer clinical note with more content to encode into embeddings.\"\n",
    "]\n",
    "\n",
    "encoder = TextEmbedding(embedding_dim=128)\n",
    "embeddings, mask = encoder(texts)\n",
    "\n",
    "print(f\"Batch shape: {embeddings.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"\")\n",
    "print(f\"Sample 1 valid tokens: {mask[0].sum().item()}\")\n",
    "print(f\"Sample 2 valid tokens: {mask[1].sum().item()}\")\n",
    "print(f\"\")\n",
    "print(\"Mask convention: True=valid, False=padding\")\n",
    "print(f\"Sample 1 mask: {mask[0].tolist()[:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad6f0f",
   "metadata": {},
   "source": [
    "## 8. Freezing Pretrained Weights\n",
    "\n",
    "For multimodal fusion, freeze the encoder to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frozen encoder (recommended for multimodal fusion)\n",
    "encoder_frozen = TextEmbedding(embedding_dim=128, freeze=True)\n",
    "\n",
    "transformer_params = sum(p.numel() for p in encoder_frozen.transformer.parameters())\n",
    "trainable_params = sum(p.numel() for p in encoder_frozen.transformer.parameters() if p.requires_grad)\n",
    "projection_params = sum(p.numel() for p in encoder_frozen.fc.parameters())\n",
    "\n",
    "print(f\"Transformer parameters: {transformer_params:,}\")\n",
    "print(f\"Trainable transformer params: {trainable_params:,}\")\n",
    "print(f\"Projection layer params: {projection_params:,} (always trainable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval_mode",
   "metadata": {},
   "source": [
    "## 9. Eval Mode Determinism\n",
    "\n",
    "In eval mode, dropout is disabled and outputs are deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determinism_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TextEmbedding(embedding_dim=128)\n",
    "encoder.eval()\n",
    "\n",
    "text = [\"Patient stable.\"]\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb1, _ = encoder(text)\n",
    "    emb2, _ = encoder(text)\n",
    "\n",
    "is_equal = torch.allclose(emb1, emb2)\n",
    "print(f\"Outputs identical in eval mode: {is_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuple_time_processor",
   "metadata": {},
   "source": [
    "## 10. TupleTimeTextProcessor for Temporal Data\n",
    "\n",
    "The `TupleTimeTextProcessor` handles clinical text paired with temporal information (time differences). This is useful for multimodal models that need to automatically route different modality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuple_time_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.processors import TupleTimeTextProcessor\n",
    "\n",
    "# Initialize processor with type tag for modality routing\n",
    "processor = TupleTimeTextProcessor(type_tag=\"clinical_note\")\n",
    "\n",
    "# Clinical notes with time differences (e.g., hours since admission)\n",
    "texts = [\n",
    "    \"Patient admitted with chest pain.\",\n",
    "    \"Follow-up: symptoms improved.\",\n",
    "    \"Discharge: stable condition.\"\n",
    "]\n",
    "time_diffs = [0.0, 24.0, 72.0]  # hours\n",
    "\n",
    "# Process tuple\n",
    "processed_texts, time_tensor, modality_tag = processor.process((texts, time_diffs))\n",
    "\n",
    "print(f\"Texts: {processed_texts}\")\n",
    "print(f\"Time tensor: {time_tensor}\")\n",
    "print(f\"Time tensor shape: {time_tensor.shape}\")\n",
    "print(f\"Modality tag: '{modality_tag}'\")\n",
    "print(f\"\\nProcessor repr: {repr(processor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tuple_time_multimodal",
   "metadata": {},
   "source": [
    "### Multimodal Fusion Example\n",
    "\n",
    "The `type_tag` enables automatic routing in multimodal pipelines without hardcoding feature names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tuple_time_fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different modality types with different processors\n",
    "note_processor = TupleTimeTextProcessor(type_tag=\"note\")\n",
    "ehr_processor = TupleTimeTextProcessor(type_tag=\"ehr\")\n",
    "observation_processor = TupleTimeTextProcessor(type_tag=\"observation\")\n",
    "\n",
    "# Process different data types\n",
    "notes = [\"Admission note\", \"Progress note\"]\n",
    "note_times = [0.0, 24.0]\n",
    "_, note_tensor, note_tag = note_processor.process((notes, note_times))\n",
    "\n",
    "ehr_events = [\"Lab ordered\", \"Medication given\"]\n",
    "ehr_times = [2.0, 6.0]\n",
    "_, ehr_tensor, ehr_tag = ehr_processor.process((ehr_events, ehr_times))\n",
    "\n",
    "# Tags can be used for automatic routing in models\n",
    "print(f\"Note modality tag: '{note_tag}'\")\n",
    "print(f\"EHR modality tag: '{ehr_tag}'\")\n",
    "print(f\"\\nNote times: {note_tensor}\")\n",
    "print(f\"EHR times: {ehr_tensor}\")\n",
    "\n",
    "# Can combine tensors for temporal modeling\n",
    "combined_times = torch.cat([note_tensor, ehr_tensor])\n",
    "print(f\"\\nCombined times: {combined_times}\")\n",
    "print(f\"Combined shape: {combined_times.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The `TextEmbedding` module and `TupleTimeTextProcessor` provide:\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Chunking** | Splits long texts into 128-token chunks |\n",
    "| **Pooling** | none/cls/mean modes for different use cases |\n",
    "| **Mask** | Boolean tensor compatible with TransformerLayer |\n",
    "| **Guardrails** | max_chunks prevents OOM on long texts |\n",
    "| **Compatibility** | return_mask=False for legacy code |\n",
    "| **Temporal Processing** | TupleTimeTextProcessor for time-aware text data |\n",
    "| **Modality Routing** | Type tags enable automatic multimodal fusion |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
