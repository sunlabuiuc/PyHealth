{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HALO Synthetic Data Generation for MIMIC-III\n\nThis notebook trains the HALO (Hierarchical Autoregressive Language mOdel) on your MIMIC-III data and generates synthetic patients.\n\n## What You'll Need\n\n1. **MIMIC-III Access**: Download these files from PhysioNet:\n   - `ADMISSIONS.csv`\n   - `DIAGNOSES_ICD.csv`\n\n2. **Google Colab**: Free tier works, but GPU recommended (Runtime \u2192 Change runtime type \u2192 GPU)\n\n3. **Time**:\n   - Demo (5 epochs, 1K samples): ~20-30 min on GPU\n   - Production (80 epochs, 10K samples): ~6-10 hours on GPU\n\n## How It Works\n\n1. **Setup**: Install PyHealth and mount Google Drive\n2. **Upload Data**: Upload your MIMIC-III CSV files\n3. **Configure**: Set hyperparameters (epochs, batch size, etc.)\n4. **Train**: Train HALO model (checkpoints saved to Drive)\n5. **Generate**: Create synthetic patients using trained model\n6. **Download**: Get CSV file with synthetic data\n\n## Important Notes\n\n\u26a0\ufe0f **Colab Timeout**: Free Colab sessions timeout after 12 hours. For production training (80 epochs), consider:\n- Colab Pro for longer sessions\n- Running on your own GPU cluster using `examples/slurm/train_halo_mimic3.slurm`\n\n\ud83d\udcca **Demo vs Production**:\n- Demo defaults (5 epochs, 1K samples) let you test the pipeline quickly\n- Production settings (80 epochs, 10K samples) match the published HALO results\n\n## References\n\n- [HALO Paper](https://arxiv.org/abs/2406.16061)\n- [PyHealth Documentation](https://pyhealth.readthedocs.io/)\n- [MIMIC-III Access](https://physionet.org/content/mimiciii/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Install PyHealth from GitHub (gets latest HALO implementation)\n",
    "# For development/CI: set BRANCH to specific branch name (e.g., 'halo-pr-528')\n",
    "# For production: leave BRANCH as None to use main branch\n",
    "FORK = 'jalengg'\n",
    "BRANCH = 'halo-pr-528'  # Change to 'halo-pr-528' or other branch name for development\n",
    "\n",
    "if BRANCH:\n",
    "    install_url = f\"git+https://github.com/{FORK}/PyHealth.git@{BRANCH}\"\n",
    "    print(f\"Installing PyHealth from branch '{BRANCH}'...\")\n",
    "else:\n",
    "    install_url = f\"git+https://github.com/{FORK}/PyHealth.git\"\n",
    "    print(\"Installing PyHealth from main branch...\")\n",
    "\n",
    "# Install with error checking (--no-cache-dir ensures latest version)\n",
    "result = subprocess.run(\n",
    "    [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-cache-dir\", install_url],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"\u2713 PyHealth installed successfully! [{datetime.now().strftime('%Y%m%d%H%M%S')}]\")\n",
    "else:\n",
    "    print(\"\u274c PyHealth installation failed!\")\n",
    "    print(f\"\nError output:\")\n",
    "    print(result.stderr)\n",
    "    raise RuntimeError(\"PyHealth installation failed. Please check the error above.\")\n"
   ],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Import required libraries\nimport os\nimport sys\nimport torch\nimport pickle\nimport pandas as pd\nimport shutil\nfrom google.colab import drive, files\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown, HTML\n\nprint(\"\u2713 All libraries imported successfully!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive for persistent storage\nprint(\"Mounting Google Drive...\")\ndrive.mount('/content/drive')\nprint(\"\u2713 Google Drive mounted at /content/drive\")\n\n# Create directory structure in Drive\nbase_dir = '/content/drive/MyDrive/HALO_Training'\ndata_dir = f'{base_dir}/data'\ncheckpoint_dir = f'{base_dir}/checkpoints'\npkl_data_dir = f'{base_dir}/pkl_data'\noutput_dir = f'{base_dir}/output'\n\nfor dir_path in [base_dir, data_dir, checkpoint_dir, pkl_data_dir, output_dir]:\n    os.makedirs(dir_path, exist_ok=True)\n\nprint(f\"\\n\u2713 Directory structure created:\")\nprint(f\"  Base: {base_dir}\")\nprint(f\"  Data: {data_dir}\")\nprint(f\"  Checkpoints: {checkpoint_dir}\")\nprint(f\"  Vocabulary: {pkl_data_dir}\")\nprint(f\"  Output: {output_dir}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your training and generation parameters below.\n",
    "\n",
    "**For Quick Demo (recommended first time):**\n",
    "- Leave defaults (5 epochs, 1K samples)\n",
    "- Runs in ~20-30 minutes on GPU\n",
    "\n",
    "**For Production Quality:**\n",
    "- Set `EPOCHS = 80`\n",
    "- Set `N_SYNTHETIC_SAMPLES = 10000`\n",
    "- Expect ~6-10 hours on GPU"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# ============================================\n# CONFIGURATION - Modify these parameters\n# ============================================\n\n# Training configuration\nEPOCHS = 5                  # Demo: 5, Production: 80\nBATCH_SIZE = 32            # Demo: 32, Production: 48\nLEARNING_RATE = 0.0001     # Standard HALO learning rate\n\n# Generation configuration\nN_SYNTHETIC_SAMPLES = 1000  # Demo: 1000, Production: 10000\n\n# Advanced HALO model configuration (usually don't need to change)\nN_POSITIONS = 56           # Maximum sequence length\nN_CTX = 48                 # Context window\nN_EMBD = 768              # Embedding dimension\nN_LAYER = 12              # Number of transformer layers\nN_HEAD = 12               # Number of attention heads\n\n# Display configuration\nprint(\"=\" * 60)\nprint(\"HALO CONFIGURATION\")\nprint(\"=\" * 60)\nprint(f\"Training:\")\nprint(f\"  Epochs: {EPOCHS}\")\nprint(f\"  Batch size: {BATCH_SIZE}\")\nprint(f\"  Learning rate: {LEARNING_RATE}\")\nprint(f\"\\nGeneration:\")\nprint(f\"  Synthetic samples: {N_SYNTHETIC_SAMPLES}\")\nprint(f\"\\nModel architecture:\")\nprint(f\"  Embedding dim: {N_EMBD}\")\nprint(f\"  Layers: {N_LAYER}\")\nprint(f\"  Attention heads: {N_HEAD}\")\nprint(\"=\" * 60)\n\n# Estimate runtime\nif torch.cuda.is_available():\n    runtime_min = EPOCHS * 4  # Rough estimate: 4 min per epoch on GPU\n    print(f\"\\nEstimated training time: ~{runtime_min}-{runtime_min*2} minutes on GPU\")\nelse:\n    print(f\"\\n\u26a0\ufe0f  WARNING: No GPU detected! Training will be very slow.\")\n    print(f\"   Go to Runtime \u2192 Change runtime type \u2192 Select GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Data Upload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Upload your MIMIC-III CSV files. You need these 2 files:\n\n1. `ADMISSIONS.csv` - Patient admission records\n2. `DIAGNOSES_ICD.csv` - Diagnosis codes (ICD-9)\n\n**Note**: Files will be saved to Google Drive and persist across Colab sessions. If files already exist in Drive, you can skip uploading and reuse them."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import os\n\n# Required MIMIC-III files (only 2 needed for HALO)\nrequired_files = {\n    'ADMISSIONS.csv': 'Patient admission records',\n    'DIAGNOSES_ICD.csv': 'Diagnosis codes (ICD-9)'\n}\n\n# Check if files already exist in Google Drive\nexisting_files = {f: os.path.exists(f'{data_dir}/{f}') for f in required_files}\nmissing_files = [f for f, exists in existing_files.items() if not exists]\n\nif not missing_files:\n    print(\"\u2713 All required files already exist in Google Drive!\")\n    for filename in required_files:\n        file_path = f'{data_dir}/{filename}'\n        file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n        print(f\"  \u2713 {filename} ({file_size_mb:.1f} MB)\")\n    print(f\"\\nSkipping upload. To re-upload, delete files from: {data_dir}\")\nelse:\n    print(\"Upload the following MIMIC-III files:\")\n    for i, filename in enumerate(missing_files, 1):\n        print(f\"  {i}. {filename} - {required_files[filename]}\")\n    print(\"\\nYou can upload files one at a time or all together.\\n\")\n    \n    uploaded = files.upload()\n    \n    # Normalize filenames - handle Colab's automatic renaming (e.g., \"ADMISSIONS (1).csv\")\n    print(\"\\nProcessing uploaded files...\")\n    for uploaded_name in uploaded.keys():\n        # Find which required file this matches\n        matched_file = None\n        for req_file in required_files:\n            # Extract base name without extension\n            req_base = req_file.replace('.csv', '')\n            # Check if uploaded name contains the required base name\n            if req_base in uploaded_name and uploaded_name.endswith('.csv'):\n                matched_file = req_file\n                break\n        \n        if matched_file:\n            src = f'/content/{uploaded_name}'\n            dst = f'{data_dir}/{matched_file}'\n            shutil.copy(src, dst)\n            file_size_mb = os.path.getsize(dst) / (1024 * 1024)\n            print(f\"  \u2713 {matched_file} ({file_size_mb:.1f} MB)\")\n            if matched_file in missing_files:\n                missing_files.remove(matched_file)\n    \n    # Check if we still have missing files\n    if missing_files:\n        print(f\"\\n\u26a0\ufe0f  Still missing: {missing_files}\")\n        print(\"Please run this cell again to upload the remaining files.\")\n    else:\n        print(\"\\n\u2713 All files uploaded successfully!\")\n        print(f\"   Location: {data_dir}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Validate uploaded files\nprint(\"Validating data files...\")\n\n# Check ADMISSIONS.csv\nadmissions = pd.read_csv(f'{data_dir}/ADMISSIONS.csv', nrows=5)\nprint(f\"\\n\u2713 ADMISSIONS.csv: {len(admissions.columns)} columns\")\nprint(f\"   Sample columns: {', '.join(admissions.columns[:3])}\")\n\n# Check DIAGNOSES_ICD.csv\ndiagnoses = pd.read_csv(f'{data_dir}/DIAGNOSES_ICD.csv', nrows=5)\nprint(f\"\\n\u2713 DIAGNOSES_ICD.csv: {len(diagnoses.columns)} columns\")\nprint(f\"   Sample columns: {', '.join(diagnoses.columns[:3])}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\u2713 DATA VALIDATION COMPLETE\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the HALO model on your MIMIC-III data.\n",
    "\n",
    "**What happens during training:**\n",
    "- Dataset is preprocessed and vocabularies are created\n",
    "- HALO model trains for the specified number of epochs\n",
    "- Best checkpoint is automatically saved when validation loss improves\n",
    "- Progress updates print every 1,000 iterations\n",
    "\n",
    "**Checkpoints saved to Drive:**\n",
    "- `halo_model` - Best model (lowest validation loss)\n",
    "- `halo_model_final` - Final model after all epochs\n",
    "- Vocabulary files in `pkl_data/`\n",
    "\n",
    "**Resume capability**: If training is interrupted, checkpoints in Drive allow resuming."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyhealth.datasets.halo_mimic3 import HALO_MIMIC3Dataset\nfrom pyhealth.models.generators.halo import HALO\nfrom pyhealth.models.generators.halo_resources.halo_config import HALOConfig\n\n# Check for existing checkpoint\nexisting_checkpoint = os.path.exists(f'{checkpoint_dir}/halo_model')\nif existing_checkpoint:\n    print(\"=\" * 60)\n    print(\"\u26a0\ufe0f  EXISTING CHECKPOINT FOUND\")\n    print(\"=\" * 60)\n    print(f\"Found checkpoint at: {checkpoint_dir}/halo_model\")\n    print(\"\\nOptions:\")\n    print(\"  1. Skip training and go to generation (recommended if training completed)\")\n    print(\"  2. Continue running this cell to retrain (will overwrite checkpoint)\")\n    print(\"\\nTo skip training, jump to the 'Generation' section below.\")\n    print(\"=\" * 60)\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"\\nUsing device: {device}\", flush=True)\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\", flush=True)\n    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\", flush=True)\nelse:\n    print(\"\u26a0\ufe0f  WARNING: Training on CPU will be very slow!\", flush=True)\n\n# Load and preprocess dataset\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"Loading and preprocessing MIMIC-III dataset...\", flush=True)\nprint(f\"{'='*60}\", flush=True)\nprint(f\"Data directory: {data_dir}\", flush=True)\nprint(f\"Output directory: {checkpoint_dir}\", flush=True)\n\ndataset = HALO_MIMIC3Dataset(\n    mimic3_dir=data_dir,\n    pkl_data_dir=pkl_data_dir,\n    gzip=False\n)\n\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"Dataset preprocessing complete!\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\n# Load vocabulary sizes\ncode_to_index = pickle.load(open(f\"{pkl_data_dir}/codeToIndex.pkl\", \"rb\"))\nid_to_label = pickle.load(open(f\"{pkl_data_dir}/idToLabel.pkl\", \"rb\"))\n\ncode_vocab_size = len(code_to_index)\nlabel_vocab_size = len(id_to_label)\nspecial_vocab_size = 3\ntotal_vocab_size = code_vocab_size + label_vocab_size + special_vocab_size\n\nprint(f\"Vocabulary sizes:\", flush=True)\nprint(f\"  Code vocabulary: {code_vocab_size}\", flush=True)\nprint(f\"  Label vocabulary: {label_vocab_size}\", flush=True)\nprint(f\"  Special tokens: {special_vocab_size}\", flush=True)\nprint(f\"  Total vocabulary: {total_vocab_size}\", flush=True)\n\n# HALO configuration\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"Initializing HALO configuration\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\nconfig = HALOConfig(\n    total_vocab_size=total_vocab_size,\n    code_vocab_size=code_vocab_size,\n    label_vocab_size=label_vocab_size,\n    special_vocab_size=special_vocab_size,\n    n_positions=N_POSITIONS,\n    n_ctx=N_CTX,\n    n_embd=N_EMBD,\n    n_layer=N_LAYER,\n    n_head=N_HEAD,\n    layer_norm_epsilon=1e-5,\n    initializer_range=0.02,\n    batch_size=BATCH_SIZE,\n    sample_batch_size=256,\n    epoch=EPOCHS,\n    pos_loss_weight=None,\n    lr=LEARNING_RATE\n)\n\nprint(\"Configuration:\", flush=True)\nprint(f\"  Embedding dim: {config.n_embd}\", flush=True)\nprint(f\"  Layers: {config.n_layer}\", flush=True)\nprint(f\"  Attention heads: {config.n_head}\", flush=True)\nprint(f\"  Batch size: {config.batch_size}\", flush=True)\nprint(f\"  Epochs: {config.epoch}\", flush=True)\nprint(f\"  Learning rate: {config.lr}\", flush=True)\n\n# Train HALO model\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"Training HALO model...\", flush=True)\nprint(f\"{'='*60}\", flush=True)\nprint(f\"Training for {EPOCHS} epochs\", flush=True)\nprint(f\"Progress updates every 1,000 iterations\", flush=True)\nprint(f\"Checkpoints saved when validation loss improves\", flush=True)\nprint(f\"{'='*60}\\n\", flush=True)\n\nmodel = HALO(\n    dataset=dataset,\n    config=config,\n    save_dir=checkpoint_dir,\n    train_on_init=True\n)\n\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"\u2713 TRAINING COMPLETE!\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\n# Save final checkpoint\nfinal_state = {\n    'model': model.model.state_dict(),\n    'optimizer': model.optimizer.state_dict(),\n    'iteration': 'final',\n    'epoch': config.epoch\n}\ntorch.save(final_state, os.path.join(checkpoint_dir, 'halo_model_final'))\nprint(f\"\u2713 Final checkpoint saved to: {checkpoint_dir}/halo_model_final\", flush=True)\n\n# Copy best checkpoint\nbest_path = os.path.join(checkpoint_dir, 'halo_model')\nif os.path.exists(best_path):\n    shutil.copy(best_path, os.path.join(checkpoint_dir, 'halo_model_best'))\n    print(f\"\u2713 Best checkpoint copied to: {checkpoint_dir}/halo_model_best\", flush=True)\n\nprint(f\"\\n\u2713 Training artifacts saved:\", flush=True)\nprint(f\"  - Checkpoints: {checkpoint_dir}\", flush=True)\nprint(f\"  - Vocabulary: {pkl_data_dir}\", flush=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate synthetic patients using the trained HALO model.\n",
    "\n",
    "**What happens during generation:**\n",
    "- Loads the best checkpoint from training\n",
    "- Generates the specified number of synthetic patients\n",
    "- Each patient has multiple visits with ICD-9 diagnosis codes\n",
    "- Outputs CSV file with columns: SUBJECT_ID, VISIT_NUM, ICD9_CODE\n",
    "\n",
    "**Generation time**:\n",
    "- 1,000 patients: ~10-15 minutes\n",
    "- 10,000 patients: ~1-2 hours"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pyhealth.models.generators.halo_resources.halo_model import HALOModel\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\", flush=True)\n\n# Load vocabulary and configuration\nprint(f\"\\n{'='*60}\", flush=True)\nprint(f\"Loading vocabulary from {pkl_data_dir}\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\ncode_to_index = pickle.load(open(f\"{pkl_data_dir}/codeToIndex.pkl\", \"rb\"))\nindex_to_code = pickle.load(open(f\"{pkl_data_dir}/indexToCode.pkl\", \"rb\"))\nid_to_label = pickle.load(open(f\"{pkl_data_dir}/idToLabel.pkl\", \"rb\"))\ntrain_dataset = pickle.load(open(f\"{pkl_data_dir}/trainDataset.pkl\", \"rb\"))\n\ncode_vocab_size = len(code_to_index)\nlabel_vocab_size = len(id_to_label)\nspecial_vocab_size = 3\ntotal_vocab_size = code_vocab_size + label_vocab_size + special_vocab_size\n\nprint(f\"Vocabulary sizes:\", flush=True)\nprint(f\"  Code vocabulary: {code_vocab_size}\", flush=True)\nprint(f\"  Label vocabulary: {label_vocab_size}\", flush=True)\nprint(f\"  Total vocabulary: {total_vocab_size}\", flush=True)\n\n# Create config (same as training)\nconfig = HALOConfig(\n    total_vocab_size=total_vocab_size,\n    code_vocab_size=code_vocab_size,\n    label_vocab_size=label_vocab_size,\n    special_vocab_size=special_vocab_size,\n    n_positions=N_POSITIONS,\n    n_ctx=N_CTX,\n    n_embd=N_EMBD,\n    n_layer=N_LAYER,\n    n_head=N_HEAD,\n    layer_norm_epsilon=1e-5,\n    initializer_range=0.02,\n    batch_size=BATCH_SIZE,\n    sample_batch_size=256,\n    epoch=EPOCHS,\n    pos_loss_weight=None,\n    lr=LEARNING_RATE\n)\n\n# Create minimal dataset object for interface compatibility\nclass MinimalDataset:\n    def __init__(self, pkl_data_dir):\n        self.pkl_data_dir = pkl_data_dir\n\ndataset = MinimalDataset(pkl_data_dir)\n\n# Load trained model\ncheckpoint_path = f'{checkpoint_dir}/halo_model'\nif not os.path.exists(checkpoint_path):\n    checkpoint_path = f'{checkpoint_dir}/halo_model_best'\nif not os.path.exists(checkpoint_path):\n    checkpoint_path = f'{checkpoint_dir}/halo_model_final'\n\nif not os.path.exists(checkpoint_path):\n    raise FileNotFoundError(\n        f\"No checkpoint found! Expected one of:\\n\"\n        f\"  {checkpoint_dir}/halo_model\\n\"\n        f\"  {checkpoint_dir}/halo_model_best\\n\"\n        f\"  {checkpoint_dir}/halo_model_final\\n\"\n        f\"Please run the Training section first.\"\n    )\n\nprint(f\"\\n{'='*60}\", flush=True)\nprint(f\"Loading checkpoint from {checkpoint_path}\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\nhalo_model = HALOModel(config).to(device)\ncheckpoint = torch.load(checkpoint_path, map_location=device)\nhalo_model.load_state_dict(checkpoint['model'])\nhalo_model.eval()\n\nprint(\"\u2713 Model loaded successfully\", flush=True)\n\n# Generate synthetic patients\nprint(f\"\\n{'='*60}\", flush=True)\nprint(f\"Generating {N_SYNTHETIC_SAMPLES} synthetic patients...\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\n# Estimate time\nif torch.cuda.is_available():\n    est_minutes = max(10, N_SYNTHETIC_SAMPLES // 100)\n    print(f\"Estimated time: ~{est_minutes} minutes on GPU\", flush=True)\nprint(f\"\", flush=True)\n\n# Create HALO instance for generation\nhalo = HALO(\n    dataset=dataset,\n    config=config,\n    save_dir=checkpoint_dir,\n    train_on_init=False\n)\nhalo.model = halo_model\nhalo.train_ehr_dataset = train_dataset[:N_SYNTHETIC_SAMPLES]\nhalo.index_to_code = index_to_code\n\n# Generate synthetic data (creates pickle file)\nhalo.synthesize_dataset(pkl_save_dir=output_dir + \"/\")\n\n# Load generated pickle\ngenerated_pkl = os.path.join(output_dir, \"haloDataset.pkl\")\nsynthetic_data = pickle.load(open(generated_pkl, \"rb\"))\n\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"\u2713 GENERATION COMPLETE!\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\n# Print statistics\nprint(f\"\\nSynthetic data statistics:\", flush=True)\nprint(f\"  Total patients: {len(synthetic_data)}\", flush=True)\n\ntotal_visits = sum(len(p['visits']) for p in synthetic_data)\navg_visits = total_visits / len(synthetic_data)\nprint(f\"  Total visits: {total_visits}\", flush=True)\nprint(f\"  Avg visits per patient: {avg_visits:.2f}\", flush=True)\n\ntotal_codes = sum(len(code) for p in synthetic_data for visit in p['visits'] for code in [visit])\navg_codes = total_codes / total_visits if total_visits > 0 else 0\nprint(f\"  Avg codes per visit: {avg_codes:.2f}\", flush=True)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Convert to CSV format\nprint(f\"\\n{'='*60}\", flush=True)\nprint(\"Converting to CSV format...\", flush=True)\nprint(f\"{'='*60}\", flush=True)\n\ncsv_path = f'{output_dir}/halo_synthetic_data.csv'\n\nrecords = []\nfor patient_idx, patient in enumerate(synthetic_data):\n    patient_id = f\"SYNTHETIC_{patient_idx+1:06d}\"\n    for visit_num, visit in enumerate(patient['visits'], 1):\n        for code_idx in visit:\n            icd9_code = index_to_code[code_idx]\n            records.append({\n                'SUBJECT_ID': patient_id,\n                'VISIT_NUM': visit_num,\n                'ICD9_CODE': icd9_code\n            })\n\ndf = pd.DataFrame(records)\ndf.to_csv(csv_path, index=False)\n\nprint(f\"\u2713 CSV saved to: {csv_path}\", flush=True)\nprint(f\"  Total records: {len(df):,}\", flush=True)\nprint(f\"  File size: {os.path.getsize(csv_path) / (1024*1024):.2f} MB\", flush=True)\n\n# Display sample\nprint(f\"\\nFirst 10 rows:\", flush=True)\ndisplay(df.head(10))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Results & Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View statistics and download your synthetic data.\n",
    "\n",
    "**CSV Format**:\n",
    "- `SUBJECT_ID`: Synthetic patient identifier (SYNTHETIC_000001, etc.)\n",
    "- `VISIT_NUM`: Visit sequence number (1, 2, 3, ...)\n",
    "- `ICD9_CODE`: ICD-9 diagnosis code\n",
    "\n",
    "**Data Quality Checks**:\n",
    "- Patient IDs should be unique and sequential\n",
    "- Visit numbers should be sequential for each patient\n",
    "- ICD-9 codes should match MIMIC-III code format"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Validate generated data\nprint(\"=\" * 60)\nprint(\"DATA QUALITY CHECKS\")\nprint(\"=\" * 60)\n\n# Check 1: Patient IDs\nunique_patients = df['SUBJECT_ID'].nunique()\nprint(f\"\\n\u2713 Unique patients: {unique_patients}\")\nassert unique_patients == N_SYNTHETIC_SAMPLES, f\"Expected {N_SYNTHETIC_SAMPLES} patients\"\n\n# Check 2: No empty values\nempty_subjects = df['SUBJECT_ID'].isna().sum()\nempty_visits = df['VISIT_NUM'].isna().sum()\nempty_codes = df['ICD9_CODE'].isna().sum()\n\nprint(f\"\u2713 Empty values check:\")\nprint(f\"  Subject IDs: {empty_subjects} (should be 0)\")\nprint(f\"  Visit numbers: {empty_visits} (should be 0)\")\nprint(f\"  ICD9 codes: {empty_codes} (should be 0)\")\n\nassert empty_subjects == 0 and empty_visits == 0 and empty_codes == 0, \"Found empty values!\"\n\n# Check 3: Visit number sequencing (sample first patient)\nfirst_patient = df[df['SUBJECT_ID'] == 'SYNTHETIC_000001']['VISIT_NUM'].tolist()\nis_sequential = first_patient == list(range(1, len(first_patient) + 1))\nprint(f\"\\n\u2713 Visit sequencing (first patient): {is_sequential}\")\n\n# Check 4: ICD9 code format (should be strings, common patterns)\nsample_codes = df['ICD9_CODE'].head(20).tolist()\nprint(f\"\\n\u2713 Sample ICD9 codes: {', '.join(map(str, sample_codes[:10]))}\")\n\n# Check 5: Distribution statistics\ncodes_per_patient = df.groupby('SUBJECT_ID').size()\nprint(f\"\\n\u2713 Codes per patient distribution:\")\nprint(f\"  Min: {codes_per_patient.min()}\")\nprint(f\"  Max: {codes_per_patient.max()}\")\nprint(f\"  Mean: {codes_per_patient.mean():.2f}\")\nprint(f\"  Median: {codes_per_patient.median():.2f}\")\n\nvisits_per_patient = df.groupby('SUBJECT_ID')['VISIT_NUM'].max()\nprint(f\"\\n\u2713 Visits per patient distribution:\")\nprint(f\"  Min: {visits_per_patient.min()}\")\nprint(f\"  Max: {visits_per_patient.max()}\")\nprint(f\"  Mean: {visits_per_patient.mean():.2f}\")\nprint(f\"  Median: {visits_per_patient.median():.2f}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\u2713 ALL QUALITY CHECKS PASSED\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Download CSV file\nprint(\"=\" * 60)\nprint(\"DOWNLOAD SYNTHETIC DATA\")\nprint(\"=\" * 60)\n\nprint(f\"\\nYour synthetic data is ready:\")\nprint(f\"  File: halo_synthetic_data.csv\")\nprint(f\"  Patients: {unique_patients:,}\")\nprint(f\"  Total records: {len(df):,}\")\nprint(f\"  Size: {os.path.getsize(csv_path) / (1024*1024):.2f} MB\")\n\nprint(f\"\\nDownloading file to your computer...\")\nfiles.download(csv_path)\n\nprint(f\"\\n\u2713 Download started!\")\nprint(f\"\\nFile also saved in Google Drive:\")\nprint(f\"  {csv_path}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83c\udf89 Congratulations!\n",
    "\n",
    "You've successfully:\n",
    "1. \u2705 Trained a HALO model on your MIMIC-III data\n",
    "2. \u2705 Generated synthetic patients\n",
    "3. \u2705 Validated the synthetic data quality\n",
    "4. \u2705 Downloaded the CSV file\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Use your synthetic data:**\n",
    "- Train predictive models (readmission, mortality, etc.)\n",
    "- Develop clinical decision support tools\n",
    "- Share data without privacy concerns\n",
    "\n",
    "**Generate more samples:**\n",
    "- Change `N_SYNTHETIC_SAMPLES` and re-run generation section\n",
    "- No need to retrain - the checkpoint is saved!\n",
    "\n",
    "**Production training:**\n",
    "- For publication-quality results, train with `EPOCHS = 80`\n",
    "- Consider using Colab Pro or a dedicated GPU cluster\n",
    "- See `examples/slurm/train_halo_mimic3.slurm` for cluster usage\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Out of memory errors:**\n",
    "- Reduce `BATCH_SIZE` (try 24 or 16)\n",
    "- Close other browser tabs\n",
    "- Restart runtime and try again\n",
    "\n",
    "**Training too slow:**\n",
    "- Verify GPU is enabled (Runtime \u2192 Change runtime type)\n",
    "- Reduce dataset size for testing\n",
    "- Consider using Colab Pro for better GPUs\n",
    "\n",
    "**Questions or issues:**\n",
    "- PyHealth docs: https://pyhealth.readthedocs.io/\n",
    "- HALO paper: https://arxiv.org/abs/2406.16061\n",
    "- GitHub issues: https://github.com/sunlabuiuc/PyHealth/issues"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}