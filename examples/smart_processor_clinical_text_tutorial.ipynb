{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# Smart Processor: Clinical Text + Temporal Data in PyHealth\n",
                "\n",
                "This notebook demonstrates the **Smart Processor** pattern in PyHealth, where tokenization logic lives inside the `Processor` class rather than being scattered across models.\n",
                "\n",
                "**Contents:**\n",
                "1. `TupleTimeTextProcessor` — raw vs. tokenized\n",
                "2. `EmbeddingModel` — 3D input handling: `(B, N_notes, L)` → `(B, N_notes, H)`\n",
                "3. `MLP` with tokenized clinical notes\n",
                "4. `Transformer` with tokenized clinical notes\n",
                "5. `RNN` with tokenized clinical notes\n",
                "6. `MultimodalRNN` — notes **+** EHR codes\n",
                "7. End-to-end backward pass\n",
                "\n",
                "> Uses `prajjwal1/bert-tiny` (4.4M params) — runs on CPU in under a minute."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "setup_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "# !pip install pyhealth transformers -q\n",
                "\n",
                "import warnings, random\n",
                "warnings.filterwarnings('ignore')\n",
                "import torch\n",
                "from torch.utils.data import DataLoader\n",
                "from torch.nn.utils.rnn import pad_sequence\n",
                "\n",
                "EMBEDDING_DIM = 128\n",
                "MODEL_NAME    = 'prajjwal1/bert-tiny'\n",
                "random.seed(42)\n",
                "\n",
                "print(f'Device  : {\"cuda\" if torch.cuda.is_available() else \"cpu\"}')\n",
                "print(f'PyTorch : {torch.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part1_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. TupleTimeTextProcessor\n",
                "\n",
                "Takes `(List[str], List[float])` — clinical notes paired with hours-since-admission timestamps.  \n",
                "When `tokenizer_model` is set, returns `(input_ids, attn_mask, token_type_ids, time, type_tag)` tensors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "proc_no_tok",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyhealth.processors import TupleTimeTextProcessor\n",
                "\n",
                "notes = ['Admission: chest pain.', 'Day 3: improved.', 'Discharge: stable.']\n",
                "times = [0.0, 72.0, 120.0]\n",
                "\n",
                "# ── Without tokenizer ──────────────────────────────────────────────────────\n",
                "proc_raw = TupleTimeTextProcessor(type_tag='note')\n",
                "texts_out, time_t, tag = proc_raw.process((notes, times))\n",
                "print('Without tokenizer')\n",
                "print(f'  schema()   : {proc_raw.schema()}')\n",
                "print(f'  is_token() : {proc_raw.is_token()}')\n",
                "print(f'  output     : {texts_out}, time={time_t}, tag=\"{tag}\"')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "proc_with_tok",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ── With HuggingFace tokenizer ─────────────────────────────────────────────\n",
                "proc_tok = TupleTimeTextProcessor(tokenizer_model=MODEL_NAME, max_length=32)\n",
                "input_ids, attn_mask, token_type_ids, time_t, tag = proc_tok.process((notes, times))\n",
                "\n",
                "print('With tokenizer')\n",
                "print(f'  schema()        : {proc_tok.schema()}')\n",
                "print(f'  is_token()      : {proc_tok.is_token()}')\n",
                "print(f'  input_ids shape : {input_ids.shape}   # (N_notes, max_length)')\n",
                "print(f'  attn_mask shape : {attn_mask.shape}')\n",
                "print(f'  sample tokens   : {input_ids[0, :8].tolist()} ...')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "part2_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. EmbeddingModel — 3D Input Handling\n",
                "\n",
                "`EmbeddingModel` detects `is_token()=True` and loads the HuggingFace model automatically. It handles **3D** inputs:\n",
                "```\n",
                "(B, N_notes, L) → flatten(B·N, L) → BERT → CLS[:, 0, :] → unflatten → (B, N_notes, H)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "embedding_model_test",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyhealth.datasets import create_sample_dataset\n",
                "from pyhealth.models.embedding import EmbeddingModel\n",
                "\n",
                "# Binary label processor requires at least one sample per class\n",
                "ds_emb = create_sample_dataset(\n",
                "    samples=[\n",
                "        {'patient_id': 'p0', 'visit_id': 'v0',\n",
                "         'notes': (['Chest pain.', 'Day 3.', 'Discharge.'], [0., 72., 120.]),\n",
                "         'label': 1},\n",
                "        {'patient_id': 'p1', 'visit_id': 'v1',\n",
                "         'notes': (['Pneumonia.', 'Antibiotics.', 'Stable.'], [0., 24., 72.]),\n",
                "         'label': 0},\n",
                "    ],\n",
                "    input_schema={'notes': proc_tok},\n",
                "    output_schema={'label': 'binary'},\n",
                "    dataset_name='emb_demo',\n",
                ")\n",
                "\n",
                "embedding_model = EmbeddingModel(ds_emb, embedding_dim=EMBEDDING_DIM)\n",
                "print(f'Embedding layer : {type(embedding_model.embedding_layers[\"notes\"]).__name__}')\n",
                "\n",
                "# Simulate a padded batch: 2 patients × 3 notes × 32 tokens\n",
                "B, N, L = 2, 3, 32\n",
                "fake_ids  = torch.randint(1, 1000, (B, N, L))\n",
                "fake_mask = torch.ones(B, N, L, dtype=torch.long)\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = embedding_model({'notes': fake_ids}, masks={'notes': fake_mask})\n",
                "\n",
                "print(f'\\nInput  : {fake_ids.shape}   → (B, N_notes, Seq_len)')\n",
                "print(f'Output : {out[\"notes\"].shape}  → (B, N_notes, Embedding_dim)')\n",
                "assert out['notes'].shape == (B, N, EMBEDDING_DIM)\n",
                "print('\\n✓  3D → CLS pool → 3D works.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "collate_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Dataset + Custom Collate\n",
                "\n",
                "Our processor returns **5-element tuples**. The default PyHealth collate handles 2-element tuples only.  \n",
                "We provide a `collate_smart_processor` that properly stacks each element across the batch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "collate_code",
            "metadata": {},
            "outputs": [],
            "source": [
                "def collate_smart_processor(batch):\n",
                "    \"\"\"Batch collator for TupleTimeTextProcessor 5-element tuples.\n",
                "\n",
                "    For tuple-valued features, zips across the batch and stacks each\n",
                "    tensor element to produce (B, N_notes, ...) tensors.\n",
                "    \"\"\"\n",
                "    result = {}\n",
                "    for key in batch[0].keys():\n",
                "        vals = [s[key] for s in batch]\n",
                "\n",
                "        if isinstance(vals[0], tuple):\n",
                "            collated = []\n",
                "            for elem_list in zip(*vals):\n",
                "                if isinstance(elem_list[0], torch.Tensor):\n",
                "                    if all(e.shape == elem_list[0].shape for e in elem_list):\n",
                "                        collated.append(torch.stack(list(elem_list)))\n",
                "                    else:\n",
                "                        collated.append(pad_sequence(list(elem_list), batch_first=True))\n",
                "                else:\n",
                "                    collated.append(list(elem_list))   # e.g. type_tag strings\n",
                "            result[key] = tuple(collated)\n",
                "\n",
                "        elif isinstance(vals[0], torch.Tensor):\n",
                "            if all(v.shape == vals[0].shape for v in vals):\n",
                "                result[key] = torch.stack(vals)\n",
                "            else:\n",
                "                result[key] = pad_sequence(vals, batch_first=True)\n",
                "        else:\n",
                "            result[key] = vals\n",
                "\n",
                "    return result\n",
                "\n",
                "print('collate_smart_processor defined ✓')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_dataset",
            "metadata": {},
            "outputs": [],
            "source": [
                "CLINICAL_NOTES = [\n",
                "    'Patient presents with acute chest pain.',\n",
                "    'Labs: elevated troponin. ST elevation on ECG.',\n",
                "    'Hemodynamically stable post-PCI procedure.',\n",
                "    'Discharge summary: no complications. Follow-up in 2 weeks.',\n",
                "    'Readmission: persistent cough, possible pneumonia.',\n",
                "    'CXR: bilateral infiltrates. Started antibiotics.',\n",
                "]\n",
                "\n",
                "def make_sample(pid, vid, label):\n",
                "    \"\"\"3 notes per patient — constant N so tensors collate cleanly.\"\"\"\n",
                "    ns = random.choices(CLINICAL_NOTES, k=3)\n",
                "    return {'patient_id': pid, 'visit_id': vid,\n",
                "            'notes': (ns, [0., 24., 72.]), 'label': label}\n",
                "\n",
                "samples = [\n",
                "    make_sample('p0', 'v0', 1),\n",
                "    make_sample('p1', 'v1', 0),\n",
                "    make_sample('p2', 'v2', 1),\n",
                "    make_sample('p3', 'v3', 0),\n",
                "    make_sample('p4', 'v4', 1),\n",
                "]\n",
                "\n",
                "dataset = create_sample_dataset(\n",
                "    samples=samples,\n",
                "    input_schema={'notes': proc_tok},\n",
                "    output_schema={'label': 'binary'},\n",
                "    dataset_name='smart_proc_demo',\n",
                ")\n",
                "dataset.set_shuffle(False)\n",
                "\n",
                "loader = DataLoader(dataset, batch_size=2, collate_fn=collate_smart_processor)\n",
                "batch  = next(iter(loader))\n",
                "\n",
                "# notes is a 5-tuple: (input_ids, attn_mask, token_type_ids, time, [type_tags])\n",
                "input_ids_b, attn_mask_b, _, time_b, tags_b = batch['notes']\n",
                "\n",
                "print(f'Batch keys         : {list(batch.keys())}')\n",
                "print(f'input_ids shape    : {input_ids_b.shape}   # (B, N_notes, max_length)')\n",
                "print(f'attn_mask shape    : {attn_mask_b.shape}')\n",
                "print(f'time shape         : {time_b.shape}')\n",
                "print(f'labels             : {batch[\"label\"]}')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "mlp_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. MLP Model\n",
                "\n",
                "Note embeddings `(B, N, H)` are pooled across the note sequence → linear classification head."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mlp_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyhealth.models import MLP\n",
                "\n",
                "mlp_model = MLP(dataset=dataset, embedding_dim=EMBEDDING_DIM, hidden_dim=64)\n",
                "mlp_model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = mlp_model(**batch)\n",
                "\n",
                "print('MLP')\n",
                "print(f'  loss   : {out[\"loss\"].item():.4f}')\n",
                "print(f'  y_prob : {out[\"y_prob\"].squeeze().tolist()}')\n",
                "print(f'  y_true : {out[\"y_true\"].tolist()}')\n",
                "print('\\n✓  MLP succeeded.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "transformer_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Transformer Model\n",
                "\n",
                "Self-attention over the note embedding sequence (`N_notes` steps), capturing inter-note relationships."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "transformer_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyhealth.models import Transformer\n",
                "\n",
                "# Note: Transformer uses heads= (not num_heads=)\n",
                "tfm_model = Transformer(\n",
                "    dataset=dataset, embedding_dim=EMBEDDING_DIM,\n",
                "    heads=4, num_layers=2, dropout=0.1,\n",
                ")\n",
                "tfm_model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = tfm_model(**batch)\n",
                "\n",
                "print('Transformer')\n",
                "print(f'  loss   : {out[\"loss\"].item():.4f}')\n",
                "print(f'  y_prob : {out[\"y_prob\"].squeeze().tolist()}')\n",
                "print('\\n✓  Transformer succeeded.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rnn_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. RNN Model\n",
                "\n",
                "GRU processes the note embedding sequence in chronological order."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rnn_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyhealth.models import RNN\n",
                "\n",
                "rnn_model = RNN(\n",
                "    dataset=dataset, embedding_dim=EMBEDDING_DIM,\n",
                "    hidden_dim=64, rnn_type='GRU',\n",
                ")\n",
                "rnn_model.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = rnn_model(**batch)\n",
                "\n",
                "print('RNN  (GRU over note sequence)')\n",
                "print(f'  loss   : {out[\"loss\"].item():.4f}')\n",
                "print(f'  y_prob : {out[\"y_prob\"].squeeze().tolist()}')\n",
                "print('\\n✓  RNN succeeded.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "multimodal_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. MultimodalRNN — Clinical Notes + EHR Codes\n",
                "\n",
                "**Flagship use-case**: two modalities fused in a single model:\n",
                "- **Notes** → BERT-tiny → `(B, 3, H)` note embeddings  \n",
                "- **EHR codes** → learned embeddings → `(B, T, H)` code sequence  \n",
                "\n",
                "Both are independently embedded and then concatenated before the shared GRU."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "multimodal_dataset",
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyhealth.processors import SequenceProcessor\n",
                "from pyhealth.models import MultimodalRNN\n",
                "\n",
                "code_proc = SequenceProcessor()\n",
                "\n",
                "EHR_CODES = [\n",
                "    ['I21.0', 'Z87.39', 'I10'],      # STEMI + cardiovascular hx + HTN\n",
                "    ['I21.0', 'R07.9',  'I10'],      # STEMI + chest pain + HTN\n",
                "    ['J18.9', 'R05',    'Z87.01'],   # Pneumonia + cough + smoking hx\n",
                "    ['I21.0', 'I25.10', 'Z95.1'],   # STEMI + CAD + cardiac stent\n",
                "    ['J18.9', 'J96.00', 'R09.02'],  # Pneumonia + ARDS + hypoxia\n",
                "]\n",
                "\n",
                "def make_mm_sample(pid, vid, codes, label):\n",
                "    ns = random.choices(CLINICAL_NOTES, k=3)\n",
                "    return {\n",
                "        'patient_id': pid, 'visit_id': vid,\n",
                "        'notes': (ns, [0., 24., 72.]),\n",
                "        'ehr_codes': codes,\n",
                "        'label': label,\n",
                "    }\n",
                "\n",
                "mm_samples = [\n",
                "    make_mm_sample('p0', 'v0', EHR_CODES[0], 1),\n",
                "    make_mm_sample('p1', 'v1', EHR_CODES[1], 0),\n",
                "    make_mm_sample('p2', 'v2', EHR_CODES[2], 1),\n",
                "    make_mm_sample('p3', 'v3', EHR_CODES[3], 0),\n",
                "    make_mm_sample('p4', 'v4', EHR_CODES[4], 1),\n",
                "]\n",
                "\n",
                "mm_dataset = create_sample_dataset(\n",
                "    samples=mm_samples,\n",
                "    input_schema={'notes': proc_tok, 'ehr_codes': code_proc},\n",
                "    output_schema={'label': 'binary'},\n",
                "    dataset_name='multimodal_demo',\n",
                ")\n",
                "mm_dataset.set_shuffle(False)\n",
                "\n",
                "mm_loader = DataLoader(mm_dataset, batch_size=2, collate_fn=collate_smart_processor)\n",
                "mm_batch  = next(iter(mm_loader))\n",
                "\n",
                "print(f'Batch keys               : {list(mm_batch.keys())}')\n",
                "print(f'notes (input_ids) shape  : {mm_batch[\"notes\"][0].shape}   # (B, N_notes, max_len)')\n",
                "ehr_val = mm_batch['ehr_codes']\n",
                "print(f'ehr_codes                : {type(ehr_val).__name__}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "multimodal_rnn",
            "metadata": {},
            "outputs": [],
            "source": [
                "mm_rnn = MultimodalRNN(\n",
                "    dataset=mm_dataset, embedding_dim=EMBEDDING_DIM,\n",
                "    hidden_dim=64, rnn_type='GRU',\n",
                ")\n",
                "mm_rnn.eval()\n",
                "\n",
                "with torch.no_grad():\n",
                "    out = mm_rnn(**mm_batch)\n",
                "\n",
                "print('MultimodalRNN  (BERT notes + EHR codes → GRU → prediction)')\n",
                "print(f'  loss   : {out[\"loss\"].item():.4f}')\n",
                "print(f'  y_prob : {out[\"y_prob\"].squeeze().tolist()}')\n",
                "print(f'  y_true : {out[\"y_true\"].tolist()}')\n",
                "print('\\n✓  MultimodalRNN fused clinical text + EHR codes!')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "backprop_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. End-to-End Backpropagation\n",
                "\n",
                "Verify gradients flow from the classification loss all the way back through BERT."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "backprop_test",
            "metadata": {},
            "outputs": [],
            "source": [
                "print('Gradient flow — all 4 models')\n",
                "print('─' * 55)\n",
                "\n",
                "for name, model, b in [\n",
                "    ('MLP',           mlp_model, batch),\n",
                "    ('Transformer',   tfm_model, batch),\n",
                "    ('RNN',           rnn_model, batch),\n",
                "    ('MultimodalRNN', mm_rnn,    mm_batch),\n",
                "]:\n",
                "    model.train()\n",
                "    model.zero_grad()\n",
                "    model(**b)['loss'].backward()\n",
                "\n",
                "    # nn.ModuleDict doesn't support .get() — cast to plain dict first\n",
                "    el   = dict(model.embedding_model.embedding_layers)\n",
                "    proj = el.get('notes_proj', None)\n",
                "    if proj is not None:\n",
                "        g     = proj.weight.grad.norm().item()\n",
                "        label = 'projection layer'\n",
                "    else:\n",
                "        g = next(\n",
                "            (p.grad.norm().item() for p in el['notes'].parameters() if p.grad is not None),\n",
                "            0.0\n",
                "        )\n",
                "        label = 'BERT parameter'\n",
                "\n",
                "    print(f'  {\"✓\" if g > 0 else \"✗ NO GRAD\"}  {name:<15} {label} grad_norm = {g:.4f}')\n",
                "\n",
                "print('\\nAll models propagate gradients through the text encoder. ✓')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "summary_header",
            "metadata": {},
            "source": [
                "---\n",
                "## 9. Summary\n",
                "\n",
                "| Model | Text encoder | EHR codes | Forward | Backward |\n",
                "|-------|-------------|-----------|---------|----------|\n",
                "| `MLP` | BERT-tiny | — | ✓ | ✓ |\n",
                "| `Transformer` | BERT-tiny | — | ✓ | ✓ |\n",
                "| `RNN` | BERT-tiny | — | ✓ | ✓ |\n",
                "| `MultimodalRNN` | BERT-tiny | ✓ | ✓ | ✓ |\n",
                "\n",
                "### Key Design Principles\n",
                "\n",
                "| Principle | Detail |\n",
                "|-----------|--------|\n",
                "| **Smart Processor** | Tokenization in `TupleTimeTextProcessor`, not in models |\n",
                "| **Pre-tokenization** | Text → `int` tensors at build time, no `List[str]` in DataLoader |\n",
                "| **3D Handling** | `EmbeddingModel`: `(B, N, L)` → BERT → CLS → `(B, N, H)` |\n",
                "| **Mask propagation** | Attention masks forwarded to BERT, padding ignored |\n",
                "| **RNN mask reduction** | 3D token mask reduced to 2D note mask for `pack_padded_sequence` |\n",
                "| **Backward-compatible** | `RETAIN`, `ConCare`, `StageAttentionNet` unchanged |\n",
                "\n",
                "### Production usage (Bio_ClinicalBERT)\n",
                "\n",
                "```python\n",
                "from pyhealth.processors import TupleTimeTextProcessor\n",
                "from pyhealth.models import MultimodalRNN\n",
                "\n",
                "note_proc = TupleTimeTextProcessor(\n",
                "    tokenizer_model='emilyalsentzer/Bio_ClinicalBERT',\n",
                "    max_length=512,\n",
                ")\n",
                "# Build dataset and models exactly as shown above\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}