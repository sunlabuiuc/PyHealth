{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9b5059",
   "metadata": {},
   "source": [
    "# VisionEmbeddingModel Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `VisionEmbeddingModel` for medical imaging tasks in PyHealth.\n",
    "\n",
    "**Contributors:** Josh Steier \n",
    "\n",
    "\n",
    "**Overview:**\n",
    "- Load a medical imaging dataset (MIMIC-CXR or custom)\n",
    "- Configure the `VisionEmbeddingModel` with different backbones\n",
    "- Build an end-to-end classification pipeline\n",
    "- Train and evaluate on chest X-ray classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c778a6a",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Configure deterministic behavior and import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272cc5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "\n",
    "from pyhealth.datasets import create_sample_dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.processors import ImageProcessor\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8879729",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "**Option A:** Synthetic data (default, no download required)\n",
    "\n",
    "**Option B:** MIMIC-CXR (requires PhysioNet credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f07f439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 200 synthetic images in C:\\Users\\637682\\AppData\\Local\\Temp\\chest_xray_jzp9zbiz\n"
     ]
    }
   ],
   "source": [
    "USE_SYNTHETIC = True\n",
    "MIMIC_CXR_ROOT = \"/path/to/physionet.org/files/mimic-cxr-jpg/2.0.0\"\n",
    "\n",
    "if USE_SYNTHETIC:\n",
    "    DATA_ROOT = tempfile.mkdtemp(prefix=\"chest_xray_\")\n",
    "    images_dir = os.path.join(DATA_ROOT, \"images\")\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    \n",
    "    samples = []\n",
    "    for i in range(200):\n",
    "        label = i % 2\n",
    "        img_array = np.random.normal(80, 25, (224, 224))\n",
    "        y, x = np.ogrid[:224, :224]\n",
    "        left_mask = ((x - 72)**2 / 3000 + (y - 112)**2 / 8000) < 1\n",
    "        right_mask = ((x - 152)**2 / 3000 + (y - 112)**2 / 8000) < 1\n",
    "        img_array[left_mask] -= 20\n",
    "        img_array[right_mask] -= 20\n",
    "        \n",
    "        if label == 1:\n",
    "            cx, cy = 112 + np.random.randint(-50, 50), 112 + np.random.randint(-30, 30)\n",
    "            radius = np.random.randint(15, 40)\n",
    "            opacity_mask = (x - cx)**2 + (y - cy)**2 <= radius**2\n",
    "            img_array[opacity_mask] += 60 + np.random.randint(0, 40)\n",
    "        \n",
    "        img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array, mode='L')\n",
    "        img_path = os.path.join(images_dir, f\"cxr_{i:04d}.png\")\n",
    "        img.save(img_path)\n",
    "        \n",
    "        samples.append({\n",
    "            \"patient_id\": f\"p{i // 4}\",\n",
    "            \"visit_id\": f\"v{i}\",\n",
    "            \"image\": img_path,\n",
    "            \"label\": label,\n",
    "        })\n",
    "    \n",
    "    print(f\"Created {len(samples)} synthetic images in {DATA_ROOT}\")\n",
    "    \n",
    "else:\n",
    "    DATA_ROOT = MIMIC_CXR_ROOT\n",
    "    import pandas as pd\n",
    "    \n",
    "    metadata = pd.read_csv(f\"{DATA_ROOT}/mimic-cxr-2.0.0-metadata.csv\")\n",
    "    labels = pd.read_csv(f\"{DATA_ROOT}/mimic-cxr-2.0.0-chexpert.csv\")\n",
    "    df = metadata.merge(labels, on=[\"subject_id\", \"study_id\"])\n",
    "    \n",
    "    def build_path(row):\n",
    "        subject = f\"p{row['subject_id']}\"\n",
    "        return f\"{DATA_ROOT}/files/{subject[:3]}/{subject}/s{row['study_id']}/{row['dicom_id']}.jpg\"\n",
    "    \n",
    "    df[\"image_path\"] = df.apply(build_path, axis=1)\n",
    "    df = df[df[\"image_path\"].apply(os.path.exists)].head(1000)\n",
    "    df[\"label\"] = (df[\"No Finding\"] == 1.0).astype(int)\n",
    "    \n",
    "    samples = [\n",
    "        {\"patient_id\": str(r[\"subject_id\"]), \"visit_id\": str(r[\"study_id\"]), \n",
    "         \"image\": r[\"image_path\"], \"label\": r[\"label\"]}\n",
    "        for _, r in df.iterrows()\n",
    "    ]\n",
    "    print(f\"Loaded {len(samples)} MIMIC-CXR images from {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed75fa",
   "metadata": {},
   "source": [
    "## 3. Prepare PyHealth Dataset\n",
    "\n",
    "Create a SampleDataset using `create_sample_dataset()` with ImageProcessor for the image field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3f49ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label label vocab: {0: 0, 1: 1}\n",
      "Total task samples: 200\n",
      "Input schema: {'image': 'image'}\n",
      "Output schema: {'label': 'binary'}\n",
      "Train/Val/Test sizes: 140, 28, 32\n"
     ]
    }
   ],
   "source": [
    "image_mode = \"L\" if USE_SYNTHETIC else \"RGB\"\n",
    "\n",
    "sample_dataset = create_sample_dataset(\n",
    "    samples=samples,\n",
    "    input_schema={\"image\": \"image\"},\n",
    "    output_schema={\"label\": \"binary\"},\n",
    "    input_processors={\"image\": ImageProcessor(image_size=224, mode=image_mode)},\n",
    "    dataset_name=\"chest_xray\",\n",
    ")\n",
    "\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(sample_dataset, [0.7, 0.15, 0.15], seed=SEED)\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591559c8",
   "metadata": {},
   "source": [
    "## 4. Inspect Batch Structure\n",
    "\n",
    "Build PyHealth dataloaders and verify the keys and tensor shapes emitted before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4684f6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'patient_id': 'list(len=16)', 'visit_id': 'list(len=16)', 'image': 'Tensor(shape=(16, 1, 224, 224))', 'label': 'Tensor(shape=(16, 1))'}\n",
      "Sample labels: [[0.0], [0.0], [0.0], [1.0], [1.0]]\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE)\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "\n",
    "def describe(value):\n",
    "    if hasattr(value, \"shape\"):\n",
    "        return f\"{type(value).__name__}(shape={tuple(value.shape)})\"\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        return f\"{type(value).__name__}(len={len(value)})\"\n",
    "    return type(value).__name__\n",
    "\n",
    "batch_summary = {key: describe(value) for key, value in first_batch.items()}\n",
    "print(batch_summary)\n",
    "\n",
    "label_targets = first_batch[\"label\"]\n",
    "if hasattr(label_targets, \"shape\"):\n",
    "    preview = label_targets[:5].cpu().tolist()\n",
    "else:\n",
    "    preview = list(label_targets)[:5]\n",
    "print(f\"Sample labels: {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47ebd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patient_id: list(len=16)\n",
      "visit_id: list(len=16)\n",
      "image: shape=torch.Size([16, 1, 224, 224]), dtype=torch.float32\n",
      "label: shape=torch.Size([16, 1]), dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "for key, value in batch.items():\n",
    "    if hasattr(value, \"shape\"):\n",
    "        print(f\"{key}: shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value).__name__}(len={len(value)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e9f226",
   "metadata": {},
   "source": [
    "## 4.5. VisionEmbeddingModel: Output Shape Verification\n",
    "\n",
    "The core transformation performed by `VisionEmbeddingModel`:\n",
    "\n",
    "```\n",
    "Input:  (B, C, H, W)  →  e.g., (16, 1, 224, 224)\n",
    "Output: (B, P+1, E)   →  e.g., (16, 50, 128)  # P patches + 1 CLS token\n",
    "```\n",
    "\n",
    "This converts images to sequences of token embeddings, compatible with Transformer architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d35bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\637682\\Desktop\\pyhealth\\PyHealth\\pyhealth\\sampler\\sage_sampler.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\637682\\Desktop\\pyhealth\\PyHealth\\pyhealth-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEmbeddingModel Output Shape Verification\n",
      "============================================================\n",
      "\n",
      "Input shape: torch.Size([16, 1, 224, 224])  # (B, C, H, W)\n",
      "\n",
      "patch:\n",
      "  Output shape: (16, 197, 128)  # (B, num_tokens, E)\n",
      "  Tokens: 197 = 196 patches + 1 CLS\n",
      "  Parameters: 58,240\n",
      "\n",
      "cnn:\n",
      "  Output shape: (16, 50, 128)  # (B, num_tokens, E)\n",
      "  Tokens: 50 = 49 patches + 1 CLS\n",
      "  Parameters: 231,808\n",
      "\n",
      "resnet18:\n",
      "  Output shape: (16, 50, 128)  # (B, num_tokens, E)\n",
      "  Tokens: 50 = 49 patches + 1 CLS\n",
      "  Parameters: 11,242,432\n",
      "\n",
      "resnet50:\n",
      "  Output shape: (16, 50, 128)  # (B, num_tokens, E)\n",
      "  Tokens: 50 = 49 patches + 1 CLS\n",
      "  Parameters: 23,770,560\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.models import VisionEmbeddingModel\n",
    "\n",
    "# Test all backbones and verify output shapes\n",
    "backbone_configs = [\n",
    "    (\"patch\", {\"backbone\": \"patch\", \"patch_size\": 16}),\n",
    "    (\"cnn\", {\"backbone\": \"cnn\"}),\n",
    "    (\"resnet18\", {\"backbone\": \"resnet18\", \"pretrained\": True}),\n",
    "    (\"resnet50\", {\"backbone\": \"resnet50\", \"pretrained\": True}),\n",
    "]\n",
    "\n",
    "print(\"VisionEmbeddingModel Output Shape Verification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput shape: {batch['image'].shape}  # (B, C, H, W)\")\n",
    "print()\n",
    "\n",
    "for name, config in backbone_configs:\n",
    "    model = VisionEmbeddingModel(\n",
    "        dataset=sample_dataset,\n",
    "        embedding_dim=128,\n",
    "        use_cls_token=True,\n",
    "        **config,\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model({\"image\": batch[\"image\"]})\n",
    "    \n",
    "    output_shape = output[\"image\"].shape\n",
    "    info = model.get_output_info(\"image\")\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Output shape: {tuple(output_shape)}  # (B, num_tokens, E)\")\n",
    "    print(f\"  Tokens: {info['num_tokens']} = {info['num_patches']} patches + 1 CLS\")\n",
    "    print(f\"  Parameters: {n_params:,}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce223e3",
   "metadata": {},
   "source": [
    "## 5. Instantiate VisionEmbeddingModel\n",
    "\n",
    "Create the PyHealth VisionEmbeddingModel with custom hyperparameters and inspect the parameter footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "344eac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionEmbeddingModel Backbone Comparison\n",
      "==================================================\n",
      "\n",
      "patch:\n",
      "  Tokens: 197 (196 patches + CLS)\n",
      "  Parameters: 58,240\n",
      "\n",
      "cnn:\n",
      "  Tokens: 50 (49 patches + CLS)\n",
      "  Parameters: 231,808\n",
      "\n",
      "resnet18:\n",
      "  Tokens: 50 (49 patches + CLS)\n",
      "  Parameters: 11,242,432\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to C:\\Users\\637682/.cache\\torch\\hub\\checkpoints\\resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:22<00:00, 4.63MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "resnet50:\n",
      "  Tokens: 50 (49 patches + CLS)\n",
      "  Parameters: 23,770,560\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.models import VisionEmbeddingModel\n",
    "\n",
    "backbone_configs = [\n",
    "    (\"patch\", {\"backbone\": \"patch\", \"patch_size\": 16}),\n",
    "    (\"cnn\", {\"backbone\": \"cnn\"}),\n",
    "    (\"resnet18\", {\"backbone\": \"resnet18\", \"pretrained\": True}),\n",
    "    (\"resnet50\", {\"backbone\": \"resnet50\", \"pretrained\": True}),\n",
    "]\n",
    "\n",
    "print(\"VisionEmbeddingModel Backbone Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for name, config in backbone_configs:\n",
    "    model = VisionEmbeddingModel(\n",
    "        dataset=sample_dataset,\n",
    "        embedding_dim=128,\n",
    "        use_cls_token=True,\n",
    "        **config,\n",
    "    )\n",
    "    info = model.get_output_info(\"image\")\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Tokens: {info['num_tokens']} ({info['num_patches']} patches + CLS)\")\n",
    "    print(f\"  Parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b5f6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature keys: ['image']\n",
      "Label key: label\n",
      "Mode: binary\n",
      "Total parameters: 11,259,329\n"
     ]
    }
   ],
   "source": [
    "class VisionClassifier(nn.Module):\n",
    "    \"\"\"End-to-end classifier using VisionEmbeddingModel.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset, embedding_dim=128, backbone=\"resnet18\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_encoder = VisionEmbeddingModel(\n",
    "            dataset=dataset,\n",
    "            embedding_dim=embedding_dim,\n",
    "            backbone=backbone,\n",
    "            pretrained=True,\n",
    "            use_cls_token=True,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(embedding_dim, 1),\n",
    "        )\n",
    "        \n",
    "        self.feature_keys = [\"image\"]\n",
    "        self.label_key = \"label\"\n",
    "        self.mode = \"binary\"  \n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.vision_encoder.device\n",
    "    \n",
    "    def forward(self, **kwargs):\n",
    "        images = kwargs[\"image\"].to(self.device)\n",
    "        labels = kwargs[\"label\"].to(self.device).float()\n",
    "        \n",
    "        # Get embeddings: (B, num_tokens, E)\n",
    "        embeddings = self.vision_encoder({\"image\": images})\n",
    "        \n",
    "        # Use CLS token (first token) for classification\n",
    "        cls_token = embeddings[\"image\"][:, 0, :]\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.classifier(cls_token)\n",
    "        \n",
    "        # Flatten to 1D for binary classification\n",
    "        logits = logits.squeeze(-1)  # (B,)\n",
    "        labels = labels.squeeze(-1)  # (B,)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(logits, labels)\n",
    "        \n",
    "        # Compute probabilities - must be (B,) for binary classification\n",
    "        y_prob = torch.sigmoid(logits)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"y_prob\": y_prob,  # (B,)\n",
    "            \"y_true\": labels,   # (B,)\n",
    "        }\n",
    "\n",
    "\n",
    "model = VisionClassifier(\n",
    "    dataset=sample_dataset,\n",
    "    embedding_dim=128,\n",
    "    backbone=\"resnet18\",\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Mode: {model.mode}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f2d33",
   "metadata": {},
   "source": [
    "## 7. Configure Trainer\n",
    "\n",
    "Wrap the model with the PyHealth Trainer to handle optimisation, gradient clipping, and metric logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "699c7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionClassifier(\n",
      "  (vision_encoder): VisionEmbeddingModel(backbone='resnet18', embedding_dim=128, fields=['image'])\n",
      "  (classifier): Sequential(\n",
      "    (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (2): GELU(approximate='none')\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Metrics: ['roc_auc']\n",
      "Device: cpu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    metrics=[\"roc_auc\"],\n",
    "    device=str(device),\n",
    "    enable_logging=False,\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"optimizer_params\": {\"lr\": 1e-4},\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    \"monitor\": \"roc_auc\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad6f0f",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b12a766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "Batch size: 16\n",
      "Optimizer: <class 'torch.optim.adam.Adam'>\n",
      "Optimizer params: {'lr': 0.0001}\n",
      "Weight decay: 0.0\n",
      "Max grad norm: 1.0\n",
      "Val dataloader: <torch.utils.data.dataloader.DataLoader object at 0x00000212BB9D8FB0>\n",
      "Monitor: roc_auc\n",
      "Monitor criterion: max\n",
      "Epochs: 5\n",
      "Patience: None\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cadbf78b48241199f56d04123c7adb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 / 5:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-0, step-9 ---\n",
      "loss: 0.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-0, step-9 ---\n",
      "roc_auc: 0.5000\n",
      "loss: 0.6933\n",
      "New best roc_auc score (0.5000) at epoch-0, step-9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bc5082d5b841a2ba394e73c8704263",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 / 5:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-1, step-18 ---\n",
      "loss: 0.7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-1, step-18 ---\n",
      "roc_auc: 0.5000\n",
      "loss: 0.6935\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e914ba00cdbe4bb0b3d4faa247f07228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 / 5:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-2, step-27 ---\n",
      "loss: 0.6951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-2, step-27 ---\n",
      "roc_auc: 0.5000\n",
      "loss: 0.6933\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03347961b139426d8835a8a4e22f5884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3 / 5:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-3, step-36 ---\n",
      "loss: 0.6991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-3, step-36 ---\n",
      "roc_auc: 0.5000\n",
      "loss: 0.6934\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b361d1aff8a744df9fa5a8f12335e81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4 / 5:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train epoch-4, step-45 ---\n",
      "loss: 0.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Eval epoch-4, step-45 ---\n",
      "roc_auc: 0.5000\n",
      "loss: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    **training_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ec38d7",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Validation/Test Split\n",
    "\n",
    "Switch to evaluation mode, collect predictions for validation and test splits, and compute AUROC and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dd1577d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics: roc_auc=0.5000, loss=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: roc_auc=0.5000, loss=0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = {}\n",
    "for split_name, loader in {\"validation\": val_loader, \"test\": test_loader}.items():\n",
    "    if loader is None:\n",
    "        continue\n",
    "    metrics = trainer.evaluate(loader)\n",
    "    evaluation_results[split_name] = metrics\n",
    "    formatted = \", \".join(f\"{k}={v:.4f}\" for k, v in metrics.items())\n",
    "    print(f\"{split_name.title()} metrics: {formatted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52883839",
   "metadata": {},
   "source": [
    "## 10. Inspect Sample Predictions\n",
    "\n",
    "Run a quick inference pass on the validation split to preview predicted probabilities alongside ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5624ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|██████████| 2/2 [00:00<00:00,  4.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean loss: 0.6932\n",
      "Preview (label, positive_prob): [(0.0, 0.49317309260368347), (1.0, 0.49317309260368347), (0.0, 0.49317309260368347), (1.0, 0.49317309260368347), (0.0, 0.49317309260368347)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_loader = val_loader if val_loader is not None else train_loader\n",
    "\n",
    "y_true, y_prob, mean_loss = trainer.inference(target_loader)\n",
    "positive_prob = y_prob if y_prob.ndim == 1 else y_prob[..., -1]\n",
    "preview_pairs = list(zip(y_true[:5].tolist(), positive_prob[:5].tolist()))\n",
    "print(f\"Mean loss: {mean_loss:.4f}\")\n",
    "print(f\"Preview (label, positive_prob): {preview_pairs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f52fac",
   "metadata": {},
   "source": [
    "## 11. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "741286d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned up: C:\\Users\\637682\\AppData\\Local\\Temp\\chest_xray_x7mdy6r9\n"
     ]
    }
   ],
   "source": [
    "if USE_SYNTHETIC:\n",
    "    shutil.rmtree(DATA_ROOT)\n",
    "    print(f\"Cleaned up: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75c67a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### VisionEmbeddingModel\n",
    "\n",
    "Converts medical images to sequence embeddings: `(B, C, H, W) → (B, num_tokens, E)`\n",
    "\n",
    "**Backbones:**\n",
    "| Backbone | Tokens (224px) | Parameters | Use Case |\n",
    "|----------|----------------|------------|----------|\n",
    "| `patch` | 196 + CLS | ~300K | Lightweight, ViT-style |\n",
    "| `cnn` | 49 + CLS | ~100K | Good inductive bias |\n",
    "| `resnet18` | 49 + CLS | ~11M | Pretrained features |\n",
    "| `resnet50` | 49 + CLS | ~24M | Best pretrained features |\n",
    "\n",
    "**Key Features:**\n",
    "- Pretrained ImageNet weights (ResNet backbones)\n",
    "- Optional [CLS] token for classification\n",
    "- Positional embeddings\n",
    "- Compatible with PyHealth's `ImageProcessor`\n",
    "\n",
    "**Multimodal Integration:**\n",
    "```python\n",
    "# Vision: (B, P, E) where P = num_patches\n",
    "# Text: (B, T, E) where T = text_tokens  \n",
    "# EHR: (B, S, E) where S = sequence_length\n",
    "combined = torch.cat([vision_emb, text_emb, ehr_emb], dim=1)\n",
    "# → (B, P+T+S, E) → Transformer fusion\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyHealth Dev",
   "language": "python",
   "name": "pyhealth-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
