{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1ec419",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "Use this section to configure deterministic behaviour and import the libraries required for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb256423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.tasks.mortality_prediction import MortalityPredictionMIMIC4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693d12",
   "metadata": {},
   "source": [
    "# 2. Load MIMIC-IV Sample Extract\n",
    "Point to the preprocessed MIMIC-IV tables, optionally override individual files, and preview their structure before building a task dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e61694c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage Starting MIMIC4Dataset init: 1280.3 MB\n",
      "Initializing MIMIC4EHRDataset with tables: ['patients', 'admissions', 'diagnoses_icd', 'procedures_icd', 'prescriptions', 'labevents'] (dev mode: True)\n",
      "Using default EHR config: /home/logic/miniforge3/envs/pyhealth/lib/python3.12/site-packages/pyhealth/datasets/configs/mimic4_ehr.yaml\n",
      "Memory usage Before initializing mimic4_ehr: 1280.3 MB\n",
      "Duplicate table names in tables list. Removing duplicates.\n",
      "Initializing mimic4_ehr dataset from /home/logic/Github/mimic4 (dev mode: False)\n",
      "Scanning table: procedures_icd from /home/logic/Github/mimic4/hosp/procedures_icd.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: prescriptions from /home/logic/Github/mimic4/hosp/prescriptions.csv.gz\n",
      "Initializing MIMIC4EHRDataset with tables: ['patients', 'admissions', 'diagnoses_icd', 'procedures_icd', 'prescriptions', 'labevents'] (dev mode: True)\n",
      "Using default EHR config: /home/logic/miniforge3/envs/pyhealth/lib/python3.12/site-packages/pyhealth/datasets/configs/mimic4_ehr.yaml\n",
      "Memory usage Before initializing mimic4_ehr: 1280.3 MB\n",
      "Duplicate table names in tables list. Removing duplicates.\n",
      "Initializing mimic4_ehr dataset from /home/logic/Github/mimic4 (dev mode: False)\n",
      "Scanning table: procedures_icd from /home/logic/Github/mimic4/hosp/procedures_icd.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: prescriptions from /home/logic/Github/mimic4/hosp/prescriptions.csv.gz\n",
      "Scanning table: admissions from /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: diagnoses_icd from /home/logic/Github/mimic4/hosp/diagnoses_icd.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: icustays from /home/logic/Github/mimic4/icu/icustays.csv.gz\n",
      "Scanning table: labevents from /home/logic/Github/mimic4/hosp/labevents.csv.gz\n",
      "Scanning table: admissions from /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: diagnoses_icd from /home/logic/Github/mimic4/hosp/diagnoses_icd.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/admissions.csv.gz\n",
      "Scanning table: icustays from /home/logic/Github/mimic4/icu/icustays.csv.gz\n",
      "Scanning table: labevents from /home/logic/Github/mimic4/hosp/labevents.csv.gz\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/d_labitems.csv.gz\n",
      "Scanning table: patients from /home/logic/Github/mimic4/hosp/patients.csv.gz\n",
      "Memory usage After initializing mimic4_ehr: 1280.3 MB\n",
      "Memory usage After EHR dataset initialization: 1280.3 MB\n",
      "Memory usage Before combining data: 1280.3 MB\n",
      "Combining data from ehr dataset\n",
      "Creating combined dataframe\n",
      "Memory usage After combining data: 1280.3 MB\n",
      "Memory usage Completed MIMIC4Dataset init: 1280.3 MB\n",
      "Using MIMIC-IV root: /home/logic/Github/mimic4\n",
      "Requested tables: ['patients', 'admissions', 'diagnoses_icd', 'procedures_icd', 'prescriptions', 'labevents']\n",
      "EHR tables actually loaded: ['admissions', 'diagnoses_icd', 'icustays', 'labevents', 'patients', 'prescriptions', 'procedures_icd']\n",
      "Collecting global event dataframe...\n",
      "Dev mode enabled: limiting to 1000 patients\n",
      "Joining with table: /home/logic/Github/mimic4/hosp/d_labitems.csv.gz\n",
      "Scanning table: patients from /home/logic/Github/mimic4/hosp/patients.csv.gz\n",
      "Memory usage After initializing mimic4_ehr: 1280.3 MB\n",
      "Memory usage After EHR dataset initialization: 1280.3 MB\n",
      "Memory usage Before combining data: 1280.3 MB\n",
      "Combining data from ehr dataset\n",
      "Creating combined dataframe\n",
      "Memory usage After combining data: 1280.3 MB\n",
      "Memory usage Completed MIMIC4Dataset init: 1280.3 MB\n",
      "Using MIMIC-IV root: /home/logic/Github/mimic4\n",
      "Requested tables: ['patients', 'admissions', 'diagnoses_icd', 'procedures_icd', 'prescriptions', 'labevents']\n",
      "EHR tables actually loaded: ['admissions', 'diagnoses_icd', 'icustays', 'labevents', 'patients', 'prescriptions', 'procedures_icd']\n",
      "Collecting global event dataframe...\n",
      "Dev mode enabled: limiting to 1000 patients\n",
      "Collected dataframe with shape: (131557, 47)\n",
      "Found 100 unique patient IDs\n",
      "Unique patients discovered: 100\n",
      "Collected dataframe with shape: (131557, 47)\n",
      "Found 100 unique patient IDs\n",
      "Unique patients discovered: 100\n"
     ]
    }
   ],
   "source": [
    "EHR_ROOT = os.environ.get(\"PYHEALTH_MIMIC4_EHR_ROOT\", \"/home/logic/Github/mimic4\")\n",
    "EHR_TABLES = [\n",
    "    \"patients\",\n",
    "    \"admissions\",\n",
    "    \"diagnoses_icd\",\n",
    "    \"procedures_icd\",\n",
    "    \"prescriptions\",\n",
    "    \"labevents\",\n",
    "]\n",
    "\n",
    "dataset = MIMIC4Dataset(\n",
    "    ehr_root=EHR_ROOT,\n",
    "    ehr_tables=EHR_TABLES,\n",
    "    dev=True,\n",
    " )\n",
    "\n",
    "print(f\"Using MIMIC-IV root: {EHR_ROOT}\")\n",
    "print(f\"Requested tables: {EHR_TABLES}\")\n",
    "print(f\"EHR tables actually loaded: {sorted(dataset.sub_datasets['ehr'].tables)}\")\n",
    "print(f\"Unique patients discovered: {len(dataset.unique_patient_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648825d7",
   "metadata": {},
   "source": [
    "# 3. Prepare PyHealth Dataset\n",
    "Instantiate a task that augments the mortality labels with a lightweight lab feature vector and split the resulting samples into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b83716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortalityWithPseudoLabs(MortalityPredictionMIMIC4):\n",
    "    \"\"\"Extend the default mortality task with a simple numeric lab surrogate.\"\"\"\n",
    "\n",
    "    input_schema = {\n",
    "        \"conditions\": \"sequence\",\n",
    "        \"procedures\": \"sequence\",\n",
    "        \"drugs\": \"sequence\",\n",
    "        \"labs\": \"tensor\",\n",
    "    }\n",
    "\n",
    "    def __call__(self, patient):\n",
    "        base_samples = super().__call__(patient)\n",
    "        enriched_samples = []\n",
    "        for sample in base_samples:\n",
    "            labs_vector = [\n",
    "                float(len(sample.get(\"conditions\", []))),\n",
    "                float(len(sample.get(\"procedures\", []))),\n",
    "                float(len(sample.get(\"drugs\", []))),\n",
    "            ]\n",
    "            enriched_sample = dict(sample)\n",
    "            enriched_sample[\"labs\"] = labs_vector\n",
    "            enriched_samples.append(enriched_sample)\n",
    "        return enriched_samples\n",
    "\n",
    "task = MortalityWithPseudoLabs()\n",
    "sample_dataset = dataset.set_task(task)\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "if len(sample_dataset) == 0:\n",
    "    raise RuntimeError(\"The task did not produce any samples. Disable dev mode or adjust table selections.\")\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(sample_dataset, [0.7, 0.1, 0.2], seed=SEED)\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ca0dd",
   "metadata": {},
   "source": [
    "# 4. Inspect Batch Structure\n",
    "Build PyHealth dataloaders and verify that the tensors emitted for `conditions` and `labs` have the expected shapes before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE) if len(val_ds) else None\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE) if len(test_ds) else None\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    raise RuntimeError(\"The training loader is empty. Increase the dataset size or adjust the task configuration.\")\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {list(first_batch.keys())}\")\n",
    "\n",
    "conditions_batch = first_batch[\"conditions\"]\n",
    "labs_batch = first_batch[\"labs\"]\n",
    "\n",
    "print(f\"conditions tensor shape: {conditions_batch.shape}\")\n",
    "print(f\"labs tensor shape: {labs_batch.shape}\")\n",
    "print(f\"mortality tensor shape: {first_batch['mortality'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e648fe",
   "metadata": {},
   "source": [
    "# 5. Instantiate CNN Model\n",
    "Create the PyHealth CNN with custom hyperparameters and inspect the parameter footprint prior to optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8864561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import CNN\n",
    "\n",
    "model = CNN(\n",
    "    dataset=train_ds,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    ).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6771c",
   "metadata": {},
   "source": [
    "# 6. Define Training Utilities\n",
    "Configure the optimiser, learning-rate scheduler, and helper functions for batching, evaluation, and metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def build_optimizer(model: torch.nn.Module) -> torch.optim.Optimizer:\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.Adam(params, lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def build_scheduler(optimizer: torch.optim.Optimizer) -> StepLR:\n",
    "    return StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "def run_batch(batch, device):\n",
    "    features = {key: value.to(device) for key, value in batch['x'].items()}\n",
    "    labels = batch['y'].to(device)\n",
    "    return features, labels\n",
    "\n",
    "def compute_loss(model, features, labels):\n",
    "    outputs = model(features)\n",
    "    loss = F.binary_cross_entropy_with_logits(outputs, labels.float())\n",
    "    return outputs, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56da21",
   "metadata": {},
   "source": [
    "# 7. Train the Model\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f62657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "optimizer = build_optimizer(model)\n",
    "scheduler = build_scheduler(optimizer)\n",
    "num_epochs = 10\n",
    "clip_norm = 5.0\n",
    "training_history = defaultdict(list)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    total_labels = []\n",
    "    total_preds = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        features, labels = run_batch(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = compute_loss(model, features, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * labels.size(0)\n",
    "        total_labels.append(labels.detach().cpu())\n",
    "        total_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "\n",
    "    scheduler.step()\n",
    "    labels_tensor = torch.cat(total_labels)\n",
    "    preds_tensor = torch.cat(total_preds)\n",
    "    train_loss = epoch_loss / len(labels_tensor)\n",
    "    train_auc = roc_auc_score(labels_tensor.numpy(), preds_tensor.numpy())\n",
    "\n",
    "    training_history['epoch'].append(epoch)\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['train_auc'].append(train_auc)\n",
    "    print(f\"Epoch {epoch:02d} | loss={train_loss:.4f} | auc={train_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659ad5b",
   "metadata": {},
   "source": [
    "# 8. Evaluate on Validation Split\n",
    "Switch to evaluation mode, collect predictions for the validation split, and compute AUROC and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb62f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_labels = []\n",
    "val_logits = []\n",
    "val_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        features, labels = run_batch(batch, device)\n",
    "        logits, loss = compute_loss(model, features, labels)\n",
    "        val_labels.append(labels.detach().cpu())\n",
    "        val_logits.append(torch.sigmoid(logits).detach().cpu())\n",
    "        val_loss_total += loss.item() * labels.size(0)\n",
    "\n",
    "val_labels = torch.cat(val_labels)\n",
    "val_probs = torch.cat(val_logits)\n",
    "val_loss = val_loss_total / len(val_labels)\n",
    "val_auc = roc_auc_score(val_labels.numpy(), val_probs.numpy())\n",
    "\n",
    "print(f\"Validation loss = {val_loss:.4f}\")\n",
    "print(f\"Validation AUROC = {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fc6d6",
   "metadata": {},
   "source": [
    "# 9. Visualise and Save Artifacts\n",
    "Plot the training curves, probability histogram, and optionally persist trained weights and metrics for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3510f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(training_history)\n",
    "display(history_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "history_df.plot(x='epoch', y=['train_loss', 'train_auc'], ax=ax)\n",
    "ax.set_title('Training Progress')\n",
    "ax.set_ylabel('Metric Value')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(val_probs.numpy(), bins=20, alpha=0.7)\n",
    "ax.set_title('Validation Probability Distribution')\n",
    "ax.set_xlabel('Positive Class Probability')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "artifact_dir = Path('artifacts/cnn_mimic4_demo')\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), artifact_dir / 'cnn_mimic4_state_dict.pt')\n",
    "history_df.to_csv(artifact_dir / 'training_history.csv', index=False)\n",
    "with open(artifact_dir / 'validation_metrics.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump({'val_loss': float(val_loss), 'val_auc': float(val_auc)}, fp, indent=2)\n",
    "\n",
    "print(f\"Artifacts saved to {artifact_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
