{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1ec419",
   "metadata": {},
   "source": [
    "# 1. Environment Setup\n",
    "Use this section to configure deterministic behaviour and import the libraries required for the rest of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb256423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "from pyhealth.datasets import MIMIC4Dataset\n",
    "from pyhealth.datasets.splitter import split_by_patient\n",
    "from pyhealth.datasets.utils import get_dataloader\n",
    "from pyhealth.tasks.mortality_prediction import MortalityPredictionMIMIC4\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23693d12",
   "metadata": {},
   "source": [
    "# 2. Load MIMIC-IV Sample Extract\n",
    "Point to the preprocessed MIMIC-IV tables, optionally override individual files, and preview their structure before building a task dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e61694c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Set PYHEALTH_MIMIC4_EHR_ROOT to a valid directory. Currently: /path/to/mimic-iv-ehr",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m ehr_root_path = Path(EHR_ROOT).expanduser()\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ehr_root_path.exists():\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m      6\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSet PYHEALTH_MIMIC4_EHR_ROOT to a valid directory. Currently: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mehr_root_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m     )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mresolve_table_path\u001b[39m(env_key: \u001b[38;5;28mstr\u001b[39m, default_path: Path) -> Path:\n\u001b[32m     11\u001b[39m     override = os.environ.get(env_key)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Set PYHEALTH_MIMIC4_EHR_ROOT to a valid directory. Currently: /path/to/mimic-iv-ehr"
     ]
    }
   ],
   "source": [
    "# Resolve the location of the MIMIC-IV EHR extract.\n",
    "EHR_ROOT = os.environ.get(\"PYHEALTH_MIMIC4_EHR_ROOT\", \"/path/to/mimic-iv-ehr\")\n",
    "ehr_root_path = Path(EHR_ROOT).expanduser()\n",
    "if not ehr_root_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Set PYHEALTH_MIMIC4_EHR_ROOT to a valid directory. Currently: {ehr_root_path}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def resolve_table_path(env_key: str, default_path: Path) -> Path:\n",
    "    override = os.environ.get(env_key)\n",
    "    if override:\n",
    "        override_path = Path(override).expanduser()\n",
    "        if override_path.exists():\n",
    "            return override_path\n",
    "    if default_path.exists():\n",
    "        return default_path\n",
    "    gz_path = default_path.with_suffix(default_path.suffix + \".gz\")\n",
    "    return gz_path if gz_path.exists() else default_path\n",
    "\n",
    "\n",
    "patients_path = resolve_table_path(\"PYHEALTH_MIMIC4_PATIENTS\", ehr_root_path / \"patients.csv\")\n",
    "labs_path = resolve_table_path(\"PYHEALTH_MIMIC4_LABEVENTS\", ehr_root_path / \"labevents.csv\")\n",
    "\n",
    "table_paths = {\"patients\": patients_path, \"labevents\": labs_path}\n",
    "for name, path in table_paths.items():\n",
    "    status = \"OK\" if path.exists() else \"MISSING\"\n",
    "    print(f\"[{status}] {name} table path: {path}\")\n",
    "\n",
    "for name, path in table_paths.items():\n",
    "    if not path.exists():\n",
    "        continue\n",
    "    try:\n",
    "        preview = pd.read_csv(path, nrows=5)\n",
    "        print(f\"\\n{name} preview:\")\n",
    "        display(preview)\n",
    "    except Exception as exc:\n",
    "        print(f\"Could not preview {name} ({exc}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648825d7",
   "metadata": {},
   "source": [
    "# 3. Prepare PyHealth Dataset\n",
    "Instantiate a task that augments the mortality labels with a lightweight lab feature vector and split the resulting samples into training, validation, and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b83716",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MortalityWithPseudoLabs(MortalityPredictionMIMIC4):\n",
    "    \"\"\"Extend the default mortality task with a simple numeric lab surrogate.\"\"\"\n",
    "\n",
    "    input_schema = {\n",
    "        \"conditions\": \"sequence\",\n",
    "        \"procedures\": \"sequence\",\n",
    "        \"drugs\": \"sequence\",\n",
    "        \"labs\": \"tensor\",\n",
    "    }\n",
    "\n",
    "    def __call__(self, patient):\n",
    "        base_samples = super().__call__(patient)\n",
    "        enriched_samples = []\n",
    "        for sample in base_samples:\n",
    "            labs_vector = [\n",
    "                float(len(sample.get(\"conditions\", []))),\n",
    "                float(len(sample.get(\"procedures\", []))),\n",
    "                float(len(sample.get(\"drugs\", []))),\n",
    "            ]\n",
    "            enriched_sample = dict(sample)\n",
    "            enriched_sample[\"labs\"] = labs_vector\n",
    "            enriched_samples.append(enriched_sample)\n",
    "        return enriched_samples\n",
    "\n",
    "\n",
    "dataset = MIMIC4Dataset(\n",
    "    ehr_root=str(ehr_root_path),\n",
    "    ehr_tables=[\n",
    "        \"patients\",\n",
    "        \"admissions\",\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"prescriptions\",\n",
    "    ],\n",
    "    dev=True,\n",
    "\n",
    ")\n",
    "\n",
    "task = MortalityWithPseudoLabs()\n",
    "sample_dataset = dataset.set_task(task)\n",
    "print(f\"Total task samples: {len(sample_dataset)}\")\n",
    "print(f\"Input schema: {sample_dataset.input_schema}\")\n",
    "print(f\"Output schema: {sample_dataset.output_schema}\")\n",
    "\n",
    "if len(sample_dataset) == 0:\n",
    "    raise RuntimeError(\"The task did not produce any samples. Disable dev mode or adjust table selections.\")\n",
    "\n",
    "train_ds, val_ds, test_ds = split_by_patient(sample_dataset, [0.7, 0.1, 0.2], seed=SEED)\n",
    "print(f\"Train/Val/Test sizes: {len(train_ds)}, {len(val_ds)}, {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5ca0dd",
   "metadata": {},
   "source": [
    "# 4. Inspect Batch Structure\n",
    "Build PyHealth dataloaders and verify that the tensors emitted for `conditions` and `labs` have the expected shapes before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = get_dataloader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = get_dataloader(val_ds, batch_size=BATCH_SIZE) if len(val_ds) else None\n",
    "test_loader = get_dataloader(test_ds, batch_size=BATCH_SIZE) if len(test_ds) else None\n",
    "\n",
    "if len(train_loader) == 0:\n",
    "    raise RuntimeError(\"The training loader is empty. Increase the dataset size or adjust the task configuration.\")\n",
    "\n",
    "first_batch = next(iter(train_loader))\n",
    "print(f\"Batch keys: {list(first_batch.keys())}\")\n",
    "\n",
    "conditions_batch = first_batch[\"conditions\"]\n",
    "labs_batch = first_batch[\"labs\"]\n",
    "\n",
    "print(f\"conditions tensor shape: {conditions_batch.shape}\")\n",
    "print(f\"labs tensor shape: {labs_batch.shape}\")\n",
    "print(f\"mortality tensor shape: {first_batch['mortality'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e648fe",
   "metadata": {},
   "source": [
    "# 5. Instantiate CNN Model\n",
    "Create the PyHealth CNN with custom hyperparameters and inspect the parameter footprint prior to optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8864561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import CNN\n",
    "\n",
    "model = CNN(\n",
    "    dataset=train_ds,\n",
    "    embedding_dim=64,\n",
    "    hidden_dim=64,\n",
    "    num_layers=2,\n",
    "    ).to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Feature keys: {model.feature_keys}\")\n",
    "print(f\"Label key: {model.label_key}\")\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef6771c",
   "metadata": {},
   "source": [
    "# 6. Define Training Utilities\n",
    "Configure the optimiser, learning-rate scheduler, and helper functions for batching, evaluation, and metric tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def build_optimizer(model: torch.nn.Module) -> torch.optim.Optimizer:\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    return torch.optim.Adam(params, lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "def build_scheduler(optimizer: torch.optim.Optimizer) -> StepLR:\n",
    "    return StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "def run_batch(batch, device):\n",
    "    features = {key: value.to(device) for key, value in batch['x'].items()}\n",
    "    labels = batch['y'].to(device)\n",
    "    return features, labels\n",
    "\n",
    "def compute_loss(model, features, labels):\n",
    "    outputs = model(features)\n",
    "    loss = F.binary_cross_entropy_with_logits(outputs, labels.float())\n",
    "    return outputs, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56da21",
   "metadata": {},
   "source": [
    "# 7. Train the Model\n",
    "Run multiple epochs with gradient clipping, scheduler updates, and logging of loss/metrics per epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f62657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "torch.manual_seed(42)\n",
    "optimizer = build_optimizer(model)\n",
    "scheduler = build_scheduler(optimizer)\n",
    "num_epochs = 10\n",
    "clip_norm = 5.0\n",
    "training_history = defaultdict(list)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    total_labels = []\n",
    "    total_preds = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        features, labels = run_batch(batch, device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = compute_loss(model, features, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item() * labels.size(0)\n",
    "        total_labels.append(labels.detach().cpu())\n",
    "        total_preds.append(torch.sigmoid(logits).detach().cpu())\n",
    "\n",
    "    scheduler.step()\n",
    "    labels_tensor = torch.cat(total_labels)\n",
    "    preds_tensor = torch.cat(total_preds)\n",
    "    train_loss = epoch_loss / len(labels_tensor)\n",
    "    train_auc = roc_auc_score(labels_tensor.numpy(), preds_tensor.numpy())\n",
    "\n",
    "    training_history['epoch'].append(epoch)\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['train_auc'].append(train_auc)\n",
    "    print(f\"Epoch {epoch:02d} | loss={train_loss:.4f} | auc={train_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a659ad5b",
   "metadata": {},
   "source": [
    "# 8. Evaluate on Validation Split\n",
    "Switch to evaluation mode, collect predictions for the validation split, and compute AUROC and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb62f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_labels = []\n",
    "val_logits = []\n",
    "val_loss_total = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        features, labels = run_batch(batch, device)\n",
    "        logits, loss = compute_loss(model, features, labels)\n",
    "        val_labels.append(labels.detach().cpu())\n",
    "        val_logits.append(torch.sigmoid(logits).detach().cpu())\n",
    "        val_loss_total += loss.item() * labels.size(0)\n",
    "\n",
    "val_labels = torch.cat(val_labels)\n",
    "val_probs = torch.cat(val_logits)\n",
    "val_loss = val_loss_total / len(val_labels)\n",
    "val_auc = roc_auc_score(val_labels.numpy(), val_probs.numpy())\n",
    "\n",
    "print(f\"Validation loss = {val_loss:.4f}\")\n",
    "print(f\"Validation AUROC = {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fc6d6",
   "metadata": {},
   "source": [
    "# 9. Visualise and Save Artifacts\n",
    "Plot the training curves, probability histogram, and optionally persist trained weights and metrics for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3510f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "history_df = pd.DataFrame(training_history)\n",
    "display(history_df)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "history_df.plot(x='epoch', y=['train_loss', 'train_auc'], ax=ax)\n",
    "ax.set_title('Training Progress')\n",
    "ax.set_ylabel('Metric Value')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.hist(val_probs.numpy(), bins=20, alpha=0.7)\n",
    "ax.set_title('Validation Probability Distribution')\n",
    "ax.set_xlabel('Positive Class Probability')\n",
    "ax.set_ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "artifact_dir = Path('artifacts/cnn_mimic4_demo')\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(model.state_dict(), artifact_dir / 'cnn_mimic4_state_dict.pt')\n",
    "history_df.to_csv(artifact_dir / 'training_history.csv', index=False)\n",
    "with open(artifact_dir / 'validation_metrics.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump({'val_loss': float(val_loss), 'val_auc': float(val_auc)}, fp, indent=2)\n",
    "\n",
    "print(f\"Artifacts saved to {artifact_dir.resolve()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
