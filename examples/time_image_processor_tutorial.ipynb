{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeImageProcessor Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the `TimeImageProcessor` for multimodal PyHealth pipelines.\n",
    "\n",
    "**Contributors:** Josh Steier\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `TimeImageProcessor` is a time-aware image processor that pairs image loading with temporal metadata. It is designed for tasks where each patient has **multiple images taken at different times** (e.g., serial chest X-rays during an ICU stay).\n",
    "\n",
    "**Input:** `(List[image_path], List[time_diff_from_first_admission])`\n",
    "\n",
    "**Output:** `(N×C×H×W image tensor, N timestamp tensor, \"image\")`\n",
    "\n",
    "### Steps\n",
    "1. Create synthetic time-stamped chest X-ray data\n",
    "2. Standalone processor usage and verification\n",
    "3. Processor with normalization and truncation\n",
    "4. Integration with `create_sample_dataset`\n",
    "5. Verify multimodal compatibility for downstream fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from pyhealth.datasets import create_sample_dataset\n",
    "from pyhealth.processors.time_image_processor import TimeImageProcessor\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Time-Stamped X-ray Data\n",
    "\n",
    "We simulate a scenario where each patient has 1–5 chest X-rays taken at different times during their hospital stay. Each image gets a timestamp representing days from the patient's first admission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = tempfile.mkdtemp(prefix=\"time_image_example_\")\n",
    "images_dir = os.path.join(DATA_ROOT, \"images\")\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "NUM_PATIENTS = 20\n",
    "MAX_IMAGES_PER_PATIENT = 5\n",
    "\n",
    "samples = []\n",
    "for pid in range(NUM_PATIENTS):\n",
    "    # Each patient has 1-5 X-rays taken at different times\n",
    "    n_images = np.random.randint(1, MAX_IMAGES_PER_PATIENT + 1)\n",
    "\n",
    "    # Time differences from first admission in days\n",
    "    time_diffs = sorted(np.random.uniform(0, 30, size=n_images).tolist())\n",
    "    time_diffs[0] = 0.0  # First image always at t=0\n",
    "\n",
    "    image_paths = []\n",
    "    for j in range(n_images):\n",
    "        # Synthetic grayscale X-ray with noise\n",
    "        img_array = np.random.normal(80, 25, (224, 224))\n",
    "\n",
    "        # Add lung-shaped regions\n",
    "        y, x = np.ogrid[:224, :224]\n",
    "        left_mask = ((x - 72)**2 / 3000 + (y - 112)**2 / 8000) < 1\n",
    "        right_mask = ((x - 152)**2 / 3000 + (y - 112)**2 / 8000) < 1\n",
    "        img_array[left_mask] -= 20\n",
    "        img_array[right_mask] -= 20\n",
    "\n",
    "        img_array = np.clip(img_array, 0, 255).astype(np.uint8)\n",
    "        img = Image.fromarray(img_array, mode=\"L\")\n",
    "\n",
    "        img_path = os.path.join(images_dir, f\"p{pid:03d}_t{j:02d}.png\")\n",
    "        img.save(img_path)\n",
    "        image_paths.append(img_path)\n",
    "\n",
    "    # Binary mortality label\n",
    "    label = pid % 2\n",
    "\n",
    "    samples.append({\n",
    "        \"patient_id\": f\"p{pid}\",\n",
    "        \"visit_id\": f\"v{pid}\",\n",
    "        \"chest_xray\": (image_paths, time_diffs),\n",
    "        \"label\": label,\n",
    "    })\n",
    "\n",
    "print(f\"Created {NUM_PATIENTS} patients in {DATA_ROOT}\")\n",
    "print(f\"Images per patient: 1-{MAX_IMAGES_PER_PATIENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a sample patient\n",
    "sample = samples[0]\n",
    "paths, times = sample[\"chest_xray\"]\n",
    "\n",
    "print(f\"Patient {sample['patient_id']}:\")\n",
    "print(f\"  Number of images: {len(paths)}\")\n",
    "print(f\"  Times (days from admission): {[round(t, 1) for t in times]}\")\n",
    "print(f\"  Mortality label: {sample['label']}\")\n",
    "print(f\"  Image paths:\")\n",
    "for p, t in zip(paths, times):\n",
    "    print(f\"    t={t:5.1f}d  {os.path.basename(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Standalone Processor Usage\n",
    "\n",
    "The `TimeImageProcessor` takes a tuple `(image_paths, time_diffs)` and returns `(images_tensor, timestamps_tensor, \"image\")`.\n",
    "\n",
    "Key behaviors:\n",
    "- **Sorts images chronologically** by timestamp\n",
    "- **Truncates** to `max_images` most recent if set\n",
    "- Returns the `\"image\"` tag for modality routing in the multimodal embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc = TimeImageProcessor(\n",
    "    image_size=224,\n",
    "    mode=\"L\",\n",
    ")\n",
    "\n",
    "images, timestamps, tag = proc.process(sample[\"chest_xray\"])\n",
    "\n",
    "print(f\"Input: ({len(paths)} paths, {len(times)} timestamps)\")\n",
    "print(f\"\")\n",
    "print(f\"Output:\")\n",
    "print(f\"  images shape:     {images.shape}  # (N, C, H, W)\")\n",
    "print(f\"  timestamps shape: {timestamps.shape}  # (N,)\")\n",
    "print(f\"  modality tag:     {tag!r}\")\n",
    "print(f\"\")\n",
    "print(f\"  images dtype:     {images.dtype}\")\n",
    "print(f\"  timestamps dtype: {timestamps.dtype}\")\n",
    "print(f\"  pixel range:      [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"  timestamps:       {timestamps.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chronological Sorting Verification\n",
    "\n",
    "Even if image paths are provided in random order, the processor always returns them sorted by timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide images in reverse order\n",
    "reversed_paths = list(reversed(paths))\n",
    "reversed_times = list(reversed(times))\n",
    "\n",
    "print(f\"Input order (reversed):\")\n",
    "for p, t in zip(reversed_paths, reversed_times):\n",
    "    print(f\"  t={t:5.1f}d  {os.path.basename(p)}\")\n",
    "\n",
    "_, sorted_timestamps, _ = proc.process((reversed_paths, reversed_times))\n",
    "\n",
    "print(f\"\\nOutput timestamps (sorted): {sorted_timestamps.tolist()}\")\n",
    "print(f\"Correctly sorted: {all(sorted_timestamps[i] <= sorted_timestamps[i+1] for i in range(len(sorted_timestamps)-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Truncation with `max_images`\n",
    "\n",
    "When `max_images` is set, the processor keeps only the **most recent** images (by timestamp). This is useful for patients with many X-rays where you want to cap compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_truncated = TimeImageProcessor(\n",
    "    image_size=224,\n",
    "    mode=\"L\",\n",
    "    max_images=2,\n",
    ")\n",
    "\n",
    "imgs_trunc, ts_trunc, _ = proc_truncated.process(sample[\"chest_xray\"])\n",
    "\n",
    "print(f\"Original images: {len(paths)}\")\n",
    "print(f\"max_images: 2\")\n",
    "print(f\"Output images: {imgs_trunc.shape[0]}\")\n",
    "print(f\"Kept timestamps: {ts_trunc.tolist()}\")\n",
    "print(f\"(These are the 2 most recent observations)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Normalization\n",
    "\n",
    "ImageNet-style normalization can be applied for pretrained backbone compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_norm = TimeImageProcessor(\n",
    "    image_size=128,\n",
    "    mode=\"L\",\n",
    "    normalize=True,\n",
    "    mean=[0.5],\n",
    "    std=[0.5],\n",
    ")\n",
    "\n",
    "imgs_norm, ts_norm, _ = proc_norm.process(sample[\"chest_xray\"])\n",
    "\n",
    "print(f\"Without normalization:\")\n",
    "print(f\"  pixel range: [{images.min():.3f}, {images.max():.3f}]\")\n",
    "print(f\"\")\n",
    "print(f\"With normalization (mean=0.5, std=0.5):\")\n",
    "print(f\"  pixel range: [{imgs_norm.min():.3f}, {imgs_norm.max():.3f}]\")\n",
    "print(f\"  output shape: {imgs_norm.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integration with `create_sample_dataset`\n",
    "\n",
    "The processor is registered as `\"time_image\"` in PyHealth's processor registry, so it can be used in task schemas. Here we show it working with `create_sample_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_sample_dataset(\n",
    "    samples=samples,\n",
    "    input_schema={\n",
    "        \"chest_xray\": \"time_image\",\n",
    "    },\n",
    "    output_schema={\n",
    "        \"label\": \"binary\",\n",
    "    },\n",
    "    input_processors={\n",
    "        \"chest_xray\": TimeImageProcessor(\n",
    "            image_size=224,\n",
    "            mode=\"L\",\n",
    "            max_images=4,\n",
    "        ),\n",
    "    },\n",
    "    dataset_name=\"time_xray_example\",\n",
    ")\n",
    "\n",
    "print(f\"Dataset: {dataset}\")\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "print(f\"Input schema:  {dataset.input_schema}\")\n",
    "print(f\"Output schema: {dataset.output_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a processed sample\n",
    "processed = dataset[0]\n",
    "print(f\"Processed sample keys: {list(processed.keys())}\")\n",
    "print()\n",
    "\n",
    "xray_data = processed[\"chest_xray\"]\n",
    "if isinstance(xray_data, tuple):\n",
    "    img_tensor, ts_tensor, modality_tag = xray_data\n",
    "    print(f\"chest_xray output:\")\n",
    "    print(f\"  images shape:     {img_tensor.shape}  # (N, C, H, W)\")\n",
    "    print(f\"  timestamps shape: {ts_tensor.shape}  # (N,)\")\n",
    "    print(f\"  modality tag:     {modality_tag!r}\")\n",
    "else:\n",
    "    print(f\"  type: {type(xray_data)}\")\n",
    "\n",
    "print(f\"\\nlabel: {processed['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Patient Output Shape Summary\n",
    "\n",
    "Since patients have different numbers of X-rays, the output tensor shapes vary per patient. This is expected — the multimodal embedding model handles variable-length inputs via masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_demo = TimeImageProcessor(image_size=224, mode=\"L\", max_images=4)\n",
    "\n",
    "print(f\"{'Patient':<10} {'N imgs':<8} {'Output Shape':<25} {'Time Range (days)'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i in range(min(10, len(samples))):\n",
    "    s = samples[i]\n",
    "    paths_i, times_i = s[\"chest_xray\"]\n",
    "    imgs_i, ts_i, _ = proc_demo.process((paths_i, times_i))\n",
    "    print(\n",
    "        f\"{s['patient_id']:<10} \"\n",
    "        f\"{len(paths_i):<8} \"\n",
    "        f\"{str(tuple(imgs_i.shape)):<25} \"\n",
    "        f\"[{ts_i[0]:.1f}, {ts_i[-1]:.1f}]\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multimodal Compatibility\n",
    "\n",
    "The `TimeImageProcessor` output format is designed to feed directly into the **unified multimodal embedding model**:\n",
    "\n",
    "```\n",
    "TimeImageProcessor\n",
    "    ↓\n",
    "(N, C, H, W) images + (N,) timestamps + \"image\" tag\n",
    "    ↓\n",
    "VisionEncoder(images) → (B, P, E') patch embeddings\n",
    "TimeEmbedding(timestamps) → temporal encoding\n",
    "ModalityEmbedding(\"image\") → modality type encoding\n",
    "    ↓\n",
    "Combined: (B, P, E') vision tokens\n",
    "    ↓\n",
    "Concatenate with other modalities:\n",
    "    TextEncoder    → (B, T, E') text tokens\n",
    "    TimeseriesProc → (B, S, E') timeseries tokens\n",
    "    ↓\n",
    "(B, P+T+S, E') → BottleneckTransformer\n",
    "```\n",
    "\n",
    "This matches the architecture specified in the multimodal design doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated multimodal input shapes for one patient\n",
    "E_prime = 128  # shared embedding dimension\n",
    "\n",
    "# Vision: TimeImageProcessor -> VisionEncoder\n",
    "P = 49  # patches from CNN/ResNet backbone\n",
    "vision_tokens = torch.randn(1, P, E_prime)\n",
    "\n",
    "# Text: TextProcessor -> TextEncoder (Medical RoBERTa)\n",
    "T = 64  # 128-token chunks\n",
    "text_tokens = torch.randn(1, T, E_prime)\n",
    "\n",
    "# Timeseries: TimeseriesProcessor -> TimeseriesEncoder\n",
    "S = 48  # hourly lab values over 2 days\n",
    "ts_tokens = torch.randn(1, S, E_prime)\n",
    "\n",
    "# Concatenate for transformer fusion\n",
    "combined = torch.cat([vision_tokens, text_tokens, ts_tokens], dim=1)\n",
    "\n",
    "print(f\"Vision tokens:     {tuple(vision_tokens.shape)}  # P={P} patches\")\n",
    "print(f\"Text tokens:       {tuple(text_tokens.shape)}  # T={T} tokens\")\n",
    "print(f\"Timeseries tokens: {tuple(ts_tokens.shape)}  # S={S} steps\")\n",
    "print(f\"\")\n",
    "print(f\"Combined sequence: {tuple(combined.shape)}  # P+T+S={P+T+S} tokens\")\n",
    "print(f\"Ready for BottleneckTransformer input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(DATA_ROOT)\n",
    "print(f\"Cleaned up: {DATA_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### TimeImageProcessor\n",
    "\n",
    "| Feature | Details |\n",
    "|---|---|\n",
    "| **Registry name** | `\"time_image\"` |\n",
    "| **Input** | `(List[image_path], List[time_diff])` |\n",
    "| **Output** | `(N×C×H×W tensor, N tensor, \"image\")` |\n",
    "| **Sorting** | Chronological by timestamp |\n",
    "| **Truncation** | `max_images` keeps most recent |\n",
    "| **Normalization** | Optional ImageNet-style |\n",
    "| **Mode** | RGB, L (grayscale), RGBA |\n",
    "\n",
    "### Usage in task schema\n",
    "\n",
    "```python\n",
    "input_schema = {\n",
    "    \"chest_xray\": (\"time_image\", {\n",
    "        \"image_size\": 224,\n",
    "        \"mode\": \"RGB\",\n",
    "        \"normalize\": True,\n",
    "        \"mean\": [0.485, 0.456, 0.406],\n",
    "        \"std\": [0.229, 0.224, 0.225],\n",
    "        \"max_images\": 8,\n",
    "    }),\n",
    "}\n",
    "```\n",
    "\n",
    "### Downstream pipeline\n",
    "\n",
    "```\n",
    "TimeImageProcessor → VisionEmbeddingModel → MultimodalEmbeddingModel → BottleneckTransformer\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
