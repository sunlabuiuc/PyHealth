{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7e5b21",
   "metadata": {},
   "source": [
    "# Medical Image Classification with PyHealth\n",
    "\n",
    "Welcome to the PyHealth tutorial on image classification and saliency mapping. In this notebook, we will explore how to use PyHealth to analyze chest X-ray images, classify them into various chest diseases, and visualize the model's decision-making process using gradient saliency maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519fe4c",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e39fafe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (2.10.0)\n",
      "Requirement already satisfied: torchvision in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (0.25.0)\n",
      "Requirement already satisfied: transformers in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (4.57.6)\n",
      "Requirement already satisfied: peft in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (0.18.1)\n",
      "Requirement already satisfied: accelerate in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: rdkit in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (2025.9.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: networkx in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (3.6.1)\n",
      "Requirement already satisfied: mne in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (1.11.0)\n",
      "Requirement already satisfied: tqdm in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: polars in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (1.37.1)\n",
      "Requirement already satisfied: pandas in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: pydantic in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (2.12.5)\n",
      "Requirement already satisfied: litdata in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (0.2.60)\n",
      "Requirement already satisfied: pyarrow in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (23.0.0)\n",
      "Requirement already satisfied: narwhals in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (2.15.0)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting einops\n",
      "  Using cached einops-0.8.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting linear-attention-transformer\n",
      "  Using cached linear_attention_transformer-0.19.1-py3-none-any.whl.metadata (787 bytes)\n",
      "Requirement already satisfied: dask[complete] in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (2026.1.1)\n",
      "Requirement already satisfied: filelock in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (80.10.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (2026.1.0)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch) (1.3.3)\n",
      "Requirement already satisfied: numpy in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torchvision) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from torchvision) (12.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from peft) (7.2.1)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from scikit-learn) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: decorator in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: lazy-loader>=0.3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from mne) (0.4)\n",
      "Requirement already satisfied: matplotlib>=3.8 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from mne) (3.10.8)\n",
      "Requirement already satisfied: pooch>=1.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from mne) (1.8.2)\n",
      "Requirement already satisfied: polars-runtime-32==1.37.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from polars) (1.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from pydantic) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from pydantic) (0.4.2)\n",
      "Requirement already satisfied: click>=8.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from dask[complete]) (8.3.1)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from dask[complete]) (3.1.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from dask[complete]) (1.4.2)\n",
      "Requirement already satisfied: toolz>=0.12.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from dask[complete]) (1.1.0)\n",
      "Collecting lz4>=4.3.2 (from dask[complete])\n",
      "  Using cached lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: lightning-utilities in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from litdata) (0.15.2)\n",
      "Requirement already satisfied: boto3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from litdata) (1.42.38)\n",
      "Requirement already satisfied: tifffile in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from litdata) (2026.1.28)\n",
      "Requirement already satisfied: obstore in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from litdata) (0.8.2)\n",
      "Collecting axial-positional-embedding (from linear-attention-transformer)\n",
      "  Using cached axial_positional_embedding-0.3.12-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting linformer>=0.1.0 (from linear-attention-transformer)\n",
      "  Using cached linformer-0.2.3-py3-none-any.whl.metadata (602 bytes)\n",
      "Collecting local-attention (from linear-attention-transformer)\n",
      "  Using cached local_attention-1.11.2-py3-none-any.whl.metadata (929 bytes)\n",
      "Collecting product-key-memory>=0.1.5 (from linear-attention-transformer)\n",
      "  Using cached product_key_memory-0.3.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from matplotlib>=3.8->mne) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from matplotlib>=3.8->mne) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from matplotlib>=3.8->mne) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from matplotlib>=3.8->mne) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from matplotlib>=3.8->mne) (3.3.2)\n",
      "Requirement already satisfied: locket in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from pooch>=1.5->mne) (4.5.1)\n",
      "Collecting colt5-attention>=0.10.14 (from product-key-memory>=0.1.5->linear-attention-transformer)\n",
      "  Using cached CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from requests->transformers) (2026.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: botocore<1.43.0,>=1.42.38 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from boto3->litdata) (1.42.38)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from boto3->litdata) (1.1.0)\n",
      "Requirement already satisfied: s3transfer<0.17.0,>=0.16.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from boto3->litdata) (0.16.0)\n",
      "Requirement already satisfied: distributed<2026.1.2,>=2026.1.1 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from dask[complete]) (2026.1.1)\n",
      "Collecting bokeh>=3.1.0 (from dask[complete])\n",
      "  Using cached bokeh-3.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Collecting hyper-connections>=0.1.8 (from local-attention->linear-attention-transformer)\n",
      "  Using cached hyper_connections-0.4.7-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from bokeh>=3.1.0->dask[complete]) (6.5.4)\n",
      "Collecting xyzservices>=2021.09.1 (from bokeh>=3.1.0->dask[complete])\n",
      "  Using cached xyzservices-2025.11.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from distributed<2026.1.2,>=2026.1.1->dask[complete]) (1.1.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from distributed<2026.1.2,>=2026.1.1->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib!=3.2.0,!=3.2.1,>=1.6.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from distributed<2026.1.2,>=2026.1.1->dask[complete]) (3.2.2)\n",
      "Requirement already satisfied: zict>=3.0.0 in /opt/workspace/PyHealth-fitzpa15/venv/lib/python3.12/site-packages (from distributed<2026.1.2,>=2026.1.1->dask[complete]) (3.0.0)\n",
      "Using cached more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Using cached einops-0.8.2-py3-none-any.whl (65 kB)\n",
      "Using cached linear_attention_transformer-0.19.1-py3-none-any.whl (12 kB)\n",
      "Using cached linformer-0.2.3-py3-none-any.whl (6.2 kB)\n",
      "Using cached lz4-4.4.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "Using cached product_key_memory-0.3.0-py3-none-any.whl (8.3 kB)\n",
      "Using cached axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\n",
      "Using cached local_attention-1.11.2-py3-none-any.whl (9.5 kB)\n",
      "Using cached bokeh-3.8.2-py3-none-any.whl (7.2 MB)\n",
      "Using cached CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
      "Using cached hyper_connections-0.4.7-py3-none-any.whl (28 kB)\n",
      "Using cached xyzservices-2025.11.0-py3-none-any.whl (93 kB)\n",
      "Installing collected packages: xyzservices, more-itertools, lz4, einops, bokeh, linformer, hyper-connections, axial-positional-embedding, local-attention, colt5-attention, product-key-memory, linear-attention-transformer\n",
      "Successfully installed axial-positional-embedding-0.3.12 bokeh-3.8.2 colt5-attention-0.11.1 einops-0.8.2 hyper-connections-0.4.7 linear-attention-transformer-0.19.1 linformer-0.2.3 local-attention-1.11.2 lz4-4.4.5 more-itertools-10.8.0 product-key-memory-0.3.0 xyzservices-2025.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision transformers peft accelerate rdkit scikit-learn networkx mne tqdm polars pandas pydantic \"dask[complete]\" litdata pyarrow narwhals more-itertools einops linear-attention-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82593a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'PyHealth'...\n",
      "remote: Enumerating objects: 9107, done.\u001b[K\n",
      "remote: Counting objects: 100% (100/100), done.\u001b[K\n",
      "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
      "Receiving objects:  10% (936/9107), 6.05 MiB | 6.03 MiB/s\r"
     ]
    }
   ],
   "source": [
    "!rm -rf PyHealth\n",
    "# !git clone https://github.com/sunlabuiuc/PyHealth.git\n",
    "!git clone -b layer-relevance-propagation https://github.com/Nimanui/PyHealth-fitzpa15.git PyHealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd4b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"./PyHealth\")\n",
    "sys.path.append(\"./PyHealth-fitzpa15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67302afe",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "\n",
    "Next, we will download the dataset containing COVID-19 data. This dataset includes chest X-ray images of normal cases, lung opacity, viral pneumonia, and COVID-19 patients. You can find more information about the dataset [here](https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32539a",
   "metadata": {},
   "source": [
    "Download and extract the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N https://storage.googleapis.com/pyhealth/covid19_cxr_data/archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0a6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q -o archive.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d9cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -1 COVID-19_Radiography_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccb47d",
   "metadata": {},
   "source": [
    "Next, we will proceed with the chest X-ray classification task using PyHealth, following a five-stage pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ecc90",
   "metadata": {},
   "source": [
    "## Step 1. Load Data in PyHealth\n",
    "\n",
    "The initial step involves loading the data into PyHealth's internal structure. This process is straightforward: import the appropriate dataset class from PyHealth and specify the root directory where the raw dataset is stored. PyHealth will handle the dataset processing automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd5925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import COVID19CXRDataset\n",
    "\n",
    "root = \"COVID-19_Radiography_Dataset\"\n",
    "base_dataset = COVID19CXRDataset(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04133288",
   "metadata": {},
   "source": [
    "Once the data is loaded, we can perform simple queries on the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8889c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f244846",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.get_patient(\"0\").get_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7241e29a",
   "metadata": {},
   "source": [
    "## Step 2. Define the Task\n",
    "\n",
    "The next step is to define the machine learning task. This step instructs the package to generate a list of samples with the desired features and labels based on the data for each individual patient. Please note that in this dataset, patient identification information is not available. Therefore, we will assume that each chest X-ray belongs to a unique patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16514220",
   "metadata": {},
   "source": [
    "For this dataset, PyHealth offers a default task specifically for chest X-ray classification. This task takes the image as input and aims to predict the chest diseases associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9723ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dataset.default_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc161dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = base_dataset.set_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e56f9",
   "metadata": {},
   "source": [
    "Here is an example of a single sample, represented as a dictionary. The dictionary contains keys for feature names, label names, and other metadata associated with the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256248e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3fa92",
   "metadata": {},
   "source": [
    "We can also check the input and output schemas, which specify the data types of the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d814c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset.input_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fefc93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset.output_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b356f30",
   "metadata": {},
   "source": [
    "Below, we plot the number of samples per classes, and visualize some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d3e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = sample_dataset.output_processors[\"disease\"].label_vocab\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd51e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_counts = defaultdict(int)\n",
    "for sample in sample_dataset.samples:\n",
    "    label_counts[id2label[sample[\"disease\"].item()]] += 1\n",
    "print(label_counts)\n",
    "plt.bar(label_counts.keys(), label_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a26d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "label_to_idxs = defaultdict(list)\n",
    "for idx, sample in enumerate(sample_dataset.samples):\n",
    "    label_to_idxs[sample[\"disease\"].item()].append(idx)\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 3))\n",
    "for ax, label in zip(axs, label_to_idxs.keys()):\n",
    "    ax.set_title(id2label[label], fontsize=15)\n",
    "    idx = random.choice(label_to_idxs[label])\n",
    "    sample = sample_dataset[idx]\n",
    "    image = sample[\"image\"][0]\n",
    "    ax.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d4c95",
   "metadata": {},
   "source": [
    "Finally, we will split the entire dataset into training, validation, and test sets using the ratios of 70%, 10%, and 20%, respectively. We will then obtain the corresponding data loaders for each set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666cc54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import split_by_sample\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = split_by_sample(\n",
    "    dataset=sample_dataset,\n",
    "    ratios=[0.7, 0.1, 0.2]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d83c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.datasets import get_dataloader\n",
    "\n",
    "train_dataloader = get_dataloader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataloader = get_dataloader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_dataloader = get_dataloader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54353621",
   "metadata": {},
   "source": [
    "## Step 3. Define the Model\n",
    "\n",
    "Next, we will define the deep learning model we want to use for our task. PyHealth supports all major vision models available in the Torchvision package. You can load any of these models using the model_name argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import TorchvisionModel\n",
    "\n",
    "resnet = TorchvisionModel(\n",
    "    dataset=sample_dataset,\n",
    "    model_name=\"resnet18\",\n",
    "    model_config={\"weights\": \"DEFAULT\"}\n",
    ")\n",
    "\n",
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4e2763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.models import TorchvisionModel\n",
    "\n",
    "vit = TorchvisionModel(\n",
    "    dataset=sample_dataset,\n",
    "    model_name=\"vit_b_16\",\n",
    "    model_config={\"weights\": \"DEFAULT\"}\n",
    ")\n",
    "\n",
    "vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdccc3c",
   "metadata": {},
   "source": [
    "## Step 4. Training\n",
    "\n",
    "In this step, we will train the model using PyHealth's Trainer class, which simplifies the training process and provides standard functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165bddb0",
   "metadata": {},
   "source": [
    "Let us first train the ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.trainer import Trainer\n",
    "\n",
    "resnet_trainer = Trainer(model=resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712fc710",
   "metadata": {},
   "source": [
    "Before we begin training, let's first evaluate the initial performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet_trainer.evaluate(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc22f4a",
   "metadata": {},
   "source": [
    "Now, let's start the training process. Due to computational constraints, we will train the model for only one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a18319",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer.train(\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    epochs=1,\n",
    "    monitor=\"accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be6586",
   "metadata": {},
   "source": [
    "After training the model, we can compare its performance before and after. We should expect to see an increase in the accuracy score as the model learns from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6176aa1",
   "metadata": {},
   "source": [
    "## Step 5. Evaluation\n",
    "\n",
    "Lastly, we can evaluate the ResNet model on the test set. This can be done using PyHealth's `Trainer.evaluate()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resnet_trainer.evaluate(test_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bc37c6",
   "metadata": {},
   "source": [
    "Additionally, you can perform inference using the `Trainer.inference()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_prob, loss = resnet_trainer.inference(test_dataloader)\n",
    "y_pred = y_prob.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375cbcba",
   "metadata": {},
   "source": [
    "Below we show a confusion matrix of the trained ResNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58f6f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "ax = sns.heatmap(cf_matrix, linewidths=1, annot=True, fmt='g')\n",
    "ax.set_xticklabels([id2label[i] for i in range(4)])\n",
    "ax.set_yticklabels([id2label[i] for i in range(4)])\n",
    "ax.set_xlabel(\"Pred\")\n",
    "ax.set_ylabel(\"True\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89316531",
   "metadata": {},
   "source": [
    "# 6 Gradient Saliency Mapping\n",
    "For a bonus let's look at some simple gradient saliency maps applied to our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_requires_grad(in_dataset):\n",
    "  for sample in in_dataset:\n",
    "    sample['image'].requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e87796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.interpret.methods.basic_gradient import BasicGradientSaliencyMaps\n",
    "from pyhealth.interpret.methods import SaliencyVisualizer\n",
    "import torch\n",
    "\n",
    "# Create a batch with only COVID samples\n",
    "covid_label = label2id['COVID']\n",
    "covid_samples = [sample for sample in sample_dataset.samples if sample['disease'].item() == covid_label]\n",
    "\n",
    "# Take the first 32 COVID samples and create a batch\n",
    "batch_size = min(32, len(covid_samples))\n",
    "covid_batch = {\n",
    "    'image': torch.stack([covid_samples[i]['image'] for i in range(batch_size)]),\n",
    "    'disease': torch.stack([covid_samples[i]['disease'] for i in range(batch_size)])\n",
    "}\n",
    "\n",
    "print(f\"Created COVID batch with {batch_size} samples\")\n",
    "\n",
    "# Initialize saliency maps with batch input only\n",
    "saliency_maps = BasicGradientSaliencyMaps(\n",
    "    resnet,\n",
    "    input_batch=covid_batch\n",
    ")\n",
    "\n",
    "# Initialize the visualization module with correct parameter names\n",
    "visualizer = SaliencyVisualizer(default_cmap='hot', default_alpha=0.6, figure_size=(15, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc05ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show saliency map for the first image in the batch\n",
    "image_0 = covid_batch['image'][0]\n",
    "# Compute saliency for single image using attribute method\n",
    "saliency_result_0 = saliency_maps.attribute(image=image_0.unsqueeze(0), disease=covid_batch['disease'][0:1])\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt, \n",
    "    image=image_0, \n",
    "    saliency=saliency_result_0['image'][0],\n",
    "    title=f\"Gradient Saliency - {id2label[covid_label]} (Sample 0)\"\n",
    ")\n",
    "\n",
    "# Show saliency map for another image in the batch\n",
    "image_3 = covid_batch['image'][3]\n",
    "saliency_result_3 = saliency_maps.attribute(image=image_3.unsqueeze(0), disease=covid_batch['disease'][3:4])\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt, \n",
    "    image=image_3, \n",
    "    saliency=saliency_result_3['image'][0],\n",
    "    title=f\"Gradient Saliency - {id2label[covid_label]} (Sample 3)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a5907",
   "metadata": {},
   "source": [
    "# 7. Layer-wise Relevance Propagation (LRP)\n",
    "\n",
    "LRP is a powerful interpretability method that explains neural network predictions by propagating relevance scores backward through the network. Unlike gradient-based methods, LRP satisfies the conservation property: the sum of relevances at the input layer approximately equals the model's output for the target class.\n",
    "\n",
    "**New Implementation**: PyHealth now includes **UnifiedLRP** - a modular implementation supporting both CNNs and embedding-based models with 12 layer handlers including Conv2d, MaxPool2d, BatchNorm2d, and a new AdditionHandler for skip connections!\n",
    "\n",
    "**Experimental ResNet Support**: This demonstration uses our trained ResNet18 model with **experimental skip connection support**. The implementation includes a new AdditionLRPHandler that splits relevance between residual branches, though full integration is still being refined.\n",
    "\n",
    "Let's apply LRP to our trained ResNet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25772348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear ALL LRP-related objects from memory\n",
    "import gc\n",
    "\n",
    "# Delete old lrp instances\n",
    "for var_name in ['lrp', 'lrp_alphabeta']:\n",
    "    if var_name in globals():\n",
    "        del globals()[var_name]\n",
    "        print(f\"Deleted {var_name}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "\n",
    "# Reload the LRP modules to get the latest handler cache fix\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove cached modules\n",
    "modules_to_reload = [\n",
    "    'pyhealth.interpret.methods.lrp',\n",
    "    'pyhealth.interpret.methods.lrp_base',\n",
    "    'pyhealth.interpret.methods'\n",
    "]\n",
    "\n",
    "for module_name in modules_to_reload:\n",
    "    if module_name in sys.modules:\n",
    "        del sys.modules[module_name]\n",
    "        print(f\"Cleared {module_name} from cache\")\n",
    "\n",
    "# Force reimport\n",
    "from pyhealth.interpret.methods import UnifiedLRP\n",
    "print(\"\\n‚úì Reloaded LRP modules with handler cache clearing fix\")\n",
    "print(\"‚úì This fix ensures cached activations don't persist between runs\")\n",
    "print(\"‚úì Ready to run LRP without shape mismatch errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d79732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pyhealth.interpret.methods import UnifiedLRP\n",
    "import torch\n",
    "\n",
    "# Use our trained ResNet18 model\n",
    "device = next(resnet.model.parameters()).device\n",
    "resnet.model.eval()\n",
    "\n",
    "print(\"Using trained ResNet18 model for LRP (sequential processing)\")\n",
    "print(f\"  Model has {sum(p.numel() for p in resnet.model.parameters())} parameters\")\n",
    "print(f\"  Model accuracy on test set: 84%\")\n",
    "\n",
    "# Suppress conservation warnings for cleaner output\n",
    "logging.getLogger('pyhealth.interpret.methods.lrp_base').setLevel(logging.ERROR)\n",
    "\n",
    "# Initialize UnifiedLRP with epsilon rule\n",
    "lrp = UnifiedLRP(\n",
    "    model=resnet.model,\n",
    "    rule='epsilon',\n",
    "    epsilon=0.1,\n",
    "    validate_conservation=False\n",
    ")\n",
    "\n",
    "# Compute LRP attributions for the first COVID sample\n",
    "print(f\"\\nComputing LRP attributions for COVID-19 sample...\")\n",
    "covid_image = covid_batch['image'][0:1]\n",
    "\n",
    "# Convert grayscale to RGB (ResNet expects 3 channels)\n",
    "if covid_image.shape[1] == 1:\n",
    "    covid_image = covid_image.repeat(1, 3, 1, 1)\n",
    "\n",
    "# Move to the same device as the model\n",
    "covid_image = covid_image.to(device)\n",
    "\n",
    "# Forward pass to get prediction\n",
    "with torch.no_grad():\n",
    "    output = resnet.model(covid_image)\n",
    "    predicted_class = output.argmax(dim=1).item()\n",
    "\n",
    "print(f\"\\nDEBUG: About to run LRP.attribute()\")\n",
    "print(f\"  Number of layers in model: {len(list(resnet.model.named_modules()))}\")\n",
    "print(f\"  Layer order before attribute: {len(lrp.layer_order)}\")\n",
    "\n",
    "# Compute LRP attributions\n",
    "try:\n",
    "    lrp_attributions = lrp.attribute(\n",
    "        inputs={'x': covid_image},\n",
    "        target_class=predicted_class\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì LRP attributions computed!\")\n",
    "    print(f\"  Input shape: {covid_image.shape}\")\n",
    "    print(f\"  Attribution shape: {lrp_attributions['x'].shape}\")\n",
    "    print(f\"  Predicted class: {id2label[predicted_class]}\")\n",
    "    print(f\"  Total relevance: {lrp_attributions['x'].sum().item():.4f}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"\\n‚ùå ERROR: {e}\")\n",
    "    print(f\"\\nDEBUG: Layer order after forward pass: {len(lrp.layer_order)}\")\n",
    "    if len(lrp.layer_order) > 0:\n",
    "        print(f\"Last 10 layers registered:\")\n",
    "        for i, (name, module, handler) in enumerate(lrp.layer_order[-10:]):\n",
    "            print(f\"  {len(lrp.layer_order) - 10 + i}: {name} - {type(module).__name__}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0abc71d",
   "metadata": {},
   "source": [
    "## Visualizing LRP Results\n",
    "\n",
    "LRP provides pixel-level explanations showing which image regions contributed to the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd6f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LRP relevance map\n",
    "relevance_map = lrp_attributions['x'].squeeze()\n",
    "\n",
    "# For visualization, use the first channel (all channels are the same for grayscale)\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],  # Original grayscale image\n",
    "    saliency=relevance_map[0] if relevance_map.dim() == 3 else relevance_map,  # First channel of attribution\n",
    "    title=f\"LRP Relevance Map - {id2label[predicted_class]} (Epsilon Rule)\",\n",
    ")\n",
    "\n",
    "# Also show gradient saliency for comparison\n",
    "saliency_comparison = saliency_maps.attribute(image=covid_batch['image'][0:1], disease=covid_batch['disease'][0:1])\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    saliency=saliency_comparison['image'][0],\n",
    "    title=f\"Gradient Saliency (for comparison) - {id2label[predicted_class]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0c20c",
   "metadata": {},
   "source": [
    "## Comparing Different LRP Rules\n",
    "\n",
    "LRP supports different propagation rules that handle positive and negative contributions differently:\n",
    "\n",
    "**Epsilon Rule (`rule=\"epsilon\"`):**\n",
    "- Adds a small stabilizer Œµ to prevent division by zero\n",
    "- Best for: General use, numerical stability\n",
    "- Good for layers where both positive and negative activations matter equally\n",
    "- Conservation violations: 5-50% (acceptable)\n",
    "\n",
    "**Alpha-Beta Rule (`rule=\"alphabeta\"`):**\n",
    "- Separates positive and negative contributions with different weights (Œ± and Œ≤)\n",
    "- Default: Œ±=2, Œ≤=1 (emphasizes positive contributions)\n",
    "- Best for: When you want to focus on excitatory (positive) evidence\n",
    "- Often produces sharper, more focused heatmaps\n",
    "- Conservation violations: 50-150% (acceptable)\n",
    "\n",
    "Let's compare both rules on the same image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd47895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon rule (already computed)\n",
    "print(\"LRP with Epsilon Rule (Œµ=0.1)\")\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    saliency=relevance_map[0] if relevance_map.dim() == 3 else relevance_map,\n",
    "    title=f\"LRP Epsilon Rule - {id2label[predicted_class]}\",\n",
    ")\n",
    "\n",
    "# Now compute LRP with Alpha-Beta Rule\n",
    "print(\"\\nComputing LRP with Alpha-Beta Rule (Œ±=2, Œ≤=1)...\")\n",
    "lrp_alphabeta = UnifiedLRP(\n",
    "    model=resnet.model,\n",
    "    rule='alphabeta',\n",
    "    alpha=2.0,\n",
    "    beta=1.0,\n",
    "    validate_conservation=False\n",
    ")\n",
    "\n",
    "alphabeta_attributions = lrp_alphabeta.attribute(\n",
    "    inputs={'x': covid_image},\n",
    "    target_class=predicted_class\n",
    ")\n",
    "\n",
    "alphabeta_relevance = alphabeta_attributions['x'].squeeze()\n",
    "visualizer.plot_saliency_overlay(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    saliency=alphabeta_relevance[0] if alphabeta_relevance.dim() == 3 else alphabeta_relevance,\n",
    "    title=f\"LRP Alpha-Beta Rule (Œ±=2, Œ≤=1) - {id2label[predicted_class]}\",\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Results:\")\n",
    "print(f\"  Epsilon Rule - Total relevance: {lrp_attributions['x'].sum().item():.4f}\")\n",
    "print(f\"  Alpha-Beta Rule - Total relevance: {alphabeta_attributions['x'].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706a7c5",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison of All Interpretation Methods\n",
    "\n",
    "Let's create a comprehensive comparison showing gradient saliency and both LRP rules side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b9870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison of all three methods\n",
    "attributions_dict = {\n",
    "    'Gradient Saliency': saliency_comparison['image'][0],\n",
    "    'LRP Epsilon (Œµ=0.1)': relevance_map[0] if relevance_map.dim() == 3 else relevance_map,\n",
    "    'LRP Alpha-Beta (Œ±=2, Œ≤=1)': alphabeta_relevance[0] if alphabeta_relevance.dim() == 3 else alphabeta_relevance\n",
    "}\n",
    "\n",
    "visualizer.plot_multiple_attributions(\n",
    "    plt,\n",
    "    image=covid_batch['image'][0],\n",
    "    attributions=attributions_dict\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  ‚Ä¢ Gradient Saliency: Shows regions with high gradient magnitude\")\n",
    "print(\"  ‚Ä¢ LRP Epsilon: More balanced, stable attribution across the image\")\n",
    "print(\"  ‚Ä¢ LRP Alpha-Beta: Sharper focus on positive evidence regions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b1cd7",
   "metadata": {},
   "source": [
    "## UnifiedLRP Implementation Details\n",
    "\n",
    "The **UnifiedLRP** implementation supports a wide range of neural network architectures through modular layer handlers:\n",
    "\n",
    "**Supported Layers (12 handlers):**\n",
    "- **Dense/Embedding**: Linear, ReLU, Embedding\n",
    "- **Convolutional**: Conv2d, MaxPool2d, AvgPool2d, AdaptiveAvgPool2d  \n",
    "- **Normalization**: BatchNorm2d\n",
    "- **Utility**: Flatten, Dropout\n",
    "- **Skip Connections**: Addition (experimental)\n",
    "\n",
    "This modular design makes it easy to:\n",
    "- Apply LRP to both CNNs (images) and MLPs (tabular/embedding data)\n",
    "- Handle skip connections in ResNet architectures\n",
    "- Extend with custom handlers for new layer types\n",
    "- Validate conservation property at each layer\n",
    "\n",
    "**Current Status**: Production-ready for standard CNN architectures. ResNet skip connection support is experimental and under active development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed27ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply LRP to multiple samples from the batch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "for idx in range(3):\n",
    "    sample_image = covid_batch['image'][idx:idx+1]\n",
    "    \n",
    "    # Convert grayscale to RGB for ResNet\n",
    "    sample_image_rgb = sample_image.repeat(1, 3, 1, 1) if sample_image.shape[1] == 1 else sample_image\n",
    "    \n",
    "    # Move to the correct device\n",
    "    sample_image_rgb = sample_image_rgb.to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = resnet.model(sample_image_rgb)\n",
    "        pred_class = output.argmax(dim=1).item()\n",
    "    \n",
    "    # Compute LRP\n",
    "    sample_lrp = lrp.attribute(\n",
    "        inputs={'x': sample_image_rgb},\n",
    "        target_class=pred_class\n",
    "    )\n",
    "    \n",
    "    # Plot original image (grayscale)\n",
    "    axes[0, idx].imshow(sample_image.squeeze().cpu().numpy(), cmap='gray')\n",
    "    axes[0, idx].set_title(f'Sample {idx}: {id2label[pred_class]}', fontsize=12, fontweight='bold')\n",
    "    axes[0, idx].axis('off')\n",
    "    \n",
    "    # Plot LRP heatmap (sum across RGB channels for visualization)\n",
    "    relevance = sample_lrp['x'].squeeze()\n",
    "    if relevance.dim() == 3:  # If shape is (3, H, W)\n",
    "        relevance = relevance.sum(dim=0)  # Sum across channels\n",
    "    im = axes[1, idx].imshow(relevance.detach().cpu().numpy(), cmap='seismic', vmin=-0.1, vmax=0.1)\n",
    "    axes[1, idx].set_title(f'LRP Heatmap (Œµ=0.1)', fontsize=10)\n",
    "    axes[1, idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Applied LRP to 3 different COVID-19 X-ray samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2dde88",
   "metadata": {},
   "source": [
    "## Key Takeaways: Gradient Saliency vs. LRP\n",
    "\n",
    "**Gradient Saliency Maps:**\n",
    "- ‚úì Fast - single backward pass through gradients\n",
    "- ‚úì Works with any differentiable model  \n",
    "- ‚úì Good for identifying \"where\" the model looks\n",
    "- ‚úì Straightforward implementation\n",
    "- ‚úì **Fully supports all architectures including ResNet**\n",
    "- ‚ö†Ô∏è Can be noisy and may require smoothing\n",
    "- ‚ö†Ô∏è Doesn't satisfy conservation property\n",
    "\n",
    "**Layer-wise Relevance Propagation (LRP):**\n",
    "- ‚úì **Conservation property**: Relevances sum to model output for the target class\n",
    "- ‚úì More theoretically grounded attribution\n",
    "- ‚úì Modular design with layer-specific handlers\n",
    "- ‚úì Better captures \"how much\" each pixel contributes\n",
    "- ‚úì Supports both CNNs and MLPs with UnifiedLRP\n",
    "- ‚úì **Experimental ResNet support** with skip connection handlers\n",
    "- ‚ö†Ô∏è Requires layer-specific propagation rules\n",
    "- ‚ö†Ô∏è Expected conservation violations of 5-150% depending on rule\n",
    "\n",
    "**Which one to use?**\n",
    "- Use **Gradient Saliency** for quick exploration and fast prototyping\n",
    "- Use **LRP** when you need precise, quantifiable attributions with conservation\n",
    "- Use **LRP Epsilon Rule** for numerically stable, balanced attributions\n",
    "- Use **LRP Alpha-Beta Rule** for sharper visualizations emphasizing positive evidence\n",
    "- Use **both** to get complementary insights into your model's behavior!\n",
    "\n",
    "**UnifiedLRP Status:**\n",
    "- ‚úÖ Production-ready for sequential CNNs (VGG, AlexNet)\n",
    "- ‚úÖ Supports: Conv2d, MaxPool2d, BatchNorm2d, Linear, ReLU, Flatten, Dropout, AdaptiveAvgPool2d, AvgPool2d, Embedding\n",
    "- üß™ **Experimental**: ResNet skip connections (AdditionHandler implemented, integration in progress)\n",
    "- ‚è≥ Future: Transformer attention, RNN support\n",
    "\n",
    "**Note on ResNet**: This notebook demonstrates experimental LRP support for ResNet architectures. The AdditionLRPHandler splits relevance proportionally between skip connection branches. Results should be interpreted with care as the implementation is under active development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187c4e2",
   "metadata": {},
   "source": [
    "# 8. Validating Interpretability with Faithfulness Metrics\n",
    "\n",
    "Now that we have both gradient saliency and LRP attributions, we need to validate that they're actually useful. PyHealth provides **Comprehensiveness** and **Sufficiency** metrics to quantitatively measure attribution faithfulness.\n",
    "\n",
    "## What These Metrics Measure:\n",
    "\n",
    "**Comprehensiveness (higher is better):**\n",
    "- Measures how much the prediction drops when we **REMOVE** the most important features\n",
    "- If the attribution is faithful, removing important features should significantly decrease the prediction confidence\n",
    "- Formula: `COMP = P(original) - P(top_k_removed)`\n",
    "- **Good attributions**: High comprehensiveness (model breaks when important features removed)\n",
    "\n",
    "**Sufficiency (lower is better):**\n",
    "- Measures how much the prediction drops when we **KEEP ONLY** the most important features\n",
    "- If the attribution is sufficient, keeping only important features should preserve the prediction\n",
    "- Formula: `SUFF = P(original) - P(only_top_k_kept)`\n",
    "- **Good attributions**: Low sufficiency (model works well with only important features)\n",
    "\n",
    "**Why This Matters for Medical AI:**\n",
    "- We need to trust that highlighted regions actually influence the diagnosis\n",
    "- Random or noisy attributions would fail these metrics\n",
    "- Helps identify which interpretation method is more reliable for clinical use\n",
    "\n",
    "Let's compute these metrics for both methods!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1666197",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.metrics.interpretability import Evaluator\n",
    "\n",
    "# Initialize the evaluator with our ResNet model\n",
    "# We'll test at multiple percentages: 1%, 5%, 10%, 20%, 50% of features\n",
    "evaluator = Evaluator(\n",
    "    model=resnet,\n",
    "    percentages=[1, 5, 10, 20, 50],\n",
    "    ablation_strategy='zero',  # Set removed features to 0 (black pixels)\n",
    ")\n",
    "\n",
    "print(\"‚úì Initialized interpretability evaluator\")\n",
    "print(f\"  Testing at: {evaluator.percentages}% of features\")\n",
    "print(f\"  Ablation strategy: Set removed pixels to 0 (black)\")\n",
    "print(f\"  Model: ResNet18 (84% accuracy)\")\n",
    "print(f\"\\nMetrics available:\")\n",
    "print(f\"  ‚Ä¢ Comprehensiveness: Higher is better (removing important features hurts model)\")\n",
    "print(f\"  ‚Ä¢ Sufficiency: Lower is better (keeping important features preserves prediction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded92ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs for evaluation (need to match the format expected by the model)\n",
    "# The model expects images with 3 channels (RGB) even though our X-rays are grayscale\n",
    "\n",
    "# We'll evaluate on the first COVID sample\n",
    "eval_image = covid_batch['image'][0:1]  # Shape: [1, 1, H, W]\n",
    "\n",
    "# Convert to RGB for ResNet\n",
    "eval_image_rgb = eval_image.repeat(1, 3, 1, 1)  # Shape: [1, 3, H, W]\n",
    "eval_image_rgb = eval_image_rgb.to(device)\n",
    "\n",
    "# Create input dictionary (model expects 'image' and 'disease' keys)\n",
    "eval_inputs = {\n",
    "    'image': eval_image_rgb,\n",
    "    'disease': covid_batch['disease'][0:1].to(device)  # Add label for model forward pass\n",
    "}\n",
    "\n",
    "print(\"Prepared evaluation inputs:\")\n",
    "print(f\"  Original grayscale shape: {eval_image.shape}\")\n",
    "print(f\"  RGB shape for model: {eval_image_rgb.shape}\")\n",
    "print(f\"  Label: {id2label[covid_batch['disease'][0].item()]}\")\n",
    "print(f\"  Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f4cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Gradient Saliency\n",
    "print(\"=\"*70)\n",
    "print(\"Evaluating Gradient Saliency Attributions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get gradient saliency attributions\n",
    "grad_attr = saliency_maps.attribute(\n",
    "    image=eval_image.to(device), \n",
    "    disease=covid_batch['disease'][0:1].to(device)\n",
    ")\n",
    "\n",
    "# The gradient attributions are for the grayscale image, but we need RGB format\n",
    "# Replicate across 3 channels to match model input\n",
    "grad_attr_rgb = grad_attr['image'].repeat(1, 3, 1, 1)\n",
    "grad_attributions = {'image': grad_attr_rgb}\n",
    "\n",
    "# Compute metrics\n",
    "grad_results = evaluator.evaluate(\n",
    "    inputs=eval_inputs,\n",
    "    attributions=grad_attributions,\n",
    "    metrics=['comprehensiveness', 'sufficiency'],\n",
    "    return_per_percentage=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nGradient Saliency Results:\")\n",
    "print(\"-\" * 70)\n",
    "for metric_name, results_dict in grad_results.items():\n",
    "    print(f\"\\n{metric_name.capitalize()}:\")\n",
    "    for percentage, scores in sorted(results_dict.items()):\n",
    "        print(f\"  {percentage:3d}%: {scores.mean().item():.4f}\")\n",
    "        \n",
    "# Store for comparison\n",
    "grad_comp = {pct: scores.mean().item() for pct, scores in grad_results['comprehensiveness'].items()}\n",
    "grad_suff = {pct: scores.mean().item() for pct, scores in grad_results['sufficiency'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179a5386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LRP Epsilon Rule\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Evaluating LRP Epsilon Rule Attributions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# LRP already computed attributions in RGB format\n",
    "lrp_epsilon_attributions = {'image': lrp_attributions['x']}\n",
    "\n",
    "# Compute metrics\n",
    "lrp_epsilon_results = evaluator.evaluate(\n",
    "    inputs=eval_inputs,\n",
    "    attributions=lrp_epsilon_attributions,\n",
    "    metrics=['comprehensiveness', 'sufficiency'],\n",
    "    return_per_percentage=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLRP Epsilon Rule (Œµ=0.1) Results:\")\n",
    "print(\"-\" * 70)\n",
    "for metric_name, results_dict in lrp_epsilon_results.items():\n",
    "    print(f\"\\n{metric_name.capitalize()}:\")\n",
    "    for percentage, scores in sorted(results_dict.items()):\n",
    "        print(f\"  {percentage:3d}%: {scores.mean().item():.4f}\")\n",
    "\n",
    "# Store for comparison\n",
    "lrp_eps_comp = {pct: scores.mean().item() for pct, scores in lrp_epsilon_results['comprehensiveness'].items()}\n",
    "lrp_eps_suff = {pct: scores.mean().item() for pct, scores in lrp_epsilon_results['sufficiency'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97238672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LRP Alpha-Beta Rule\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Evaluating LRP Alpha-Beta Rule Attributions\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use the alpha-beta attributions we computed earlier\n",
    "lrp_alphabeta_attributions = {'image': alphabeta_attributions['x']}\n",
    "\n",
    "# Compute metrics\n",
    "lrp_alphabeta_results = evaluator.evaluate(\n",
    "    inputs=eval_inputs,\n",
    "    attributions=lrp_alphabeta_attributions,\n",
    "    metrics=['comprehensiveness', 'sufficiency'],\n",
    "    return_per_percentage=True\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"\\nLRP Alpha-Beta Rule (Œ±=2, Œ≤=1) Results:\")\n",
    "print(\"-\" * 70)\n",
    "for metric_name, results_dict in lrp_alphabeta_results.items():\n",
    "    print(f\"\\n{metric_name.capitalize()}:\")\n",
    "    for percentage, scores in sorted(results_dict.items()):\n",
    "        print(f\"  {percentage:3d}%: {scores.mean().item():.4f}\")\n",
    "\n",
    "# Store for comparison\n",
    "lrp_ab_comp = {pct: scores.mean().item() for pct, scores in lrp_alphabeta_results['comprehensiveness'].items()}\n",
    "lrp_ab_suff = {pct: scores.mean().item() for pct, scores in lrp_alphabeta_results['sufficiency'].items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d1912",
   "metadata": {},
   "source": [
    "## Visualizing the Metric Comparison\n",
    "\n",
    "Let's create plots to compare the three methods across different feature removal percentages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Get percentages for x-axis\n",
    "percentages = sorted(grad_comp.keys())\n",
    "\n",
    "# Plot Comprehensiveness (higher is better)\n",
    "ax1.plot(percentages, [grad_comp[p] for p in percentages], \n",
    "         marker='o', linewidth=2, markersize=8, label='Gradient Saliency')\n",
    "ax1.plot(percentages, [lrp_eps_comp[p] for p in percentages], \n",
    "         marker='s', linewidth=2, markersize=8, label='LRP Epsilon (Œµ=0.1)')\n",
    "ax1.plot(percentages, [lrp_ab_comp[p] for p in percentages], \n",
    "         marker='^', linewidth=2, markersize=8, label='LRP Alpha-Beta (Œ±=2, Œ≤=1)')\n",
    "\n",
    "ax1.set_xlabel('% of Features Removed', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Comprehensiveness Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Comprehensiveness: Higher is Better\\n(Removing important features hurts prediction)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(bottom=0)\n",
    "\n",
    "# Plot Sufficiency (lower is better)\n",
    "ax2.plot(percentages, [grad_suff[p] for p in percentages], \n",
    "         marker='o', linewidth=2, markersize=8, label='Gradient Saliency')\n",
    "ax2.plot(percentages, [lrp_eps_suff[p] for p in percentages], \n",
    "         marker='s', linewidth=2, markersize=8, label='LRP Epsilon (Œµ=0.1)')\n",
    "ax2.plot(percentages, [lrp_ab_suff[p] for p in percentages], \n",
    "         marker='^', linewidth=2, markersize=8, label='LRP Alpha-Beta (Œ±=2, Œ≤=1)')\n",
    "\n",
    "ax2.set_xlabel('% of Features Kept', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Sufficiency Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Sufficiency: Lower is Better\\n(Keeping important features preserves prediction)', \n",
    "              fontsize=13, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: Which Method is More Faithful?\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nComprehensiveness (Higher = Better):\")\n",
    "print(f\"  Gradient Saliency:    {np.mean(list(grad_comp.values())):.4f}\")\n",
    "print(f\"  LRP Epsilon:          {np.mean(list(lrp_eps_comp.values())):.4f}\")\n",
    "print(f\"  LRP Alpha-Beta:       {np.mean(list(lrp_ab_comp.values())):.4f}\")\n",
    "\n",
    "print(\"\\nSufficiency (Lower = Better):\")\n",
    "print(f\"  Gradient Saliency:    {np.mean(list(grad_suff.values())):.4f}\")\n",
    "print(f\"  LRP Epsilon:          {np.mean(list(lrp_eps_suff.values())):.4f}\")\n",
    "print(f\"  LRP Alpha-Beta:       {np.mean(list(lrp_ab_suff.values())):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d433684",
   "metadata": {},
   "source": [
    "## Interpreting the Results\n",
    "\n",
    "**What Do These Metrics Tell Us?**\n",
    "\n",
    "The faithfulness metrics provide quantitative evidence about which interpretation method is more reliable:\n",
    "\n",
    "1. **Comprehensiveness Analysis:**\n",
    "   - Measures prediction drop when removing top-k most important features\n",
    "   - **Higher scores** = Better attributions (removing important features breaks the model)\n",
    "   - If all methods score low, the attributions may not identify truly important features\n",
    "   - Look for the method with highest comprehensiveness across all percentages\n",
    "\n",
    "2. **Sufficiency Analysis:**\n",
    "   - Measures prediction drop when keeping ONLY top-k most important features\n",
    "   - **Lower scores** = Better attributions (model works with just important features)\n",
    "   - If scores are high, the attribution missed important information\n",
    "   - Look for the method with lowest sufficiency, especially at higher percentages\n",
    "\n",
    "3. **Combined Interpretation:**\n",
    "   - **Ideal method**: High comprehensiveness + Low sufficiency\n",
    "   - This means: Important features are correctly identified (comprehensive) and sufficient for prediction\n",
    "   - Trade-offs: Some methods optimize for one metric over the other\n",
    "\n",
    "**For Medical AI:**\n",
    "- These metrics validate that our attributions are meaningful and not random noise\n",
    "- Higher faithfulness = More trustworthy explanations for clinicians\n",
    "- Helps select which interpretation method to use in production systems\n",
    "- Essential for regulatory approval and clinical deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
