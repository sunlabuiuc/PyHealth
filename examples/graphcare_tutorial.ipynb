{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphCare Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the **GraphCare** model for healthcare predictions with personalized knowledge graphs in PyHealth.\n",
    "\n",
    "**Contributors:** Josh Steier\n",
    "\n",
    "**Paper:** Pengcheng Jiang et al. *GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs.* ICLR 2024.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. Understand the GraphCare architecture\n",
    "2. Build synthetic patient knowledge graphs\n",
    "3. Instantiate the model with different GNN backbones\n",
    "4. Train and evaluate on binary classification\n",
    "5. Inspect attention weights for interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries and set seeds for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "from pyhealth.models.graphcare import GraphCare, BiAttentionGNNConv\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding GraphCare Architecture\n",
    "\n",
    "GraphCare operates on **personalized patient knowledge graphs** derived from EHR codes.\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "```\n",
    "Patient EHR Record\n",
    "  ├── Conditions  ──┐\n",
    "  ├── Procedures  ──┤── Concept-specific KGs ── Patient KG\n",
    "  └── Medications ──┘                              │\n",
    "                                                    ▼\n",
    "                                           ┌───────────────┐\n",
    "                                           │  GNN Encoder   │\n",
    "                                           │  (BAT/GAT/GIN) │\n",
    "                                           │  + Bi-Attention │\n",
    "                                           └───────┬───────┘\n",
    "                                                   │\n",
    "                                    ┌──────────────┼──────────────┐\n",
    "                                    ▼              ▼              ▼\n",
    "                              graph pool     node embed      joint (concat)\n",
    "                                    │              │              │\n",
    "                                    └──────────────┴──────────────┘\n",
    "                                                   │\n",
    "                                                   ▼\n",
    "                                              MLP Head\n",
    "                                                   │\n",
    "                                                   ▼\n",
    "                                              Prediction\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "- **Bi-Attention (BAT):** Visit-level (α) and node-level (β) attention with temporal decay\n",
    "- **Temporal Decay:** λ_j = exp(γ(V−j)) — more recent visits get higher weight\n",
    "- **Patient Modes:** `graph` (global pool), `node` (EHR-node avg), `joint` (both concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Synthetic Patient Graphs\n",
    "\n",
    "In a real setting, patient KGs are constructed from EHR codes using LLM-prompted subgraphs and medical KGs (see the [GraphCare repo](https://github.com/pat-jj/GraphCare) for the generation pipeline).\n",
    "\n",
    "Here we create synthetic data to demonstrate the model API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node embeddings: torch.Size([500, 64])\n",
      "Relation embeddings: torch.Size([50, 64])\n"
     ]
    }
   ],
   "source": [
    "# --- Global KG parameters ---\n",
    "NUM_NODES = 500       # Total cluster nodes in the KG\n",
    "NUM_RELS = 50         # Total relation types\n",
    "MAX_VISIT = 5         # Max visits per patient\n",
    "EMBEDDING_DIM = 64    # Pre-trained embedding dim\n",
    "HIDDEN_DIM = 64       # Model hidden dim\n",
    "NUM_PATIENTS = 200    # Synthetic patients\n",
    "\n",
    "# --- Fake pre-trained embeddings ---\n",
    "# In practice these come from word2vec / TransE on the KG\n",
    "node_emb = torch.randn(NUM_NODES, EMBEDDING_DIM)\n",
    "rel_emb = torch.randn(NUM_RELS, EMBEDDING_DIM)\n",
    "\n",
    "print(f\"Node embeddings: {node_emb.shape}\")\n",
    "print(f\"Relation embeddings: {rel_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 200 patient graphs\n",
      "Example graph: Data(edge_index=[2, 49], y=[35], relation=[49], visit_padded_node=[5, 500], ehr_nodes=[500], label=[1], num_nodes=35)\n"
     ]
    }
   ],
   "source": [
    "def create_synthetic_patient_graph(patient_idx, num_nodes, num_rels, max_visit):\n",
    "    \"\"\"Create a single synthetic patient KG as a PyG Data object.\n",
    "    \n",
    "    Each patient graph is a subgraph of the global KG, with:\n",
    "    - Node IDs (y): indices into the global KG node embedding table\n",
    "    - Relation IDs: indices into the global relation embedding table\n",
    "    - visit_padded_node: binary (max_visit, num_nodes) indicating which\n",
    "      KG nodes appear in each visit\n",
    "    - ehr_nodes: binary (num_nodes,) indicating direct EHR nodes\n",
    "    - label: binary mortality label\n",
    "    \"\"\"\n",
    "    # Random subgraph size\n",
    "    n = random.randint(15, 40)   # nodes in this patient's subgraph\n",
    "    e = random.randint(n, n * 3)  # edges\n",
    "    \n",
    "    # Local edge indices (within the subgraph)\n",
    "    src = torch.randint(0, n, (e,))\n",
    "    dst = torch.randint(0, n, (e,))\n",
    "    \n",
    "    # Node IDs: which global KG nodes are in this subgraph\n",
    "    y = torch.randint(0, num_nodes, (n,))\n",
    "    \n",
    "    # Relation IDs per edge\n",
    "    relation = torch.randint(0, num_rels, (e,))\n",
    "    \n",
    "    # Visit-padded node indicators\n",
    "    vpn = torch.zeros(max_visit, num_nodes)\n",
    "    for v in range(max_visit):\n",
    "        # Each visit activates some random KG nodes\n",
    "        active = y[torch.randint(0, n, (random.randint(2, 8),))]\n",
    "        vpn[v, active] = 1.0\n",
    "    \n",
    "    # Direct EHR nodes (subset of the patient's KG nodes)\n",
    "    ehr = torch.zeros(num_nodes)\n",
    "    ehr_active = y[torch.randint(0, n, (random.randint(3, 10),))]\n",
    "    ehr[ehr_active] = 1.0\n",
    "    \n",
    "    # Binary label (mortality)\n",
    "    label = torch.tensor([float(patient_idx % 2)])\n",
    "    \n",
    "    data = Data(\n",
    "        edge_index=torch.stack([src, dst]),\n",
    "        y=y,\n",
    "        relation=relation,\n",
    "        visit_padded_node=vpn,\n",
    "        ehr_nodes=ehr,\n",
    "        label=label,\n",
    "    )\n",
    "    data.num_nodes = n\n",
    "    return data\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "all_graphs = [\n",
    "    create_synthetic_patient_graph(i, NUM_NODES, NUM_RELS, MAX_VISIT)\n",
    "    for i in range(NUM_PATIENTS)\n",
    "]\n",
    "\n",
    "print(f\"Created {len(all_graphs)} patient graphs\")\n",
    "print(f\"Example graph: {all_graphs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 160, 20, 20\n"
     ]
    }
   ],
   "source": [
    "# --- Train / Val / Test split ---\n",
    "n_train = int(0.8 * NUM_PATIENTS)\n",
    "n_val = int(0.1 * NUM_PATIENTS)\n",
    "\n",
    "train_graphs = all_graphs[:n_train]\n",
    "val_graphs = all_graphs[n_train:n_train + n_val]\n",
    "test_graphs = all_graphs[n_train + n_val:]\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = PyGDataLoader(train_graphs, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "val_loader = PyGDataLoader(val_graphs, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "test_loader = PyGDataLoader(test_graphs, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_graphs)}, {len(val_graphs)}, {len(test_graphs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Batch Structure\n",
    "\n",
    "Examine what a PyG batch looks like after collation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch attributes:\n",
      "  edge_index: torch.Size([2, 783])\n",
      "  y (node_ids): torch.Size([436])\n",
      "  relation: torch.Size([783])\n",
      "  batch vector: torch.Size([436]) (max=15)\n",
      "  visit_padded_node: torch.Size([80, 500])\n",
      "  ehr_nodes: torch.Size([8000])\n",
      "  label: torch.Size([16])\n",
      "\n",
      "Note: visit_padded_node and ehr_nodes need reshaping before forward().\n",
      "  visit_node reshaped: (16, 5, 500)\n",
      "  ehr_nodes reshaped:  (16, 500)\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "\n",
    "print(\"Batch attributes:\")\n",
    "print(f\"  edge_index: {batch.edge_index.shape}\")\n",
    "print(f\"  y (node_ids): {batch.y.shape}\")\n",
    "print(f\"  relation: {batch.relation.shape}\")\n",
    "print(f\"  batch vector: {batch.batch.shape} (max={batch.batch.max().item()})\")\n",
    "print(f\"  visit_padded_node: {batch.visit_padded_node.shape}\")\n",
    "print(f\"  ehr_nodes: {batch.ehr_nodes.shape}\")\n",
    "print(f\"  label: {batch.label.shape}\")\n",
    "print()\n",
    "print(\"Note: visit_padded_node and ehr_nodes need reshaping before forward().\")\n",
    "print(f\"  visit_node reshaped: ({BATCH_SIZE}, {MAX_VISIT}, {NUM_NODES})\")\n",
    "print(f\"  ehr_nodes reshaped:  ({BATCH_SIZE}, {NUM_NODES})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate GraphCare\n",
    "\n",
    "Compare the three GNN backbones (BAT, GAT, GIN) and three patient modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphCare Configuration Comparison\n",
      "============================================================\n",
      "  BAT/joint  —    549,941 params\n",
      "  BAT/graph  —    549,877 params\n",
      "  BAT/node   —    549,877 params\n",
      "  GAT/joint  —    550,067 params\n",
      "  GIN/joint  —    549,811 params\n"
     ]
    }
   ],
   "source": [
    "print(\"GraphCare Configuration Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "configs = [\n",
    "    (\"BAT\", \"joint\"),\n",
    "    (\"BAT\", \"graph\"),\n",
    "    (\"BAT\", \"node\"),\n",
    "    (\"GAT\", \"joint\"),\n",
    "    (\"GIN\", \"joint\"),\n",
    "]\n",
    "\n",
    "for gnn, mode in configs:\n",
    "    model = GraphCare(\n",
    "        num_nodes=NUM_NODES,\n",
    "        num_rels=NUM_RELS,\n",
    "        max_visit=MAX_VISIT,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        out_channels=1,\n",
    "        layers=2,\n",
    "        node_emb=node_emb,\n",
    "        rel_emb=rel_emb,\n",
    "        gnn=gnn,\n",
    "        patient_mode=mode,\n",
    "    )\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  {gnn:>3s}/{mode:<6s} — {n_params:>10,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Forward Pass Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 488 total nodes across 16 patient graphs\n",
      "Output logits: torch.Size([16, 1])\n",
      "Predictions: [0.508 0.484 0.51  0.493 0.505 0.522 0.501 0.493 0.528 0.482 0.509 0.447\n",
      " 0.505 0.492 0.509 0.502]\n"
     ]
    }
   ],
   "source": [
    "model = GraphCare(\n",
    "    num_nodes=NUM_NODES,\n",
    "    num_rels=NUM_RELS,\n",
    "    max_visit=MAX_VISIT,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    out_channels=1,\n",
    "    layers=2,\n",
    "    dropout=0.5,\n",
    "    decay_rate=0.01,\n",
    "    node_emb=node_emb,\n",
    "    rel_emb=rel_emb,\n",
    "    gnn=\"BAT\",\n",
    "    patient_mode=\"joint\",\n",
    "    use_alpha=True,\n",
    "    use_beta=True,\n",
    "    use_edge_attn=True,\n",
    ").to(device)\n",
    "\n",
    "# Reshape batch tensors\n",
    "batch = next(iter(train_loader))\n",
    "batch = batch.to(device)\n",
    "\n",
    "node_ids = batch.y\n",
    "rel_ids = batch.relation\n",
    "edge_index = batch.edge_index\n",
    "batch_vec = batch.batch\n",
    "visit_node = batch.visit_padded_node.reshape(BATCH_SIZE, MAX_VISIT, NUM_NODES).float()\n",
    "ehr_nodes = batch.ehr_nodes.reshape(BATCH_SIZE, NUM_NODES).float()\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(node_ids, rel_ids, edge_index, batch_vec, visit_node, ehr_nodes)\n",
    "\n",
    "print(f\"Input: {batch.y.shape[0]} total nodes across {BATCH_SIZE} patient graphs\")\n",
    "print(f\"Output logits: {logits.shape}\")\n",
    "print(f\"Predictions: {torch.sigmoid(logits).squeeze().cpu().numpy().round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop\n",
    "\n",
    "Train GraphCare with a standard PyTorch loop. Note that GraphCare uses `torch_geometric.loader.DataLoader` rather than PyHealth's `Trainer`, since the data pipeline requires PyG batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        bs = BATCH_SIZE\n",
    "        vn = data.visit_padded_node.reshape(bs, MAX_VISIT, NUM_NODES).float()\n",
    "        en = data.ehr_nodes.reshape(bs, NUM_NODES).float()\n",
    "        \n",
    "        logits = model(\n",
    "            node_ids=data.y,\n",
    "            rel_ids=data.relation,\n",
    "            edge_index=data.edge_index,\n",
    "            batch=data.batch,\n",
    "            visit_node=vn,\n",
    "            ehr_nodes=en,\n",
    "            in_drop=True,\n",
    "        )\n",
    "        \n",
    "        labels = data.label.reshape(bs, -1).float()\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    \"\"\"Evaluate and return metrics.\"\"\"\n",
    "    model.eval()\n",
    "    y_true_all, y_prob_all = [], []\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        bs = BATCH_SIZE\n",
    "        vn = data.visit_padded_node.reshape(bs, MAX_VISIT, NUM_NODES).float()\n",
    "        en = data.ehr_nodes.reshape(bs, NUM_NODES).float()\n",
    "        \n",
    "        logits = model(\n",
    "            node_ids=data.y,\n",
    "            rel_ids=data.relation,\n",
    "            edge_index=data.edge_index,\n",
    "            batch=data.batch,\n",
    "            visit_node=vn,\n",
    "            ehr_nodes=en,\n",
    "        )\n",
    "        \n",
    "        labels = data.label.reshape(bs, -1)\n",
    "        y_prob_all.append(torch.sigmoid(logits).cpu())\n",
    "        y_true_all.append(labels.cpu())\n",
    "    \n",
    "    y_true = torch.cat(y_true_all).numpy()\n",
    "    y_prob = torch.cat(y_prob_all).numpy()\n",
    "    \n",
    "    return {\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob),\n",
    "        \"pr_auc\": average_precision_score(y_true, y_prob),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: BAT/joint, 549,941 parameters\n",
      "Training for 10 epochs...\n",
      "\n",
      "Epoch  1 | loss=0.7127 | val_roc_auc=0.5938 | val_pr_auc=0.6341\n",
      "Epoch  2 | loss=0.6975 | val_roc_auc=0.5625 | val_pr_auc=0.6046\n",
      "Epoch  3 | loss=0.6992 | val_roc_auc=0.5625 | val_pr_auc=0.5265\n",
      "Epoch  4 | loss=0.6872 | val_roc_auc=0.5625 | val_pr_auc=0.5225\n",
      "Epoch  5 | loss=0.6639 | val_roc_auc=0.5312 | val_pr_auc=0.5072\n",
      "Epoch  6 | loss=0.6854 | val_roc_auc=0.4844 | val_pr_auc=0.4868\n",
      "Epoch  7 | loss=0.6784 | val_roc_auc=0.5000 | val_pr_auc=0.4957\n",
      "Epoch  8 | loss=0.6564 | val_roc_auc=0.4219 | val_pr_auc=0.4709\n",
      "Epoch  9 | loss=0.6403 | val_roc_auc=0.3438 | val_pr_auc=0.4432\n",
      "Epoch 10 | loss=0.6539 | val_roc_auc=0.3438 | val_pr_auc=0.4446\n"
     ]
    }
   ],
   "source": [
    "# --- Training ---\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "model = GraphCare(\n",
    "    num_nodes=NUM_NODES,\n",
    "    num_rels=NUM_RELS,\n",
    "    max_visit=MAX_VISIT,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    out_channels=1,\n",
    "    layers=2,\n",
    "    dropout=0.5,\n",
    "    decay_rate=0.01,\n",
    "    node_emb=node_emb,\n",
    "    rel_emb=rel_emb,\n",
    "    gnn=\"BAT\",\n",
    "    patient_mode=\"joint\",\n",
    "    drop_rate=0.1,\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model: BAT/joint, {total_params:,} parameters\")\n",
    "print(f\"Training for {EPOCHS} epochs...\\n\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "    val_metrics = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(\n",
    "        f\"Epoch {epoch:2d} | loss={train_loss:.4f} | \"\n",
    "        f\"val_roc_auc={val_metrics['roc_auc']:.4f} | \"\n",
    "        f\"val_pr_auc={val_metrics['pr_auc']:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results\n",
      "========================================\n",
      "  roc_auc: 0.7969\n",
      "  pr_auc: 0.8318\n"
     ]
    }
   ],
   "source": [
    "test_metrics = evaluate(model, test_loader, device)\n",
    "\n",
    "print(\"Test Results\")\n",
    "print(\"=\" * 40)\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention Interpretability\n",
    "\n",
    "GraphCare's BAT layers produce interpretable attention weights:\n",
    "- **Alpha (α):** Visit-level attention — which visits matter most\n",
    "- **Beta (β):** Node-level attention with temporal decay — which nodes matter, weighted by recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weight shapes (per layer):\n",
      "  Layer 1:\n",
      "    alpha:     torch.Size([16, 5, 500])  (batch, max_visit, num_nodes)\n",
      "    beta:      torch.Size([16, 5, 1])  (batch, max_visit, 1)\n",
      "    attn:      torch.Size([914, 1])  (num_edges_in_batch, 1)\n",
      "    edge_w:    torch.Size([914, 1])  (num_edges_in_batch, 1)\n",
      "  Layer 2:\n",
      "    alpha:     torch.Size([16, 5, 500])  (batch, max_visit, num_nodes)\n",
      "    beta:      torch.Size([16, 5, 1])  (batch, max_visit, 1)\n",
      "    attn:      torch.Size([914, 1])  (num_edges_in_batch, 1)\n",
      "    edge_w:    torch.Size([914, 1])  (num_edges_in_batch, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get attention weights\n",
    "batch = next(iter(test_loader))\n",
    "batch = batch.to(device)\n",
    "\n",
    "vn = batch.visit_padded_node.reshape(BATCH_SIZE, MAX_VISIT, NUM_NODES).float()\n",
    "en = batch.ehr_nodes.reshape(BATCH_SIZE, NUM_NODES).float()\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits, alphas, betas, attns, edge_ws = model(\n",
    "        node_ids=batch.y,\n",
    "        rel_ids=batch.relation,\n",
    "        edge_index=batch.edge_index,\n",
    "        batch=batch.batch,\n",
    "        visit_node=vn,\n",
    "        ehr_nodes=en,\n",
    "        store_attn=True,\n",
    "    )\n",
    "\n",
    "print(\"Attention weight shapes (per layer):\")\n",
    "for i, (a, b, att, ew) in enumerate(zip(alphas, betas, attns, edge_ws)):\n",
    "    print(f\"  Layer {i+1}:\")\n",
    "    print(f\"    alpha:     {a.shape}  (batch, max_visit, num_nodes)\")\n",
    "    print(f\"    beta:      {b.shape}  (batch, max_visit, 1)\")\n",
    "    print(f\"    attn:      {att.shape}  (num_edges_in_batch, 1)\")\n",
    "    print(f\"    edge_w:    {ew.shape}  (num_edges_in_batch, 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: Recent visits (higher index) get boosted by temporal decay λ_j = exp(γ(V−j))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\637682\\AppData\\Local\\Temp\\ipykernel_25424\\2141043452.py:18: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# --- Visualise visit-level attention for first patient ---\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # Non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "patient_idx = 0\n",
    "\n",
    "# Beta weights for patient 0, layer 1: (max_visit, 1)\n",
    "beta_patient = betas[0][patient_idx].squeeze().cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 3))\n",
    "visits = [f\"Visit {j+1}\" for j in range(MAX_VISIT)]\n",
    "bars = ax.bar(visits, beta_patient, color=\"steelblue\")\n",
    "ax.set_ylabel(\"β attention × temporal decay\")\n",
    "ax.set_title(\"Visit-Level Attention Weights (Patient 0, Layer 1)\")\n",
    "ax.axhline(y=0, color=\"gray\", linestyle=\"--\", linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Note: Recent visits (higher index) get boosted by temporal decay λ_j = exp(γ(V−j))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most attended KG nodes for Patient 0:\n",
      " Node ID    Importance\n",
      "------------------------\n",
      "     492        1.0000\n",
      "     103        1.0000\n",
      "     450        1.0000\n",
      "      92        1.0000\n",
      "     153        1.0000\n",
      "     167        1.0000\n",
      "     205        1.0000\n",
      "     204        1.0000\n",
      "     200        1.0000\n",
      "     216        1.0000\n",
      "\n",
      "In a real setting, these node IDs map to medical concepts\n",
      "(conditions, procedures, drugs) via the cluster mapping.\n"
     ]
    }
   ],
   "source": [
    "# --- Top attended nodes ---\n",
    "# Alpha for patient 0, layer 1: (max_visit, num_nodes)\n",
    "alpha_patient = alphas[0][patient_idx].cpu()  # (max_visit, num_nodes)\n",
    "\n",
    "# Sum across visits to find globally important nodes\n",
    "node_importance = alpha_patient.sum(dim=0).numpy()  # (num_nodes,)\n",
    "\n",
    "top_k = 10\n",
    "top_indices = np.argsort(node_importance)[-top_k:][::-1]\n",
    "\n",
    "print(f\"Top {top_k} most attended KG nodes for Patient 0:\")\n",
    "print(f\"{'Node ID':>8}  {'Importance':>12}\")\n",
    "print(\"-\" * 24)\n",
    "for idx in top_indices:\n",
    "    print(f\"{idx:>8d}  {node_importance[idx]:>12.4f}\")\n",
    "print()\n",
    "print(\"In a real setting, these node IDs map to medical concepts\")\n",
    "print(\"(conditions, procedures, drugs) via the cluster mapping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Using Pre-Built KG Artifacts (Real Data)\n",
    "\n",
    "For real EHR data with pre-built KGs, use the `graphcare_utils` module:\n",
    "\n",
    "```python\n",
    "from pyhealth.models.graphcare_utils import (\n",
    "    load_kg_artifacts,\n",
    "    prepare_graphcare_data,\n",
    "    build_graphcare_dataloaders,\n",
    "    reshape_batch_tensors,\n",
    ")\n",
    "\n",
    "# 1. Load pre-built KG artifacts\n",
    "artifacts = load_kg_artifacts(\n",
    "    sample_dataset_path=\"sample_dataset_mimic3_mortality_th015.pkl\",\n",
    "    graph_path=\"graph_mimic3_mortality_th015.pkl\",\n",
    "    ent_emb_path=\"entity_embedding.pkl\",\n",
    "    rel_emb_path=\"relation_embedding.pkl\",\n",
    "    cluster_path=\"clusters_th015.json\",\n",
    "    cluster_rel_path=\"clusters_rel_th015.json\",\n",
    "    ccscm_id2clus_path=\"ccscm_id2clus.json\",\n",
    "    ccsproc_id2clus_path=\"ccsproc_id2clus.json\",\n",
    ")\n",
    "\n",
    "# 2. Prepare data (labels, embeddings, splits)\n",
    "prepared = prepare_graphcare_data(artifacts, task=\"mortality\")\n",
    "\n",
    "# 3. Build PyG DataLoaders\n",
    "train_loader, val_loader, test_loader = build_graphcare_dataloaders(\n",
    "    prepared, batch_size=64,\n",
    ")\n",
    "\n",
    "# 4. Build model\n",
    "model = GraphCare(\n",
    "    num_nodes=prepared[\"num_nodes\"],\n",
    "    num_rels=prepared[\"num_rels\"],\n",
    "    max_visit=prepared[\"max_visit\"],\n",
    "    embedding_dim=prepared[\"node_emb\"].shape[1],\n",
    "    hidden_dim=128,\n",
    "    out_channels=prepared[\"task_config\"][\"out_channels\"],\n",
    "    node_emb=prepared[\"node_emb\"],\n",
    "    rel_emb=prepared[\"rel_emb\"],\n",
    "    gnn=\"BAT\",\n",
    "    patient_mode=\"joint\",\n",
    ")\n",
    "\n",
    "# 5. Use reshape_batch_tensors in the training loop\n",
    "for data in train_loader:\n",
    "    batch_tensors = reshape_batch_tensors(\n",
    "        data, batch_size=64,\n",
    "        max_visit=prepared[\"max_visit\"],\n",
    "        num_nodes=prepared[\"num_nodes\"],\n",
    "    )\n",
    "    logits = model(**batch_tensors)  # minus 'label'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### GraphCare Model\n",
    "\n",
    "Predicts healthcare outcomes using personalized patient knowledge graphs with bi-attention augmented GNNs.\n",
    "\n",
    "| Component | Options | Description |\n",
    "|-----------|---------|-------------|\n",
    "| **GNN Backbone** | `BAT` (default), `GAT`, `GIN` | Message-passing layer; BAT adds bi-attention + edge attention |\n",
    "| **Patient Mode** | `joint` (default), `graph`, `node` | How to produce patient-level representation |\n",
    "| **Attention** | α (visit), β (node) | Visit-level and node-level attention with temporal decay |\n",
    "| **Temporal Decay** | λ_j = exp(γ(V−j)) | Exponential weighting favouring recent visits |\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- Pre-trained node/relation embeddings (from TransE or word2vec on KG)\n",
    "- Optional edge dropout for regularisation\n",
    "- Attention weights for clinical interpretability\n",
    "- Supports mortality, readmission, drug recommendation, length-of-stay tasks\n",
    "\n",
    "### Files\n",
    "\n",
    "| File | Purpose |\n",
    "|------|------|\n",
    "| `pyhealth/models/graphcare.py` | Model implementation (GraphCare + BiAttentionGNNConv) |\n",
    "| `pyhealth/models/graphcare_utils.py` | Data pipeline utilities (KG loading, subgraph extraction, dataloaders) |\n",
    "| `examples/train_graphcare.py` | End-to-end training script with CLI |\n",
    "| `tests/test_graphcare.py` | Unit tests |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
