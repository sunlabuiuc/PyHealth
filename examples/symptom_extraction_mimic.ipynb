{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "294228f8",
   "metadata": {},
   "source": [
    "# Use the SymptomExtraction Task to Train Model to Extract Symptoms from MIMIC-III Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31c3abc",
   "metadata": {},
   "source": [
    "## Make sure all required modules are installed and imported "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6379a329",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/mariellederocher/pyhealth.git@fix/dates_v2 -q\n",
    "!pip install seqeval -q\n",
    "!pip install transformers scispacy spacy -q\n",
    "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.4/en_core_sci_sm-0.5.4.tar.gz -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c77ec4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import spacy\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f1c770",
   "metadata": {},
   "source": [
    "## Load MIMIC-III Dataset with Clinical Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038771a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyhealth.datasets import MIMIC3Dataset\n",
    "\n",
    "root = \"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III\"\n",
    "dataset = MIMIC3Dataset(\n",
    "    root=root,\n",
    "    dataset_name=\"mimic3\",\n",
    "    tables=[\n",
    "        \"diagnoses_icd\",\n",
    "        \"procedures_icd\",\n",
    "        \"noteevents\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92d25b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyhealth.tasks import MIMIC3ICD9Coding\n",
    "\n",
    "mimic3_coding = MIMIC3ICD9Coding()\n",
    "samples = dataset.set_task(mimic3_coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0182003",
   "metadata": {},
   "source": [
    "## Weak Labeling using scispaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a932f82a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "nlp = en_core_sci_sm.load()\n",
    "nlp.add_pipe(\"scispacy_linker\", config={\"resolve_abbreviations\": True, \"linker_name\": \"umls\", \"threshold\": .9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f194b1c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def weak_label(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.text for token in doc]\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    \n",
    "    linker = nlp.get_pipe(\"scispacy_linker\")\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        # print(ent.label_)\n",
    "        for umls_ent in entity._.kb_ents:\n",
    "            # print(linker.kb.cui_to_entity[umls_ent[0]].aliases)\n",
    "            if 'finding' in str(linker.kb.cui_to_entity[umls_ent[0]].aliases):\n",
    "                start, end = ent.start, ent.end\n",
    "                labels[start] = \"B-SYMPTOM\"\n",
    "                for i in range(start+1, end):\n",
    "                    labels[i] = \"I-SYMPTOM\"\n",
    "                break\n",
    "    return tokens, labels\n",
    "\n",
    "samples_processed = 0\n",
    "data = []\n",
    "for sample in samples:\n",
    "    if samples_processed % 50 == 0:\n",
    "        print(\"Processed %d out of %d samples\" % (samples_processed, len(samples)))\n",
    "    data.append(weak_label(sample['text']))\n",
    "    samples_processed += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880ba69",
   "metadata": {},
   "source": [
    "## Prepare Datset with BERT Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8874fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "label_map = {\"O\": 0, \"B-SYMPTOM\": 1, \"I-SYMPTOM\": 2}\n",
    "\n",
    "def tokenize_and_align(tokens, labels):\n",
    "    encoding = tokenizer(tokens, is_split_into_words=True, return_offsets_mapping=True, return_attention_mask=True, return_tensors=None)\n",
    "    word_ids = encoding.word_ids(0)  # map subwords back to original tokens\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label_map[labels[word_idx]])\n",
    "        else:\n",
    "            label_ids.append(label_map[labels[word_idx]])  # or -100 to ignore subwords\n",
    "        previous_word_idx = word_idx\n",
    "    encoding[\"labels\"] = label_ids\n",
    "    return encoding\n",
    "\n",
    "tokenized = [tokenize_and_align(tokens, labels) for tokens, labels in data]\n",
    "train_set, val_set = train_test_split(tokenized, test_size=0.2)\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b8d7f",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e21202",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-SYMPTOM\", \"I-SYMPTOM\"]\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=3)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "task = SymptomExtraction(model=model, tokenizer=tokenizer, optimizer=optimizer, loss_fn=loss_fn, label_map=label_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a831a5",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d08fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", num_labels=3)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "task = SymptomExtraction(model=model, tokenizer=tokenizer, optimizer=optimizer, loss_fn=loss_fn, label_map=label_map)\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for batch in train_set:\n",
    "        total_loss += task.train_step([batch])\n",
    "    print(f\"Epoch {epoch + 1} - Loss: {total_loss / len(train_set):.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
