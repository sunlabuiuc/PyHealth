# -*- coding: utf-8 -*-
"""MIMIC_SBDH_BioClinicalBERT_Alcohol_Use.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IHL_R468SMV0DRhUbgxM3gUUxGZKLcI6

# MIMIC-SBDH + NOTEEVENTS: Bio-ClinicalBERT Alcohol Use

This notebook:
- Loads **MIMIC-SBDH** labels
- Streams **NOTEEVENTS.csv** in chunks and then does a simple pandas join
- Builds a **binary Alcohol Use** label from `behavior_alcohol`
- Fine-tunes **Bio-ClinicalBERT** with 5-fold CV
- Runs an input-length ablation (**128 vs 64 tokens**)

## Install Dependencies
"""

!pip install -q transformers accelerate datasets scikit-learn

"""## Import Libraries"""

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, f1_score

from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    get_linear_schedule_with_warmup,
)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('Device:', device)

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if device.type == 'cuda':
    torch.cuda.manual_seed_all(SEED)

"""## Mount Google Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""## Load MIMIC-SBDH and Prepare IDs"""

SBDH_PATH       = "/content/drive/MyDrive/Colab Notebooks/MIMIC-SBDH.csv"
NOTEEVENTS_PATH = "/content/drive/MyDrive/Colab Notebooks/NOTEEVENTS.csv"

df_sbdh = pd.read_csv(SBDH_PATH)
print("MIMIC-SBDH shape:", df_sbdh.shape)
df_sbdh.head()

# Unique row_ids referenced in MIMIC-SBDH
df_sbdh['row_id'] = df_sbdh['row_id'].astype(int)
target_ids = set(df_sbdh['row_id'].unique())
print("Number of unique row_id in SBDH:", len(target_ids))

"""## Stream NOTEEVENTS in Chunks and Then Simple Join

We only read `ROW_ID` and `TEXT`, filter rows whose `ROW_ID` is in `target_ids`,
then concatenate and do a normal pandas merge.
"""

usecols = ['ROW_ID', 'TEXT']
chunksize = 100000
filtered_chunks = []

for i, chunk in enumerate(pd.read_csv(
    NOTEEVENTS_PATH,
    usecols=usecols,
    chunksize=chunksize,
    low_memory=False,
)):
    print(f"Processing chunk {i+1}...")
    chunk['ROW_ID'] = chunk['ROW_ID'].astype(int)
    mask = chunk['ROW_ID'].isin(target_ids)
    filtered = chunk[mask]
    if not filtered.empty:
        filtered_chunks.append(filtered)

notes_small = pd.concat(filtered_chunks, ignore_index=True)
print("Filtered notes_small shape:", notes_small.shape)
notes_small.head()

"""## Simple pandas Join: SBDH + NOTEEVENTS"""

notes_small['ROW_ID'] = notes_small['ROW_ID'].astype(int)

df_merged = df_sbdh.merge(
    notes_small,
    left_on='row_id',
    right_on='ROW_ID',
    how='inner',
)

print("Merged shape:", df_merged.shape)
df_merged[['row_id', 'ROW_ID', 'TEXT', 'behavior_alcohol']].head()

"""## Build Binary Alcohol Use Labels and Modeling DataFrame

- Rename `TEXT` → `note_text`
- Map `behavior_alcohol` (0–4) → binary label `alcohol_use_bin`
"""

df_merged = df_merged.rename(columns={'TEXT': 'note_text'})

TEXT_COL_RAW = 'note_text'
ALC_COL_RAW  = 'behavior_alcohol'

# 0 = not mentioned, 1 = negative, 2 = positive, 3 = potential, 4 = N/A
bin_map = {0: 0, 1: 0, 2: 1, 3: 1, 4: 0}

df_merged['alcohol_use_bin'] = df_merged[ALC_COL_RAW].map(bin_map)

df_model = df_merged[[TEXT_COL_RAW, 'alcohol_use_bin']].dropna().copy()
df_model['alcohol_use_bin'] = df_model['alcohol_use_bin'].astype(int)
df_model = df_model.reset_index(drop=True)

print('df_model shape:', df_model.shape)
print('Label distribution:')
print(df_model['alcohol_use_bin'].value_counts())
df_model.head()

"""## Tokenizer & Dataset Class"""

pretrained_model_name = 'emilyalsentzer/Bio_ClinicalBERT'
tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)

class NotesDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=128):
        self.texts = list(texts)
        self.labels = list(labels)
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        enc = self.tokenizer(
            self.texts[idx],
            truncation=True,
            padding='max_length',
            max_length=self.max_len,
            return_tensors='pt',
        )
        item = {k: v.squeeze(0) for k, v in enc.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

"""## Upsampling to Handle Class Imbalance"""

def make_upsampled_df(df, label_col):
    counts = df[label_col].value_counts()
    max_n = counts.max()
    pieces = []
    for cls, n in counts.items():
        sub = df[df[label_col] == cls]
        if n < max_n:
            extra = sub.sample(max_n - n, replace=True, random_state=42)
            sub = pd.concat([sub, extra], axis=0)
        pieces.append(sub)
    out = pd.concat(pieces, axis=0).sample(frac=1.0, random_state=42).reset_index(drop=True)
    return out

"""## Train One Fold (Bio-ClinicalBERT)"""

def train_one_fold(
    train_df,
    val_df,
    fold_idx,
    num_labels=2,
    epochs=3,
    batch_size=8,
    lr=5e-5,
    max_len=128,
):
    label_col = 'alcohol_use_bin'

    train_df = make_upsampled_df(train_df, label_col)

    train_ds = NotesDataset(train_df[TEXT_COL_RAW], train_df[label_col], tokenizer, max_len=max_len)
    val_ds   = NotesDataset(val_df[TEXT_COL_RAW],   val_df[label_col],   tokenizer, max_len=max_len)

    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)

    model = AutoModelForSequenceClassification.from_pretrained(
        pretrained_model_name,
        num_labels=num_labels,
    )
    model.to(device)

    optimizer = AdamW(model.parameters(), lr=lr)

    total_steps = len(train_loader) * epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps) if total_steps > 0 else 0,
        num_training_steps=total_steps if total_steps > 0 else 1,
    )

    def run_epoch(loader, train=False):
        if train:
            model.train()
        else:
            model.eval()

        total_loss = 0.0
        all_y = []
        all_pred = []

        for batch in loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            labels = batch['labels']

            optimizer.zero_grad()

            with torch.set_grad_enabled(train):
                out = model(**batch)
                loss = out.loss
                logits = out.logits

                if train:
                    loss.backward()
                    optimizer.step()
                    if total_steps > 0:
                        scheduler.step()

            total_loss += loss.item() * labels.size(0)
            preds = torch.argmax(logits, dim=-1)
            all_y.extend(labels.detach().cpu().numpy().tolist())
            all_pred.extend(preds.detach().cpu().numpy().tolist())

        avg_loss = total_loss / len(loader.dataset)
        return avg_loss, np.array(all_y), np.array(all_pred)

    best_val_f1 = -1.0
    best_state = None

    for epoch in range(1, epochs + 1):
        train_loss, _, _ = run_epoch(train_loader, train=True)
        val_loss, y_true, y_pred = run_epoch(val_loader, train=False)

        macro_f1 = f1_score(y_true, y_pred, average='macro')
        print(
            f"[Fold {fold_idx}] Epoch {epoch}/{epochs} "
            f"train_loss={train_loss:.4f} val_loss={val_loss:.4f} macro_f1={macro_f1:.4f}"
        )

        if macro_f1 > best_val_f1:
            best_val_f1 = macro_f1
            best_state = {
                'model_state_dict': model.state_dict(),
                'y_true': y_true,
                'y_pred': y_pred,
            }

            global last_trained_model
            last_trained_model = model

    return best_val_f1, best_state

"""## Cross-Validation Wrapper"""

def run_cv_experiment(df_model, max_len=128, num_labels=2, epochs=1, batch_size=8, lr=5e-5):
    label_col = 'alcohol_use_bin'
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    fold_macro_f1 = []

    for fold, (train_idx, val_idx) in enumerate(skf.split(df_model[TEXT_COL_RAW], df_model[label_col]), start=1):
        print('=' * 70)
        print(f'Fold {fold} | max_len={max_len}')
        train_df = df_model.iloc[train_idx].reset_index(drop=True)
        val_df   = df_model.iloc[val_idx].reset_index(drop=True)

        best_f1, best_state = train_one_fold(
            train_df,
            val_df,
            fold_idx=fold,
            num_labels=num_labels,
            epochs=epochs,
            batch_size=batch_size,
            lr=lr,
            max_len=max_len,
        )
        fold_macro_f1.append(best_f1)

        y_true = best_state['y_true']
        y_pred = best_state['y_pred']
        print(f"\n[Fold {fold}] Best-epoch classification report:")
        print(classification_report(y_true, y_pred, digits=3))
        print('=' * 70)

    print("\n==== 5-fold CV summary (macro-F1) ====")
    print("Fold macro-F1:", [f"{x:.3f}" for x in fold_macro_f1])
    print("Mean macro-F1: {:.3f} ± {:.3f}".format(np.mean(fold_macro_f1), np.std(fold_macro_f1)))

    return fold_macro_f1

"""# Run Experiments

## Baseline: max_len = 128
"""

scores_128 = run_cv_experiment(
    df_model,
    max_len=128,
    num_labels=2,
    epochs=3,
    batch_size=8,
    lr=5e-5,
)

"""## Ablation: max_len = 64"""

scores_64 = run_cv_experiment(
    df_model,
    max_len=64,
    num_labels=2,
    epochs=3,
    batch_size=8,
    lr=5e-5,
)

"""## Inference Tests"""

import torch.nn.functional as F

threshold = 0.6  # be stricter about calling someone a drinker

test_texts = [
    "Patient denies any alcohol use. No drinking reported.",
    "Patient reports drinking beer daily for several years.",
    "Alcohol use unclear. Mentions occasional social drinks.",
    "No history of alcohol consumption noted.",
    "hello world.",
]

print("Running alcohol use predictions with thresholding...\n")

for i, text in enumerate(test_texts, start=1):
    enc = tokenizer(
        text,
        truncation=True,
        padding="max_length",
        max_length=128,
        return_tensors="pt"
    )
    enc = {k: v.to(device) for k, v in enc.items()}

    with torch.no_grad():
        logits = model(**enc).logits
        probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        p_no, p_yes = probs[0], probs[1]

    if p_yes >= threshold:
        final_label = "Alcohol Use (1)"
    elif p_yes <= 1 - threshold:
        final_label = "No Alcohol Use (0)"
    else:
        final_label = "Uncertain"

    print(f"Test {i}: {text}")
    print(f"  P(no_use=0): {p_no:.3f} | P(use=1): {p_yes:.3f}")
    print(f"  Final decision: {final_label}\n")